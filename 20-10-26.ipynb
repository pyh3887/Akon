{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn \n",
    "- 순서와 시간성을 가진데이터에서 특징을 추출하는 망 \n",
    "- rnn 을 구성하는 cell 하나는 ffnn망으로 구성된다.\n",
    "- cell 과 cell 을 연결해서 시간적으로 누적되는 특성을 추출한다.\n",
    "- rnn은 다양한 망 구성이 가능 \n",
    "    - one to one  : 바닐라망 (기본망)\n",
    "    - one to many : captioning\n",
    "    - many to one : 감정분석\n",
    "    - many to many : 번역망이나 비디오 해석\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'rnn/basic_rnn_cell/Tanh:0' shape=(?, 5) dtype=float32>, <tf.Tensor 'rnn/basic_rnn_cell/Tanh_1:0' shape=(?, 5) dtype=float32>]\n",
      "처음 데이터 특성: [[-0.86169916  0.8432061   0.47443736 -0.34020853  0.91262394]\n",
      " [-0.954275    0.97413385 -0.19946632 -0.9837568   0.9999926 ]\n",
      " [-0.9853696   0.99597186 -0.7259611  -0.99972767  1.        ]\n",
      " [ 0.99996036 -0.9998853  -0.99974436 -0.9349524   0.9999246 ]] 차수 : (4, 5)\n",
      "처음 데이터 특성: [[-0.80336785  0.96861356 -0.9838191  -0.9998541   1.        ]\n",
      " [-0.35039443  0.29315513 -0.27847555  0.65464085 -0.44384304]\n",
      " [ 0.1010991   0.6439424  -0.9837956  -0.99629325  0.9999995 ]\n",
      " [ 0.6921092  -0.82303387 -0.85478467 -0.9696164   0.97691935]] 차수 : (4, 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import os \n",
    "import tensorflow as tf \n",
    "tf.reset_default_graph()\n",
    "n_inputs = 3 # 셀당 입력차수\n",
    "n_neurons = 5 # 셀당 출력차수\n",
    "X0 = tf.placeholder(tf.float32 , [None, n_inputs])\n",
    "X1 = tf.placeholder(tf.float32, [None,n_inputs])\n",
    "\n",
    "#  셀정의\n",
    "# 셀당 필요한 가중치 (3 x 5)\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units= n_neurons) # 출력차수 \n",
    "\n",
    "# rnn 망 구성  static_rnn(메모리를 확보), dynamic_rnn(동적으로 메모리를 확보)\n",
    "\n",
    "output_seqs, states = tf.contrib.rnn.static_rnn(\n",
    "    basic_cell,[X0,X1], dtype=tf.float32) # 2 x 4 x 3  X0,X1  batch_size = 2  , 셀수 : 4 \n",
    "\n",
    "# 2 x 4 x 3 * 3 x 5  =   2 x 4 x 5  \n",
    "\n",
    "# output_seqs = 2 x 4 x 5 \n",
    "# states = 1 x 5 \n",
    "\n",
    "Y0, Y1 = output_seqs\n",
    "init = tf.global_variables_initializer()\n",
    "# 번역: 나는 학교에 간다 > encoding\n",
    "# A ASCIL CODE : A = 65 , \n",
    "\n",
    "X0_batch = np.array([[0,1,2],[3,4,5],[6,7,8],[9,0,1]])  # 입력 4 x 3 \n",
    "X1_batch = np.array([[9,8,7,],[0,0,0],[6,5,4],[3,2,1]])\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    Y0_val , Y1_val = sess.run([Y0,Y1], feed_dict={X0: X0_batch, X1: X1_batch}) # ㅊ리결과 나온 sequence\n",
    "    \n",
    "print(\"처음 데이터 특성:\" , Y0_val,\"차수 :\" , Y0_val.shape)\n",
    "print(\"처음 데이터 특성:\" , Y1_val,\"차수 :\" , Y1_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음 모델에서 차수들을 결정하시요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs_val:(4, 2, 5)\n",
      "[[[-0.54625416  0.6476071  -0.7252678  -0.6264582  -0.66258126]\n",
      "  [ 0.8503437   0.9594502  -0.96957827  0.98127    -0.9874417 ]]\n",
      "\n",
      " [[-0.25112727  0.9328735  -0.9675937  -0.5148283  -0.9565312 ]\n",
      "  [-0.68045765  0.9429903  -0.9543268   0.5114724  -0.954179  ]]\n",
      "\n",
      " [[ 0.09946536  0.98878515 -0.99659896 -0.38250393 -0.9951475 ]\n",
      "  [ 0.6233073   0.8172634  -0.89223623  0.96385914 -0.8918634 ]]\n",
      "\n",
      " [[ 0.9820139   0.9573325   0.9976685   0.54451734 -0.83294356]\n",
      "  [ 0.02791571  0.2852857  -0.80430776  0.1067356   0.27423617]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "n_steps = 2 \n",
    "n_inputs = 3 \n",
    "n_nerouns = 5\n",
    "X =tf.placeholder(tf.float32,[None, n_steps, n_inputs])\n",
    "\n",
    "basic_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons)\n",
    "# static : for 문을 사용하지 않고 메모리 공간확보\n",
    "# dynamic : for 문을 이용해서 계산\n",
    "outputs , states = tf.nn.dynamic_rnn(basic_cell,X,dtype = tf.float32)\n",
    "\n",
    "#outputs = 4 x 2 x 5 ,\n",
    "# states = 1 x 5 \n",
    "\n",
    "X_batch = np.array([\n",
    "                    [[0,1,2],[9,8,7]],\n",
    "                   [[3,4,5],[3,4,5]],\n",
    "                   [[6,7,8],[6,5,4]],\n",
    "                   [[9,0,1],[3,2,1]],\n",
    "                   ])   # 4 x 4 x 3 \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    outputs_val = outputs.eval(feed_dict={X:X_batch})\n",
    "print(\"outputs_val:{}\\n{}\".format(outputs_val.shape,outputs_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"rnn/transpose_1:0\", shape=(?, 28, 150), dtype=float32) Tensor(\"rnn/while/Exit_3:0\", shape=(?, 150), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "tf.reset_default_graph() # 그래프 초기화\n",
    "n_steps = 28 #셀수 \n",
    "n_inputs = 28  #셀당 입력수 28 x 28 = image한장\n",
    "n_neurons = 150 # 셀당 출력수 (특징확대)\n",
    "n_outputs = 10  # 150개의 추출 > 10 개로 축소\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32,[None,n_steps,n_inputs]) # ? x 28 x 28\n",
    "y = tf.placeholder(tf.int32,[None]) # ?\n",
    "# RNN 망 구성 및 특징 추출 \n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons) # 28 > 150 \n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32) # ? ,28 ,28\n",
    "\n",
    "# outputs :  150 x 28 x 150\n",
    "# states : 150 x 150\n",
    "print(outputs, states)\n",
    "# FFNN 시작\n",
    "\n",
    "logits = tf.layers.dense(states, n_outputs) #fully-connected : 가중치 생략\n",
    "# 가중치를 계산 : 150  x 150     150  x 10  \n",
    "# cost function : cross_entropy \n",
    "# 확률값으로 변환 \n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels= y , logits=logits)\n",
    "\n",
    "loss = tf.reduce_mean(xentropy) # 배치사이즈에 대한 평균값으로 = 비용값\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "training_op = optimizer.minimize(loss)  # = 학습\n",
    "correct = tf.nn.in_top_k(logits, y , 1) # 가장 큰 수의 인덱스를 구하고 실제값과 비교\n",
    "accuracy = tf.reduce_mean(tf.cast(correct,tf.float32)) # 확률값은 소수점까지 계산 > float32  = 정확도\n",
    "init = tf.global_variables_initializer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-22-66a3916be557>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\2-13\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\2-13\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\2-13\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\2-13\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "0 Train accuracy :  0.8933333 Test accuracy: 0.918\n",
      "1 Train accuracy :  0.97333336 Test accuracy: 0.9471\n",
      "2 Train accuracy :  0.97333336 Test accuracy: 0.9583\n",
      "3 Train accuracy :  0.96666664 Test accuracy: 0.9565\n",
      "4 Train accuracy :  0.96666664 Test accuracy: 0.96\n",
      "5 Train accuracy :  0.97333336 Test accuracy: 0.9616\n",
      "6 Train accuracy :  0.97333336 Test accuracy: 0.9632\n",
      "7 Train accuracy :  0.97333336 Test accuracy: 0.9647\n",
      "8 Train accuracy :  0.97333336 Test accuracy: 0.9649\n",
      "9 Train accuracy :  0.98 Test accuracy: 0.9691\n",
      "10 Train accuracy :  0.97333336 Test accuracy: 0.9743\n",
      "11 Train accuracy :  0.99333334 Test accuracy: 0.9684\n",
      "12 Train accuracy :  0.99333334 Test accuracy: 0.9719\n",
      "13 Train accuracy :  0.99333334 Test accuracy: 0.9656\n",
      "14 Train accuracy :  1.0 Test accuracy: 0.9746\n",
      "15 Train accuracy :  0.98 Test accuracy: 0.9757\n",
      "16 Train accuracy :  0.9866667 Test accuracy: 0.9731\n",
      "17 Train accuracy :  0.98 Test accuracy: 0.9704\n",
      "18 Train accuracy :  0.99333334 Test accuracy: 0.9729\n",
      "19 Train accuracy :  1.0 Test accuracy: 0.9704\n",
      "20 Train accuracy :  0.9866667 Test accuracy: 0.9755\n",
      "21 Train accuracy :  0.99333334 Test accuracy: 0.9739\n",
      "22 Train accuracy :  0.9866667 Test accuracy: 0.9774\n",
      "23 Train accuracy :  1.0 Test accuracy: 0.9737\n",
      "24 Train accuracy :  1.0 Test accuracy: 0.9755\n",
      "25 Train accuracy :  0.97333336 Test accuracy: 0.9738\n",
      "26 Train accuracy :  0.9866667 Test accuracy: 0.9775\n",
      "27 Train accuracy :  0.98 Test accuracy: 0.9737\n",
      "28 Train accuracy :  0.9866667 Test accuracy: 0.9745\n",
      "29 Train accuracy :  0.9866667 Test accuracy: 0.9732\n",
      "30 Train accuracy :  0.99333334 Test accuracy: 0.9723\n",
      "31 Train accuracy :  0.99333334 Test accuracy: 0.978\n",
      "32 Train accuracy :  0.9866667 Test accuracy: 0.966\n",
      "33 Train accuracy :  0.99333334 Test accuracy: 0.9779\n",
      "34 Train accuracy :  1.0 Test accuracy: 0.9783\n",
      "35 Train accuracy :  1.0 Test accuracy: 0.9784\n",
      "36 Train accuracy :  0.99333334 Test accuracy: 0.9761\n",
      "37 Train accuracy :  1.0 Test accuracy: 0.9774\n",
      "38 Train accuracy :  0.98 Test accuracy: 0.9777\n",
      "39 Train accuracy :  1.0 Test accuracy: 0.9771\n",
      "40 Train accuracy :  1.0 Test accuracy: 0.979\n",
      "41 Train accuracy :  1.0 Test accuracy: 0.9777\n",
      "42 Train accuracy :  0.9866667 Test accuracy: 0.9731\n",
      "43 Train accuracy :  1.0 Test accuracy: 0.9815\n",
      "44 Train accuracy :  0.9866667 Test accuracy: 0.9788\n",
      "45 Train accuracy :  0.99333334 Test accuracy: 0.9765\n",
      "46 Train accuracy :  0.99333334 Test accuracy: 0.9775\n",
      "47 Train accuracy :  0.99333334 Test accuracy: 0.9766\n",
      "48 Train accuracy :  0.99333334 Test accuracy: 0.9781\n",
      "49 Train accuracy :  1.0 Test accuracy: 0.9764\n",
      "50 Train accuracy :  0.98 Test accuracy: 0.9778\n",
      "51 Train accuracy :  1.0 Test accuracy: 0.978\n",
      "52 Train accuracy :  1.0 Test accuracy: 0.9766\n",
      "53 Train accuracy :  0.99333334 Test accuracy: 0.9788\n",
      "54 Train accuracy :  1.0 Test accuracy: 0.9788\n",
      "55 Train accuracy :  1.0 Test accuracy: 0.9776\n",
      "56 Train accuracy :  1.0 Test accuracy: 0.9788\n",
      "57 Train accuracy :  0.9866667 Test accuracy: 0.9759\n",
      "58 Train accuracy :  1.0 Test accuracy: 0.9764\n",
      "59 Train accuracy :  1.0 Test accuracy: 0.9786\n",
      "60 Train accuracy :  1.0 Test accuracy: 0.9783\n",
      "61 Train accuracy :  0.99333334 Test accuracy: 0.9621\n",
      "62 Train accuracy :  1.0 Test accuracy: 0.9762\n",
      "63 Train accuracy :  1.0 Test accuracy: 0.9802\n",
      "64 Train accuracy :  0.9866667 Test accuracy: 0.9764\n",
      "65 Train accuracy :  0.9866667 Test accuracy: 0.9762\n",
      "66 Train accuracy :  1.0 Test accuracy: 0.9784\n",
      "67 Train accuracy :  0.9866667 Test accuracy: 0.9762\n",
      "68 Train accuracy :  0.98 Test accuracy: 0.9797\n",
      "69 Train accuracy :  0.9866667 Test accuracy: 0.9784\n",
      "70 Train accuracy :  0.9866667 Test accuracy: 0.9707\n",
      "71 Train accuracy :  0.99333334 Test accuracy: 0.9746\n",
      "72 Train accuracy :  0.99333334 Test accuracy: 0.9771\n",
      "73 Train accuracy :  1.0 Test accuracy: 0.9786\n",
      "74 Train accuracy :  1.0 Test accuracy: 0.9713\n",
      "75 Train accuracy :  0.98 Test accuracy: 0.9776\n",
      "76 Train accuracy :  0.99333334 Test accuracy: 0.9781\n",
      "77 Train accuracy :  1.0 Test accuracy: 0.9754\n",
      "78 Train accuracy :  0.99333334 Test accuracy: 0.977\n",
      "79 Train accuracy :  0.99333334 Test accuracy: 0.9776\n",
      "80 Train accuracy :  0.9866667 Test accuracy: 0.9729\n",
      "81 Train accuracy :  0.99333334 Test accuracy: 0.9745\n",
      "82 Train accuracy :  1.0 Test accuracy: 0.9696\n",
      "83 Train accuracy :  0.99333334 Test accuracy: 0.9699\n",
      "84 Train accuracy :  1.0 Test accuracy: 0.9789\n",
      "85 Train accuracy :  1.0 Test accuracy: 0.9779\n",
      "86 Train accuracy :  1.0 Test accuracy: 0.9789\n",
      "87 Train accuracy :  0.9866667 Test accuracy: 0.9767\n",
      "88 Train accuracy :  0.99333334 Test accuracy: 0.9773\n",
      "89 Train accuracy :  0.99333334 Test accuracy: 0.9778\n",
      "90 Train accuracy :  1.0 Test accuracy: 0.9708\n",
      "91 Train accuracy :  0.97333336 Test accuracy: 0.9744\n",
      "92 Train accuracy :  0.9866667 Test accuracy: 0.9758\n",
      "93 Train accuracy :  1.0 Test accuracy: 0.9784\n",
      "94 Train accuracy :  1.0 Test accuracy: 0.9771\n",
      "95 Train accuracy :  0.98 Test accuracy: 0.9779\n",
      "96 Train accuracy :  1.0 Test accuracy: 0.9752\n",
      "97 Train accuracy :  1.0 Test accuracy: 0.9777\n",
      "98 Train accuracy :  0.99333334 Test accuracy: 0.9772\n",
      "99 Train accuracy :  0.9866667 Test accuracy: 0.9693\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "X_test = mnist.test.images.reshape((-1 , n_steps, n_inputs)) # 원래의 이미지 사이즈 \n",
    "y_test = mnist.test.labels\n",
    "n_epochs = 100 \n",
    "batch_size = 150 \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 60000/150\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples//batch_size):\n",
    "            X_batch , y_batch = mnist.train.next_batch(batch_size)\n",
    "            # -1 : 계산결과 나머지 : 150\n",
    "            X_batch  = X_batch.reshape((-1, n_steps, n_inputs))\n",
    "            # 150x28x28\n",
    "            sess.run(training_op , feed_dict={X:X_batch, y: y_batch}) # 학습\n",
    "        acc_train = accuracy.eval(feed_dict={X:X_batch, y: y_batch}) # 정확도 계산\n",
    "        acc_test = accuracy.eval(feed_dict={X:X_test, y: y_test})\n",
    "        print(epoch, \"Train accuracy : \", acc_train, \"Test accuracy:\", acc_test)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25}\n"
     ]
    }
   ],
   "source": [
    "# encoding > decoding\n",
    "# 끝단어 예측\n",
    "tf.reset_default_graph()\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "char_arr = ['a', 'b', 'c', 'd', 'e', 'f', 'g',\n",
    "            'h', 'i', 'j', 'k', 'l', 'm', 'n',\n",
    "            'o', 'p', 'q', 'r', 's', 't', 'u',\n",
    "            'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "# 글자에 대한 인덱스 번호 구성 \n",
    "num_dic = {n: i for i, n in enumerate(char_arr)} # enumerate \n",
    "\n",
    "print(num_dic) # num_dic > 문자 순서\n",
    "# 'a': 0, 'b': 1, 'c': 2, 'd': 3,\n",
    "\n",
    "dic_len = len(num_dic)\n",
    "\n",
    "seq_data = ['word', 'wood', 'deep', 'dive', 'cold', 'cool', 'load', 'love', 'kiss', 'kind'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원핫 인코딩\n",
    "# 셀당 입력 26  a ~ z \n",
    "# 셀개수 3 > w , o , r \n",
    "def make_batch(seq_data):\n",
    "    input_batch = [] # 3글자 \n",
    "    target_batch = [] # 1글자\n",
    "    for seq in seq_data: #  word\n",
    "        input=[num_dic[n] for n in seq[:-1]] # wor -> w : 22 , o : 14 ,r:17  > encoding\n",
    "        target = num_dic[seq[-1]] # d : 3 \n",
    "        #신경망에서 범주형 데이터 one-hot-encoding\n",
    "        # eye: 단위행렬\n",
    "        input_batch.append(np.eye(dic_len)[input]) # 원핫인코딩된 wor 값을 append\n",
    "        target_batch.append(target) # 원핫인코딩된 d 값을 append        \n",
    "    return input_batch, target_batch\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.01\n",
    "n_hidden = 128  # 가중치를 위한 히든 / 셀당 나가는 차수\n",
    "total_epoch = 30\n",
    "n_step = 3 # 셀수 3 w,o,r 개 \n",
    "n_input = n_class = dic_len # 26(a~z)\n",
    "X = tf.placeholder(tf.float32, [None, n_step, n_input])\n",
    "Y = tf.placeholder(tf.int32, [None])\n",
    "# FFNN을 위한 가중치 준비\n",
    "\n",
    "W = tf.Variable(tf.random_normal([n_hidden,n_class])) # 배치사이즈 , 셀수 , 셀당입력수\n",
    "b = tf.Variable(tf.random_normal([n_class])) #128 x 26\n",
    "# RNN으로 특징을 추출 (셀당 가중치 학습)\n",
    "# control_state 를 추가 : 장기 기억이 가능하도록 : 변종들이 많이 있음\n",
    "\n",
    "cell1 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "\n",
    "cell1 = tf.nn.rnn_cell.DropoutWrapper(cell1, output_keep_prob=0.5, seed=10) # 계산회로를 랜덤하게 줄임 (과적합) 현재 50프로만 계산\n",
    "cell2 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "\n",
    "multi_cell = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2]) # 상하로 멀티 셀을 쌓는 망 ( 더 정확하게 학습하기 위해서 # ? x 3 x 26\n",
    "outputs, states = tf.nn.dynamic_rnn(multi_cell, X, dtype=tf.float32)\n",
    " # 3 x 배치사이즈 x 128\n",
    "outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "outputs = outputs[-1]\n",
    "\n",
    "model = tf.matmul(outputs, W) + b  # Dense 하고 동일하게 연산  배치사이즈 x 26\n",
    "cost = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=model, labels=Y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost =  4.115955\n",
      "Epoch: 0002 cost =  3.165696\n",
      "Epoch: 0003 cost =  1.804698\n",
      "Epoch: 0004 cost =  1.544078\n",
      "Epoch: 0005 cost =  0.752578\n",
      "Epoch: 0006 cost =  0.982026\n",
      "Epoch: 0007 cost =  0.857749\n",
      "Epoch: 0008 cost =  0.360921\n",
      "Epoch: 0009 cost =  0.477199\n",
      "Epoch: 0010 cost =  0.613145\n",
      "Epoch: 0011 cost =  0.202396\n",
      "Epoch: 0012 cost =  0.227673\n",
      "Epoch: 0013 cost =  0.250787\n",
      "Epoch: 0014 cost =  0.151396\n",
      "Epoch: 0015 cost =  0.150340\n",
      "Epoch: 0016 cost =  0.193193\n",
      "Epoch: 0017 cost =  0.096648\n",
      "Epoch: 0018 cost =  0.149024\n",
      "Epoch: 0019 cost =  0.292985\n",
      "Epoch: 0020 cost =  0.235613\n",
      "Epoch: 0021 cost =  0.067428\n",
      "Epoch: 0022 cost =  0.171842\n",
      "Epoch: 0023 cost =  0.165552\n",
      "Epoch: 0024 cost =  0.304036\n",
      "Epoch: 0025 cost =  0.140052\n",
      "Epoch: 0026 cost =  0.027387\n",
      "Epoch: 0027 cost =  0.070316\n",
      "Epoch: 0028 cost =  0.015157\n",
      "Epoch: 0029 cost =  0.180586\n",
      "Epoch: 0030 cost =  0.127802\n",
      "\n",
      "=== 예측결과 ===\n",
      " 입력값 : ['wor ', 'woo ', 'dee ', 'div ', 'col ', 'coo ', 'loa ', 'lov ', 'kis ', 'kin ']\n",
      " 예측값: ['word', 'wood', 'deep', 'dive', 'cold', 'cool', 'load', 'love', 'kiss', 'kind']\n",
      "정확도 1.0\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# 원핫 인코딩, 문자를 숫자로 매핑\n",
    "input_batch , target_batch = make_batch(seq_data)\n",
    "for epoch in range(total_epoch):\n",
    "    _, loss = sess.run([optimizer,cost], feed_dict={X: input_batch, Y:target_batch}) # optimizer 는 학습 , cost는 에포크당 비용 출력\n",
    "    print(\"Epoch:\", '%04d' % (epoch +1),\n",
    "         'cost = ', '{:.6f}'.format(loss))\n",
    "    # model 은 분산과 같은 형태로 예측\n",
    "    \n",
    "prediction = tf.cast(tf.argmax(model,1) , tf.int32)\n",
    "prediction_check = tf.equal(prediction, Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction_check, tf.float32))  #정확도\n",
    "input_batch , target_batch =make_batch(seq_data)\n",
    "predict, accuracy_val = sess.run([prediction, accuracy], feed_dict={X: input_batch, Y: target_batch})\n",
    "# predict : 수치 > 디코딩\n",
    "# [ 3  3 15  4  3 11  3  4 18  3]\n",
    "\n",
    "predict_words =[]\n",
    "for idx, val in enumerate(seq_data):\n",
    "    last_char = char_arr[predict[idx]]\n",
    "    predict_words.append(val[:3] + last_char)\n",
    "\n",
    "print(\"\\n=== 예측결과 ===\")\n",
    "print(\" 입력값 :\", [w[:3]+' ' for w in seq_data])\n",
    "print(\" 예측값:\" , predict_words)\n",
    "print(\"정확도\", accuracy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 (seq 2 seq )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow.python.framework import ops \n",
    "ops.reset_default_graph()\n",
    "\n",
    "char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz단어놀이나무소녀연습사랑']\n",
    "num_dic = {n: i for i , n in enumerate(char_arr)}\n",
    "dic_len = len(num_dic)\n",
    "print(dic_len)\n",
    "seq_cata = [['word', '단어'],['wood','나무'],\n",
    "            ['game', '놀이'],['girl', '소녀'],\n",
    "            ['test', '연습'],['love', '사랑']]\n",
    "\n",
    "# input(영어) -> smoking gun,\n",
    "# output(한글) , target(한글)\n",
    "# 두 개의 망은 연결\n",
    "\n",
    "\n",
    "def make_batch(seq_data):\n",
    "    input_batch = [] \n",
    "    output_batch = []\n",
    "    target_batch =[]\n",
    "    for seq in seq_data:\n",
    "        input = [num_dic[n] for n in seq[0]] # 25,17,20,6 > w o r d \n",
    "        output = [num_dic[n] for n in ('S' + seq[1])] # start : 비어있음 0 , 29 ,30\n",
    "        target = [num_dic[n] for n in (seq[1] + 'E')] # 번역의 종료시점[29,30,1]\n",
    "        input_batch.append(np.eye(dic_len)[input]) # 0000000000010000 > 원핫인코딩\n",
    "        output_batch.append(np.eye(dic_len)[output])\n",
    "        target_batch.append(target) # [0,29,30]\n",
    "    return input_batch, output_batch , target_batch\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_hidden = 128\n",
    "total_epoch = 100 \n",
    "c_class = n_input = dic_len # 41\n",
    "enc_input = tf.placeholder(tf.float32, [None,None,n_input]) #  6 (batch size) x 4 (cell) x 41 (input_data)\n",
    "dec_input = tf.placeholder(tf.float32, [None,None,n_input]) # 6 x 3 x 41\n",
    "targets = tf.placeholder(tf.int64 , [None,None]) #6 x 3 \n",
    "# 연결지점을 확인하시오\n",
    "with tf.variable_scope(\"encoder\"):\n",
    "    enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden) # 출력차수 128\n",
    "    enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell,output_keep_prob=0.5, seed = 100)\n",
    "    # 셀이 4개로 결정\n",
    "    outputs,enc_states = tf.nn.dynamic_rnn(enc_cell,enc_input,dtype=tf.float32)\n",
    "\n",
    "    # output 6 x 4 x 128\n",
    "with tf.variable_scope(\"decode\"):\n",
    "    dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden) # 128\n",
    "    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob = 0.5)\n",
    "    # 셀이 3개로 결정\n",
    "    outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input, initial_state = enc_states , dtype=tf.float32) # initial state > 망연결\n",
    "\n",
    "    # output : 6 x 3 x 128\n",
    "    \n",
    "model = tf.layers.dense(outputs,n_class,activation=None) # 6 x 3 x 41\n",
    "cost = tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model, labels= targets))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 3.825981\n",
      "Epoch: 0002 cost = 2.708013\n",
      "Epoch: 0003 cost = 1.502448\n",
      "Epoch: 0004 cost = 1.036828\n",
      "Epoch: 0005 cost = 0.498542\n",
      "Epoch: 0006 cost = 0.450478\n",
      "Epoch: 0007 cost = 0.375874\n",
      "Epoch: 0008 cost = 0.228618\n",
      "Epoch: 0009 cost = 0.222773\n",
      "Epoch: 0010 cost = 0.192974\n",
      "Epoch: 0011 cost = 0.181268\n",
      "Epoch: 0012 cost = 0.076539\n",
      "Epoch: 0013 cost = 0.128576\n",
      "Epoch: 0014 cost = 0.081179\n",
      "Epoch: 0015 cost = 0.052016\n",
      "Epoch: 0016 cost = 0.055920\n",
      "Epoch: 0017 cost = 0.039262\n",
      "Epoch: 0018 cost = 0.013592\n",
      "Epoch: 0019 cost = 0.051955\n",
      "Epoch: 0020 cost = 0.030595\n",
      "Epoch: 0021 cost = 0.008282\n",
      "Epoch: 0022 cost = 0.007826\n",
      "Epoch: 0023 cost = 0.009400\n",
      "Epoch: 0024 cost = 0.004106\n",
      "Epoch: 0025 cost = 0.021159\n",
      "Epoch: 0026 cost = 0.024164\n",
      "Epoch: 0027 cost = 0.029723\n",
      "Epoch: 0028 cost = 0.012619\n",
      "Epoch: 0029 cost = 0.004249\n",
      "Epoch: 0030 cost = 0.005995\n",
      "Epoch: 0031 cost = 0.007525\n",
      "Epoch: 0032 cost = 0.002384\n",
      "Epoch: 0033 cost = 0.014018\n",
      "Epoch: 0034 cost = 0.001963\n",
      "Epoch: 0035 cost = 0.003439\n",
      "Epoch: 0036 cost = 0.012514\n",
      "Epoch: 0037 cost = 0.003142\n",
      "Epoch: 0038 cost = 0.002294\n",
      "Epoch: 0039 cost = 0.007223\n",
      "Epoch: 0040 cost = 0.001864\n",
      "Epoch: 0041 cost = 0.003957\n",
      "Epoch: 0042 cost = 0.000884\n",
      "Epoch: 0043 cost = 0.001136\n",
      "Epoch: 0044 cost = 0.001090\n",
      "Epoch: 0045 cost = 0.001177\n",
      "Epoch: 0046 cost = 0.000674\n",
      "Epoch: 0047 cost = 0.002257\n",
      "Epoch: 0048 cost = 0.000521\n",
      "Epoch: 0049 cost = 0.000507\n",
      "Epoch: 0050 cost = 0.001177\n",
      "Epoch: 0051 cost = 0.001314\n",
      "Epoch: 0052 cost = 0.000843\n",
      "Epoch: 0053 cost = 0.000757\n",
      "Epoch: 0054 cost = 0.000823\n",
      "Epoch: 0055 cost = 0.001360\n",
      "Epoch: 0056 cost = 0.000320\n",
      "Epoch: 0057 cost = 0.000494\n",
      "Epoch: 0058 cost = 0.000628\n",
      "Epoch: 0059 cost = 0.000290\n",
      "Epoch: 0060 cost = 0.000361\n",
      "Epoch: 0061 cost = 0.000475\n",
      "Epoch: 0062 cost = 0.000783\n",
      "Epoch: 0063 cost = 0.001434\n",
      "Epoch: 0064 cost = 0.004479\n",
      "Epoch: 0065 cost = 0.000274\n",
      "Epoch: 0066 cost = 0.000147\n",
      "Epoch: 0067 cost = 0.001288\n",
      "Epoch: 0068 cost = 0.000466\n",
      "Epoch: 0069 cost = 0.000832\n",
      "Epoch: 0070 cost = 0.001389\n",
      "Epoch: 0071 cost = 0.001139\n",
      "Epoch: 0072 cost = 0.001133\n",
      "Epoch: 0073 cost = 0.000404\n",
      "Epoch: 0074 cost = 0.000350\n",
      "Epoch: 0075 cost = 0.000463\n",
      "Epoch: 0076 cost = 0.000476\n",
      "Epoch: 0077 cost = 0.000288\n",
      "Epoch: 0078 cost = 0.000405\n",
      "Epoch: 0079 cost = 0.000271\n",
      "Epoch: 0080 cost = 0.000186\n",
      "Epoch: 0081 cost = 0.000248\n",
      "Epoch: 0082 cost = 0.000470\n",
      "Epoch: 0083 cost = 0.000197\n",
      "Epoch: 0084 cost = 0.000614\n",
      "Epoch: 0085 cost = 0.001690\n",
      "Epoch: 0086 cost = 0.001467\n",
      "Epoch: 0087 cost = 0.001416\n",
      "Epoch: 0088 cost = 0.000516\n",
      "Epoch: 0089 cost = 0.000610\n",
      "Epoch: 0090 cost = 0.000475\n",
      "Epoch: 0091 cost = 0.000365\n",
      "Epoch: 0092 cost = 0.000832\n",
      "Epoch: 0093 cost = 0.000350\n",
      "Epoch: 0094 cost = 0.000119\n",
      "Epoch: 0095 cost = 0.000742\n",
      "Epoch: 0096 cost = 0.000922\n",
      "Epoch: 0097 cost = 0.000281\n",
      "Epoch: 0098 cost = 0.000280\n",
      "Epoch: 0099 cost = 0.000342\n",
      "Epoch: 0100 cost = 0.000222\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "input_batch, output_batch, target_batch = make_batch(seq_data)\n",
    "for epoch in range(total_epoch):\n",
    "    _, loss = sess.run([optimizer, cost],\n",
    "                       feed_dict={enc_input: input_batch,\n",
    "                                  dec_input: output_batch,\n",
    "                                  targets: target_batch})\n",
    "    print('Epoch:', '%04d' % (epoch + 1),\n",
    "          'cost =', '{:.6f}'.format(loss))\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word -> 단어\n",
      "love -> 사랑\n",
      "abcd -> \n"
     ]
    }
   ],
   "source": [
    "def translate(word):\n",
    "    seq_data = [word, 'P' * len(word)] # 번역된 데이터  (임의의 글자 : 'P')\n",
    "    input_batch , output_batch, target_batch = make_batch([seq_data])\n",
    "    prediction = tf.argmax(model,2 ) # 예측값 6x3x41 (41 개의 단어중 하나로 예측)\n",
    "    result = sess.run(prediction, \n",
    "                      feed_dict={enc_input: input_batch ,\n",
    "                                dec_input: output_batch,\n",
    "                                targets: target_batch})\n",
    "    # 번역할 단어에 대한 41개중 하나로 예측\n",
    "    decoded = [char_arr[i] for i in result[0]] # 숫자를 문자로 매핑\n",
    "    end = decoded.index('E') # E를 예측\n",
    "    translated = ''.join(decoded[:end]) # E앞단까지 단어 조합\n",
    "    return translated\n",
    "\n",
    "print(\"word ->\", translate(\"word\"))\n",
    "print(\"love ->\", translate(\"love\"))\n",
    "print(\"abcd ->\", translate(\"abcd\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문제 : 다음 인코딩 된 값을 2개의 특성을 가진 데이터로 변환\n",
    "h= [1,1,0,0]\n",
    "e= [0,1,0,0]\n",
    "l= [0,0,1,0]\n",
    "o= [0,0,0,1]\n",
    "# hellpo 데이터에 댛아ㅕ 개의 특성 추출\n",
    "# 5x2 \n",
    "- 배치사이즈 , 셀수 , 셀당아웃풋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 4)\n",
      "(1, 5, 2)\n",
      "(1, 2)\n",
      "array([[ 0.2724337 , -0.04968274]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "ops.reset_default_graph()\n",
    "from tensorflow.contrib import rnn\n",
    "import pprint as pp\n",
    "sess = tf.InteractiveSession()\n",
    "h= [1,1,0,0]\n",
    "e= [0,1,0,0]\n",
    "l= [0,0,1,0]\n",
    "o= [0,0,0,1]\n",
    "with tf.variable_scope('hello') as scope:\n",
    "    hidden_size = 2 \n",
    "    cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "    x_data = np.array([[h,e,l,l,o]], dtype = np.float32) # 5 x 4 > hello 가 4개로 표현\n",
    "    print(x_data.shape)\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, x_data, dtype= tf.float32)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(outputs.shape)\n",
    "    print(states.shape)\n",
    "    pp.pprint(states.eval())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 batches ' hello', ' eolll' , 'lleel' 2 개의 특성으로 embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5, 4)\n",
      "(3, 5, 2)\n",
      "(3, 2)\n",
      "array([[ 0.18328455, -0.54818386],\n",
      "       [-0.17290401, -0.2799415 ],\n",
      "       [ 0.17010176,  0.10872702]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "ops.reset_default_graph()\n",
    "from tensorflow.contrib import rnn\n",
    "import pprint as pp\n",
    "sess = tf.InteractiveSession()\n",
    "h= [1,1,0,0]\n",
    "e= [0,1,0,0]\n",
    "l= [0,0,1,0]\n",
    "o= [0,0,0,1]\n",
    "with tf.variable_scope('hello') as scope:\n",
    "    hidden_size = 2 \n",
    "    cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "    x_data = np.array([[h,e,l,l,o],[e,o,l,l,l],[l,l,e,e,e]], dtype = np.float32) # 5 x 4 > hello 가 4개로 표현\n",
    "    print(x_data.shape)\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, x_data, dtype= tf.float32)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(outputs.shape)\n",
    "    print(states.shape)\n",
    "    pp.pprint(states.eval())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['대한민국', '의', '무궁', '한', '발전', '과', '세계', '를', '이끄는', '지도', '국가', '가', '되길', '희망', '합니다']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "token = okt.morphs(\"대한민국의 무궁한 발전과 세계를 이끄는 지도 국가가 되길 희망합니다\")\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'대한민국': 0, '의': 1, '무궁': 2, '한': 3, '발전': 4, '과': 5, '세계': 6, '를': 7, '이끄는': 8, '지도': 9, '국가': 10, '가': 11, '되길': 12, '희망': 13, '합니다': 14}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index = {}\n",
    "for voca in token:\n",
    "    if voca not in word2index.keys():\n",
    "        word2index[voca] = len(word2index) # 키 데이터\n",
    "print(word2index)\n",
    "\n",
    "def one_hot_encoding(word, word2index):\n",
    "    one_hot_vector = [0] * (len(word2index))\n",
    "    index = word2index[word]\n",
    "    one_hot_vector[index] = 1 \n",
    "    return one_hot_vector \n",
    "\n",
    "one_hot_encoding(\"대한민국\", word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
