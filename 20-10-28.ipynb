{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "47/47 [==============================] - 0s 524us/step - loss: 0.6482 - accuracy: 0.8128\n",
      "Epoch 2/100\n",
      "47/47 [==============================] - 0s 532us/step - loss: 0.4890 - accuracy: 0.8468\n",
      "Epoch 3/100\n",
      "47/47 [==============================] - 0s 597us/step - loss: 0.4416 - accuracy: 0.8511\n",
      "Epoch 4/100\n",
      "47/47 [==============================] - 0s 617us/step - loss: 0.4863 - accuracy: 0.8489\n",
      "Epoch 5/100\n",
      "47/47 [==============================] - 0s 612us/step - loss: 0.4430 - accuracy: 0.8532\n",
      "Epoch 6/100\n",
      "47/47 [==============================] - 0s 674us/step - loss: 0.4303 - accuracy: 0.8532\n",
      "Epoch 7/100\n",
      "47/47 [==============================] - 0s 656us/step - loss: 0.4421 - accuracy: 0.8511\n",
      "Epoch 8/100\n",
      "47/47 [==============================] - 0s 658us/step - loss: 0.4363 - accuracy: 0.8489\n",
      "Epoch 9/100\n",
      "47/47 [==============================] - 0s 672us/step - loss: 0.4165 - accuracy: 0.8489\n",
      "Epoch 10/100\n",
      "47/47 [==============================] - 0s 695us/step - loss: 0.4317 - accuracy: 0.8489\n",
      "Epoch 11/100\n",
      "47/47 [==============================] - 0s 684us/step - loss: 0.4458 - accuracy: 0.8489\n",
      "Epoch 12/100\n",
      "47/47 [==============================] - 0s 664us/step - loss: 0.4384 - accuracy: 0.8532\n",
      "Epoch 13/100\n",
      "47/47 [==============================] - 0s 636us/step - loss: 0.4651 - accuracy: 0.8532\n",
      "Epoch 14/100\n",
      "47/47 [==============================] - 0s 643us/step - loss: 0.4475 - accuracy: 0.8319\n",
      "Epoch 15/100\n",
      "47/47 [==============================] - 0s 643us/step - loss: 0.4934 - accuracy: 0.8255\n",
      "Epoch 16/100\n",
      "47/47 [==============================] - 0s 620us/step - loss: 0.4472 - accuracy: 0.8447\n",
      "Epoch 17/100\n",
      "47/47 [==============================] - 0s 567us/step - loss: 0.4747 - accuracy: 0.8383\n",
      "Epoch 18/100\n",
      "47/47 [==============================] - 0s 596us/step - loss: 0.4488 - accuracy: 0.8468\n",
      "Epoch 19/100\n",
      "47/47 [==============================] - 0s 580us/step - loss: 0.4407 - accuracy: 0.8511\n",
      "Epoch 20/100\n",
      "47/47 [==============================] - 0s 594us/step - loss: 0.4380 - accuracy: 0.8511\n",
      "Epoch 21/100\n",
      "47/47 [==============================] - 0s 642us/step - loss: 0.4331 - accuracy: 0.8532\n",
      "Epoch 22/100\n",
      "47/47 [==============================] - 0s 665us/step - loss: 0.4242 - accuracy: 0.8511\n",
      "Epoch 23/100\n",
      "47/47 [==============================] - 0s 623us/step - loss: 0.4183 - accuracy: 0.8532\n",
      "Epoch 24/100\n",
      "47/47 [==============================] - 0s 693us/step - loss: 0.4303 - accuracy: 0.8489\n",
      "Epoch 25/100\n",
      "47/47 [==============================] - 0s 692us/step - loss: 0.4246 - accuracy: 0.8511\n",
      "Epoch 26/100\n",
      "47/47 [==============================] - 0s 675us/step - loss: 0.4335 - accuracy: 0.8532\n",
      "Epoch 27/100\n",
      "47/47 [==============================] - 0s 676us/step - loss: 0.4492 - accuracy: 0.8383\n",
      "Epoch 28/100\n",
      "47/47 [==============================] - 0s 687us/step - loss: 0.4241 - accuracy: 0.8532\n",
      "Epoch 29/100\n",
      "47/47 [==============================] - 0s 643us/step - loss: 0.4212 - accuracy: 0.8532\n",
      "Epoch 30/100\n",
      "47/47 [==============================] - 0s 666us/step - loss: 0.4136 - accuracy: 0.8511\n",
      "Epoch 31/100\n",
      "47/47 [==============================] - 0s 689us/step - loss: 0.4389 - accuracy: 0.8511\n",
      "Epoch 32/100\n",
      "47/47 [==============================] - 0s 689us/step - loss: 0.4188 - accuracy: 0.8553\n",
      "Epoch 33/100\n",
      "47/47 [==============================] - 0s 640us/step - loss: 0.4428 - accuracy: 0.8532\n",
      "Epoch 34/100\n",
      "47/47 [==============================] - 0s 638us/step - loss: 0.4103 - accuracy: 0.8489\n",
      "Epoch 35/100\n",
      "47/47 [==============================] - 0s 595us/step - loss: 0.4178 - accuracy: 0.8489\n",
      "Epoch 36/100\n",
      "47/47 [==============================] - 0s 651us/step - loss: 0.4136 - accuracy: 0.8532\n",
      "Epoch 37/100\n",
      "47/47 [==============================] - 0s 632us/step - loss: 0.4429 - accuracy: 0.8511\n",
      "Epoch 38/100\n",
      "47/47 [==============================] - 0s 627us/step - loss: 0.4258 - accuracy: 0.8489\n",
      "Epoch 39/100\n",
      "47/47 [==============================] - 0s 665us/step - loss: 0.4714 - accuracy: 0.8319\n",
      "Epoch 40/100\n",
      "47/47 [==============================] - 0s 635us/step - loss: 0.4041 - accuracy: 0.8574\n",
      "Epoch 41/100\n",
      "47/47 [==============================] - 0s 650us/step - loss: 0.4210 - accuracy: 0.8511\n",
      "Epoch 42/100\n",
      "47/47 [==============================] - 0s 679us/step - loss: 0.4418 - accuracy: 0.8447\n",
      "Epoch 43/100\n",
      "47/47 [==============================] - 0s 652us/step - loss: 0.4096 - accuracy: 0.8511\n",
      "Epoch 44/100\n",
      "47/47 [==============================] - 0s 678us/step - loss: 0.4032 - accuracy: 0.8511\n",
      "Epoch 45/100\n",
      "47/47 [==============================] - 0s 696us/step - loss: 0.4067 - accuracy: 0.8532\n",
      "Epoch 46/100\n",
      "47/47 [==============================] - 0s 652us/step - loss: 0.4030 - accuracy: 0.8532\n",
      "Epoch 47/100\n",
      "47/47 [==============================] - 0s 683us/step - loss: 0.4132 - accuracy: 0.8447\n",
      "Epoch 48/100\n",
      "47/47 [==============================] - 0s 659us/step - loss: 0.4026 - accuracy: 0.8532\n",
      "Epoch 49/100\n",
      "47/47 [==============================] - 0s 697us/step - loss: 0.4082 - accuracy: 0.8511\n",
      "Epoch 50/100\n",
      "47/47 [==============================] - 0s 690us/step - loss: 0.4097 - accuracy: 0.8511\n",
      "Epoch 51/100\n",
      "47/47 [==============================] - 0s 655us/step - loss: 0.4124 - accuracy: 0.8532\n",
      "Epoch 52/100\n",
      "47/47 [==============================] - 0s 648us/step - loss: 0.3972 - accuracy: 0.8553\n",
      "Epoch 53/100\n",
      "47/47 [==============================] - 0s 616us/step - loss: 0.4128 - accuracy: 0.8489\n",
      "Epoch 54/100\n",
      "47/47 [==============================] - 0s 666us/step - loss: 0.4073 - accuracy: 0.8468\n",
      "Epoch 55/100\n",
      "47/47 [==============================] - 0s 678us/step - loss: 0.4110 - accuracy: 0.8532\n",
      "Epoch 56/100\n",
      "47/47 [==============================] - 0s 649us/step - loss: 0.3921 - accuracy: 0.8532\n",
      "Epoch 57/100\n",
      "47/47 [==============================] - 0s 675us/step - loss: 0.4091 - accuracy: 0.8532\n",
      "Epoch 58/100\n",
      "47/47 [==============================] - 0s 680us/step - loss: 0.3995 - accuracy: 0.8574\n",
      "Epoch 59/100\n",
      "47/47 [==============================] - 0s 658us/step - loss: 0.3985 - accuracy: 0.8532\n",
      "Epoch 60/100\n",
      "47/47 [==============================] - 0s 605us/step - loss: 0.3899 - accuracy: 0.8574\n",
      "Epoch 61/100\n",
      "47/47 [==============================] - 0s 642us/step - loss: 0.4041 - accuracy: 0.8553\n",
      "Epoch 62/100\n",
      "47/47 [==============================] - 0s 650us/step - loss: 0.4246 - accuracy: 0.8468\n",
      "Epoch 63/100\n",
      "47/47 [==============================] - 0s 641us/step - loss: 0.4068 - accuracy: 0.8532\n",
      "Epoch 64/100\n",
      "47/47 [==============================] - 0s 676us/step - loss: 0.4318 - accuracy: 0.8511\n",
      "Epoch 65/100\n",
      "47/47 [==============================] - 0s 645us/step - loss: 0.3902 - accuracy: 0.8553\n",
      "Epoch 66/100\n",
      "47/47 [==============================] - 0s 647us/step - loss: 0.4312 - accuracy: 0.8511\n",
      "Epoch 67/100\n",
      "47/47 [==============================] - 0s 624us/step - loss: 0.4164 - accuracy: 0.8489\n",
      "Epoch 68/100\n",
      "47/47 [==============================] - 0s 637us/step - loss: 0.4090 - accuracy: 0.8511\n",
      "Epoch 69/100\n",
      "47/47 [==============================] - 0s 638us/step - loss: 0.4020 - accuracy: 0.8511\n",
      "Epoch 70/100\n",
      "47/47 [==============================] - 0s 629us/step - loss: 0.3986 - accuracy: 0.8574\n",
      "Epoch 71/100\n",
      "47/47 [==============================] - 0s 637us/step - loss: 0.4027 - accuracy: 0.8511\n",
      "Epoch 72/100\n",
      "47/47 [==============================] - 0s 681us/step - loss: 0.4196 - accuracy: 0.8340\n",
      "Epoch 73/100\n",
      "47/47 [==============================] - 0s 662us/step - loss: 0.4019 - accuracy: 0.8532\n",
      "Epoch 74/100\n",
      "47/47 [==============================] - 0s 656us/step - loss: 0.3907 - accuracy: 0.8553\n",
      "Epoch 75/100\n",
      "47/47 [==============================] - 0s 660us/step - loss: 0.3890 - accuracy: 0.8553\n",
      "Epoch 76/100\n",
      "47/47 [==============================] - 0s 598us/step - loss: 0.4077 - accuracy: 0.8553\n",
      "Epoch 77/100\n",
      "47/47 [==============================] - 0s 645us/step - loss: 0.4178 - accuracy: 0.8426\n",
      "Epoch 78/100\n",
      "47/47 [==============================] - 0s 663us/step - loss: 0.4080 - accuracy: 0.8553\n",
      "Epoch 79/100\n",
      "47/47 [==============================] - 0s 645us/step - loss: 0.4148 - accuracy: 0.8426\n",
      "Epoch 80/100\n",
      "47/47 [==============================] - 0s 644us/step - loss: 0.4019 - accuracy: 0.8468\n",
      "Epoch 81/100\n",
      "47/47 [==============================] - 0s 655us/step - loss: 0.4057 - accuracy: 0.8532\n",
      "Epoch 82/100\n",
      "47/47 [==============================] - 0s 663us/step - loss: 0.4184 - accuracy: 0.8489\n",
      "Epoch 83/100\n",
      "47/47 [==============================] - 0s 652us/step - loss: 0.3944 - accuracy: 0.8532\n",
      "Epoch 84/100\n",
      "47/47 [==============================] - 0s 669us/step - loss: 0.4345 - accuracy: 0.8468\n",
      "Epoch 85/100\n",
      "47/47 [==============================] - 0s 712us/step - loss: 0.4050 - accuracy: 0.8489\n",
      "Epoch 86/100\n",
      "47/47 [==============================] - 0s 677us/step - loss: 0.3922 - accuracy: 0.8489\n",
      "Epoch 87/100\n",
      "47/47 [==============================] - 0s 629us/step - loss: 0.3976 - accuracy: 0.8511\n",
      "Epoch 88/100\n",
      "47/47 [==============================] - 0s 677us/step - loss: 0.3966 - accuracy: 0.8468\n",
      "Epoch 89/100\n",
      "47/47 [==============================] - 0s 682us/step - loss: 0.3886 - accuracy: 0.8468\n",
      "Epoch 90/100\n",
      "47/47 [==============================] - 0s 653us/step - loss: 0.3965 - accuracy: 0.8596\n",
      "Epoch 91/100\n",
      "47/47 [==============================] - 0s 650us/step - loss: 0.4220 - accuracy: 0.8532\n",
      "Epoch 92/100\n",
      "47/47 [==============================] - 0s 671us/step - loss: 0.4198 - accuracy: 0.8489\n",
      "Epoch 93/100\n",
      "47/47 [==============================] - 0s 658us/step - loss: 0.3953 - accuracy: 0.8511\n",
      "Epoch 94/100\n",
      "47/47 [==============================] - 0s 694us/step - loss: 0.3871 - accuracy: 0.8489\n",
      "Epoch 95/100\n",
      "47/47 [==============================] - 0s 684us/step - loss: 0.3864 - accuracy: 0.8574\n",
      "Epoch 96/100\n",
      "47/47 [==============================] - 0s 660us/step - loss: 0.4229 - accuracy: 0.8426\n",
      "Epoch 97/100\n",
      "47/47 [==============================] - 0s 637us/step - loss: 0.4149 - accuracy: 0.8447\n",
      "Epoch 98/100\n",
      "47/47 [==============================] - 0s 578us/step - loss: 0.3857 - accuracy: 0.8596\n",
      "Epoch 99/100\n",
      "47/47 [==============================] - 0s 660us/step - loss: 0.4051 - accuracy: 0.8532\n",
      "Epoch 100/100\n",
      "47/47 [==============================] - 0s 615us/step - loss: 0.3832 - accuracy: 0.8468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20cabc79348>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문제를 인식 -> 데이터 수집 > 전ㄴ처리 > 모델을 학습 > web app > deploy(배포) keras.js, tensorflow.js\n",
    "    \n",
    "# keras tf의 하위 서브모듈\n",
    "from tensorflow.keras.models import Sequential # Functional , Model class\n",
    "from tensorflow.keras.layers import Dense  # fully - connected\n",
    "\n",
    "import numpy as np \n",
    "import tensorflow as tf # wrapper 보자기로 감쌈 > 복잡한 것을 숨기기 위해 \n",
    "\n",
    "np.random.seed(3) \n",
    "tf.random.set_seed(3) # 랜덤값 초기화\n",
    "\n",
    "Data_set = np.loadtxt(\"./dataset/ThoraricSurgery.csv\", delimiter=\",\")\n",
    "\n",
    "X = Data_set[:,0:17] # 독립변수\n",
    "Y = Data_set[: ,17] # 종속변수\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(30 , input_dim=17 , activation='relu'))\n",
    "# ? x 30 30 x 1 ? x 1\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "# 0 ~ 1 사이의 값으로 출력 0.5보다 크면 1 , 0.5보다 작으면 0\n",
    "model.compile(loss='binary_crossentropy', optimizer = 'adam' , metrics=['accuracy'])\n",
    "model.fit(X,Y ,epochs = 100 , batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 30)                540       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 571\n",
      "Trainable params: 571\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary() # 17 x 30 = 510 가중치 bias 30 = 540\n",
    "30x 1 + 1(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.3427397 , 0.9322196 ],\n",
       "       [0.60834146, 0.17056336],\n",
       "       [1.0088804 , 0.8315352 ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 별도 패키지로 \n",
    "import keras.backend as K\n",
    "import numpy as np \n",
    "np.random.seed(42)\n",
    "\n",
    "x = np.random.rand(3,3)\n",
    "y = np.random.rand(3,2)\n",
    "\n",
    "x_var = K.variable(value=x)\n",
    "y_var = K.variable(value=y)\n",
    "xy = K.dot(x_var, y_var) # run 이 없이도 작업이 가능\n",
    "\n",
    "matrix_product = K.eval(xy) \n",
    "matrix_product\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pregnant  plasma  pressure  thickness  insulin   BMI  pedigree  age  class\n",
      "0         6     148        72         35        0  33.6     0.627   50      1\n",
      "1         1      85        66         29        0  26.6     0.351   31      0\n",
      "2         8     183        64          0        0  23.3     0.672   32      1\n",
      "3         1      89        66         23       94  28.1     0.167   21      0\n",
      "4         0     137        40         35      168  43.1     2.288   33      1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('./dataset/pima-indians-diabetes.csv',\n",
    "               names = [\"pregnant\", \"plasma\", \"pressure\", \"thickness\", \"insulin\", \"BMI\", \"pedigree\", \"age\", \"class\"]) \n",
    "\n",
    "print(df.head(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      "pregnant     768 non-null int64\n",
      "plasma       768 non-null int64\n",
      "pressure     768 non-null int64\n",
      "thickness    768 non-null int64\n",
      "insulin      768 non-null int64\n",
      "BMI          768 non-null float64\n",
      "pedigree     768 non-null float64\n",
      "age          768 non-null int64\n",
      "class        768 non-null int64\n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnant</th>\n",
       "      <th>plasma</th>\n",
       "      <th>pressure</th>\n",
       "      <th>thickness</th>\n",
       "      <th>insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>pedigree</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>3.845052</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>3.369578</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         pregnant      plasma    pressure   thickness     insulin         BMI  \\\n",
       "count  768.000000  768.000000  768.000000  768.000000  768.000000  768.000000   \n",
       "mean     3.845052  120.894531   69.105469   20.536458   79.799479   31.992578   \n",
       "std      3.369578   31.972618   19.355807   15.952218  115.244002    7.884160   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      1.000000   99.000000   62.000000    0.000000    0.000000   27.300000   \n",
       "50%      3.000000  117.000000   72.000000   23.000000   30.500000   32.000000   \n",
       "75%      6.000000  140.250000   80.000000   32.000000  127.250000   36.600000   \n",
       "max     17.000000  199.000000  122.000000   99.000000  846.000000   67.100000   \n",
       "\n",
       "         pedigree         age       class  \n",
       "count  768.000000  768.000000  768.000000  \n",
       "mean     0.471876   33.240885    0.348958  \n",
       "std      0.331329   11.760232    0.476951  \n",
       "min      0.078000   21.000000    0.000000  \n",
       "25%      0.243750   24.000000    0.000000  \n",
       "50%      0.372500   29.000000    0.000000  \n",
       "75%      0.626250   41.000000    1.000000  \n",
       "max      2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colormap = plt.cm.gist_heat\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.heatmap(df.corr(), linewidths = 0.1 , vmax=0.5, cmap=colormap, linecolor = 'white', annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid = sns.FacetGrid(df, col='class')\n",
    "grid.map(plt.hist, 'plasma', bins = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnant</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.342342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.214815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.184466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.338235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.368421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pregnant     class\n",
       "0          0  0.342342\n",
       "1          1  0.214815\n",
       "2          2  0.184466\n",
       "3          3  0.360000\n",
       "4          4  0.338235\n",
       "5          5  0.368421\n",
       "6          6  0.320000\n",
       "7          7  0.555556\n",
       "8          8  0.578947\n",
       "9          9  0.642857\n",
       "10        10  0.416667\n",
       "11        11  0.636364\n",
       "12        12  0.444444\n",
       "13        13  0.500000\n",
       "14        14  1.000000\n",
       "15        15  1.000000\n",
       "16        17  1.000000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 임신회수당 당뇨병 발병 확률을 구하시오 ( 데이터 요약 기능)\n",
    "\n",
    "df[['pregnant','class']].groupby(['pregnant'], as_index = False).mean().sort_values(by='pregnant' , ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy \n",
    "import tensorflow as tf \n",
    "numpy.random.seed(3)\n",
    "tf.random.set_seed(3)\n",
    "dataset = numpy.loadtxt(\"./dataset/pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "X = dataset[:,0:8] # 배열식 iloc, loc\n",
    "Y = dataset[: , 8]\n",
    "model = Sequential()\n",
    "model.add(Dense(12,input_dim=8, activation='relu')) #8 x 12\n",
    "model.add(Dense(8, activation='relu')) # 12 x 8\n",
    "model.add(Dense(1, activation='sigmoid')) # 8 x 1 \n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X,Y,epochs=100,batch_size=10)\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, show_shapes = True, to_file='network.png')\n",
    "model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 823.25x720 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "150/150 [==============================] - 0s 550us/step - loss: 1.3259 - accuracy: 0.3333\n",
      "Epoch 2/50\n",
      "150/150 [==============================] - 0s 547us/step - loss: 0.8409 - accuracy: 0.6333\n",
      "Epoch 3/50\n",
      "150/150 [==============================] - 0s 543us/step - loss: 0.7110 - accuracy: 0.7267\n",
      "Epoch 4/50\n",
      "150/150 [==============================] - 0s 536us/step - loss: 0.6229 - accuracy: 0.6867\n",
      "Epoch 5/50\n",
      "150/150 [==============================] - 0s 522us/step - loss: 0.5542 - accuracy: 0.7933\n",
      "Epoch 6/50\n",
      "150/150 [==============================] - 0s 530us/step - loss: 0.4988 - accuracy: 0.9067\n",
      "Epoch 7/50\n",
      "150/150 [==============================] - 0s 539us/step - loss: 0.4562 - accuracy: 0.9133\n",
      "Epoch 8/50\n",
      "150/150 [==============================] - 0s 540us/step - loss: 0.4287 - accuracy: 0.9067\n",
      "Epoch 9/50\n",
      "150/150 [==============================] - 0s 549us/step - loss: 0.4006 - accuracy: 0.9267\n",
      "Epoch 10/50\n",
      "150/150 [==============================] - 0s 532us/step - loss: 0.3773 - accuracy: 0.9733\n",
      "Epoch 11/50\n",
      "150/150 [==============================] - 0s 541us/step - loss: 0.3569 - accuracy: 0.9267\n",
      "Epoch 12/50\n",
      "150/150 [==============================] - 0s 542us/step - loss: 0.3367 - accuracy: 0.9333\n",
      "Epoch 13/50\n",
      "150/150 [==============================] - 0s 453us/step - loss: 0.3272 - accuracy: 0.9600\n",
      "Epoch 14/50\n",
      "150/150 [==============================] - 0s 437us/step - loss: 0.3060 - accuracy: 0.9667\n",
      "Epoch 15/50\n",
      "150/150 [==============================] - 0s 554us/step - loss: 0.2920 - accuracy: 0.9667\n",
      "Epoch 16/50\n",
      "150/150 [==============================] - 0s 545us/step - loss: 0.2784 - accuracy: 0.9600\n",
      "Epoch 17/50\n",
      "150/150 [==============================] - 0s 554us/step - loss: 0.2697 - accuracy: 0.9600\n",
      "Epoch 18/50\n",
      "150/150 [==============================] - 0s 544us/step - loss: 0.2639 - accuracy: 0.9533\n",
      "Epoch 19/50\n",
      "150/150 [==============================] - 0s 527us/step - loss: 0.2473 - accuracy: 0.9733\n",
      "Epoch 20/50\n",
      "150/150 [==============================] - 0s 546us/step - loss: 0.2359 - accuracy: 0.9733\n",
      "Epoch 21/50\n",
      "150/150 [==============================] - 0s 428us/step - loss: 0.2274 - accuracy: 0.9733\n",
      "Epoch 22/50\n",
      "150/150 [==============================] - 0s 471us/step - loss: 0.2225 - accuracy: 0.9733\n",
      "Epoch 23/50\n",
      "150/150 [==============================] - 0s 564us/step - loss: 0.2106 - accuracy: 0.9733\n",
      "Epoch 24/50\n",
      "150/150 [==============================] - 0s 534us/step - loss: 0.2062 - accuracy: 0.9667\n",
      "Epoch 25/50\n",
      "150/150 [==============================] - 0s 556us/step - loss: 0.1939 - accuracy: 0.9800\n",
      "Epoch 26/50\n",
      "150/150 [==============================] - 0s 558us/step - loss: 0.1896 - accuracy: 0.9733\n",
      "Epoch 27/50\n",
      "150/150 [==============================] - 0s 539us/step - loss: 0.1832 - accuracy: 0.9667\n",
      "Epoch 28/50\n",
      "150/150 [==============================] - 0s 540us/step - loss: 0.1683 - accuracy: 0.9800\n",
      "Epoch 29/50\n",
      "150/150 [==============================] - 0s 459us/step - loss: 0.1714 - accuracy: 0.9667\n",
      "Epoch 30/50\n",
      "150/150 [==============================] - 0s 428us/step - loss: 0.1693 - accuracy: 0.9667\n",
      "Epoch 31/50\n",
      "150/150 [==============================] - 0s 423us/step - loss: 0.1619 - accuracy: 0.9733\n",
      "Epoch 32/50\n",
      "150/150 [==============================] - 0s 452us/step - loss: 0.1555 - accuracy: 0.9667\n",
      "Epoch 33/50\n",
      "150/150 [==============================] - 0s 575us/step - loss: 0.1515 - accuracy: 0.9800\n",
      "Epoch 34/50\n",
      "150/150 [==============================] - 0s 555us/step - loss: 0.1458 - accuracy: 0.9667\n",
      "Epoch 35/50\n",
      "150/150 [==============================] - 0s 540us/step - loss: 0.1427 - accuracy: 0.9733\n",
      "Epoch 36/50\n",
      "150/150 [==============================] - 0s 550us/step - loss: 0.1421 - accuracy: 0.9667\n",
      "Epoch 37/50\n",
      "150/150 [==============================] - 0s 579us/step - loss: 0.1321 - accuracy: 0.9667\n",
      "Epoch 38/50\n",
      "150/150 [==============================] - 0s 543us/step - loss: 0.1348 - accuracy: 0.9667\n",
      "Epoch 39/50\n",
      "150/150 [==============================] - 0s 524us/step - loss: 0.1274 - accuracy: 0.9800\n",
      "Epoch 40/50\n",
      "150/150 [==============================] - 0s 469us/step - loss: 0.1313 - accuracy: 0.9667\n",
      "Epoch 41/50\n",
      "150/150 [==============================] - 0s 424us/step - loss: 0.1227 - accuracy: 0.9733\n",
      "Epoch 42/50\n",
      "150/150 [==============================] - 0s 414us/step - loss: 0.1212 - accuracy: 0.9733\n",
      "Epoch 43/50\n",
      "150/150 [==============================] - 0s 411us/step - loss: 0.1201 - accuracy: 0.9800\n",
      "Epoch 44/50\n",
      "150/150 [==============================] - 0s 417us/step - loss: 0.1156 - accuracy: 0.9667\n",
      "Epoch 45/50\n",
      "150/150 [==============================] - 0s 417us/step - loss: 0.1205 - accuracy: 0.9600\n",
      "Epoch 46/50\n",
      "150/150 [==============================] - 0s 413us/step - loss: 0.1142 - accuracy: 0.9667\n",
      "Epoch 47/50\n",
      "150/150 [==============================] - 0s 410us/step - loss: 0.1148 - accuracy: 0.9667\n",
      "Epoch 48/50\n",
      "150/150 [==============================] - 0s 427us/step - loss: 0.1099 - accuracy: 0.9667\n",
      "Epoch 49/50\n",
      "150/150 [==============================] - 0s 419us/step - loss: 0.1084 - accuracy: 0.9667\n",
      "Epoch 50/50\n",
      "150/150 [==============================] - 0s 427us/step - loss: 0.1112 - accuracy: 0.9533\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.1024 - accuracy: 0.9733\n",
      "\n",
      " Accuracy: 0.9733\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.1024 - accuracy: 0.9733\n",
      "\n",
      " loss : 0.1024\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "\n",
    "# callbacks # 시스템이 call 하는 함수 > 이벤트가 벌어지면 실행\n",
    "# accuracy 가 나빠지는 방향으로 변화가 일어 날때 중지\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint , EarlyStopping\n",
    "\n",
    "\n",
    "np.random.seed(3)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "\n",
    "df = pd.read_csv('./dataset/iris.csv', names=[\"sepal_length\",'sepal_sidth','petal_length',\"petal_width\", \"species\"])\n",
    "\n",
    "sns.pairplot(df, hue='species')\n",
    "plt.show()\n",
    "\n",
    "dataset = df.values\n",
    "X =dataset[:,0:4].astype(float)\n",
    "Y =dataset[:,4]\n",
    "\n",
    "e= LabelEncoder()\n",
    "e.fit(Y) #범주형데이터\n",
    "Y = e.transform(Y) # 원래 데이터를 변형 > 0,1,2,3\n",
    "\n",
    "Y_encoded = tf.keras.utils.to_categorical(Y) # one hot encoding \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 독립변수 (4 x 16) + 16 > 데이터간의 간격을 넓힌다\n",
    "model.add(Dense(16, input_dim= 4 , activation='relu'))\n",
    "# (? x 16)  16 x 3  > ? x 3 => one hot encoding\n",
    "model.add(Dense(3, activation='softmax')) # 확률값으로 변경\n",
    "\n",
    "# 모델 컴파일 \n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # 비용값 , rmsprop + momentum\n",
    "\n",
    "# 모델실행 \n",
    "\n",
    "history = model.fit(X,Y_encoded, epochs=50 , batch_size = 1) # 50 x 150 \n",
    "\n",
    "# 결과출력 \n",
    "print('\\n Accuracy: %.4f' % (model.evaluate(X,Y_encoded)[1])) \n",
    "print('\\n loss : %.4f' % (model.evaluate(X,Y_encoded)[0]))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문제 \n",
    "- 열별 데이터 정보를 출력하시오\n",
    "- species 를 기준으로 각열의 평균을 출력하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_sidth</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>species</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>5.006</td>\n",
       "      <td>3.418</td>\n",
       "      <td>1.464</td>\n",
       "      <td>0.244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Iris-versicolor</td>\n",
       "      <td>5.936</td>\n",
       "      <td>2.770</td>\n",
       "      <td>4.260</td>\n",
       "      <td>1.326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>6.588</td>\n",
       "      <td>2.974</td>\n",
       "      <td>5.552</td>\n",
       "      <td>2.026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 sepal_length  sepal_sidth  petal_length  petal_width\n",
       "species                                                              \n",
       "Iris-setosa             5.006        3.418         1.464        0.244\n",
       "Iris-versicolor         5.936        2.770         4.260        1.326\n",
       "Iris-virginica          6.588        2.974         5.552        2.026"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()\n",
    "df.groupby(['species']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_sidth</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_sidth  petal_length  petal_width      species\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_loss = history.history['loss']\n",
    "training_accuracy = history.history['accuracy']\n",
    "epoch_count = range(1, len(training_loss) + 1 )\n",
    "plt.plot(epoch_count, training_loss,training_accuracy , 'r--')\n",
    "plt.legend(['Training Loss' , 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "29/29 [==============================] - 0s 585us/step - loss: 0.2468 - accuracy: 0.5586\n",
      "Epoch 2/200\n",
      "29/29 [==============================] - 0s 570us/step - loss: 0.2330 - accuracy: 0.5724\n",
      "Epoch 3/200\n",
      "29/29 [==============================] - 0s 608us/step - loss: 0.2242 - accuracy: 0.6414\n",
      "Epoch 4/200\n",
      "29/29 [==============================] - 0s 580us/step - loss: 0.2174 - accuracy: 0.7034\n",
      "Epoch 5/200\n",
      "29/29 [==============================] - 0s 622us/step - loss: 0.2126 - accuracy: 0.6828\n",
      "Epoch 6/200\n",
      "29/29 [==============================] - 0s 633us/step - loss: 0.2059 - accuracy: 0.7034\n",
      "Epoch 7/200\n",
      "29/29 [==============================] - 0s 627us/step - loss: 0.1925 - accuracy: 0.7310\n",
      "Epoch 8/200\n",
      "29/29 [==============================] - 0s 602us/step - loss: 0.1866 - accuracy: 0.7517\n",
      "Epoch 9/200\n",
      "29/29 [==============================] - 0s 620us/step - loss: 0.1810 - accuracy: 0.7310\n",
      "Epoch 10/200\n",
      "29/29 [==============================] - 0s 632us/step - loss: 0.1731 - accuracy: 0.7586\n",
      "Epoch 11/200\n",
      "29/29 [==============================] - 0s 613us/step - loss: 0.1632 - accuracy: 0.7793\n",
      "Epoch 12/200\n",
      "29/29 [==============================] - 0s 643us/step - loss: 0.1617 - accuracy: 0.7793\n",
      "Epoch 13/200\n",
      "29/29 [==============================] - 0s 644us/step - loss: 0.1543 - accuracy: 0.8000\n",
      "Epoch 14/200\n",
      "29/29 [==============================] - 0s 629us/step - loss: 0.1504 - accuracy: 0.7931\n",
      "Epoch 15/200\n",
      "29/29 [==============================] - 0s 649us/step - loss: 0.1447 - accuracy: 0.8414\n",
      "Epoch 16/200\n",
      "29/29 [==============================] - 0s 624us/step - loss: 0.1453 - accuracy: 0.8207\n",
      "Epoch 17/200\n",
      "29/29 [==============================] - 0s 603us/step - loss: 0.1388 - accuracy: 0.8207\n",
      "Epoch 18/200\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.1366 - accuracy: 0.8069\n",
      "Epoch 19/200\n",
      "29/29 [==============================] - 0s 621us/step - loss: 0.1334 - accuracy: 0.8138\n",
      "Epoch 20/200\n",
      "29/29 [==============================] - 0s 663us/step - loss: 0.1314 - accuracy: 0.8276\n",
      "Epoch 21/200\n",
      "29/29 [==============================] - 0s 598us/step - loss: 0.1296 - accuracy: 0.8276\n",
      "Epoch 22/200\n",
      "29/29 [==============================] - 0s 580us/step - loss: 0.1316 - accuracy: 0.8138\n",
      "Epoch 23/200\n",
      "29/29 [==============================] - 0s 617us/step - loss: 0.1399 - accuracy: 0.7931\n",
      "Epoch 24/200\n",
      "29/29 [==============================] - 0s 624us/step - loss: 0.1237 - accuracy: 0.8345\n",
      "Epoch 25/200\n",
      "29/29 [==============================] - 0s 592us/step - loss: 0.1205 - accuracy: 0.8552\n",
      "Epoch 26/200\n",
      "29/29 [==============================] - 0s 629us/step - loss: 0.1190 - accuracy: 0.8621\n",
      "Epoch 27/200\n",
      "29/29 [==============================] - 0s 686us/step - loss: 0.1158 - accuracy: 0.8552\n",
      "Epoch 28/200\n",
      "29/29 [==============================] - 0s 670us/step - loss: 0.1210 - accuracy: 0.8483\n",
      "Epoch 29/200\n",
      "29/29 [==============================] - 0s 628us/step - loss: 0.1129 - accuracy: 0.8759\n",
      "Epoch 30/200\n",
      "29/29 [==============================] - 0s 640us/step - loss: 0.1143 - accuracy: 0.8483\n",
      "Epoch 31/200\n",
      "29/29 [==============================] - 0s 608us/step - loss: 0.1112 - accuracy: 0.8690\n",
      "Epoch 32/200\n",
      "29/29 [==============================] - 0s 575us/step - loss: 0.1076 - accuracy: 0.8828\n",
      "Epoch 33/200\n",
      "29/29 [==============================] - 0s 618us/step - loss: 0.1059 - accuracy: 0.8759\n",
      "Epoch 34/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 0.1066 - accuracy: 0.8552\n",
      "Epoch 35/200\n",
      "29/29 [==============================] - 0s 702us/step - loss: 0.1012 - accuracy: 0.8966\n",
      "Epoch 36/200\n",
      "29/29 [==============================] - 0s 673us/step - loss: 0.1046 - accuracy: 0.8759\n",
      "Epoch 37/200\n",
      "29/29 [==============================] - 0s 680us/step - loss: 0.0999 - accuracy: 0.9103\n",
      "Epoch 38/200\n",
      "29/29 [==============================] - 0s 711us/step - loss: 0.1025 - accuracy: 0.8621\n",
      "Epoch 39/200\n",
      "29/29 [==============================] - 0s 705us/step - loss: 0.0964 - accuracy: 0.8759\n",
      "Epoch 40/200\n",
      "29/29 [==============================] - 0s 694us/step - loss: 0.0929 - accuracy: 0.8897\n",
      "Epoch 41/200\n",
      "29/29 [==============================] - 0s 679us/step - loss: 0.0949 - accuracy: 0.8828\n",
      "Epoch 42/200\n",
      "29/29 [==============================] - 0s 668us/step - loss: 0.0887 - accuracy: 0.8897\n",
      "Epoch 43/200\n",
      "29/29 [==============================] - 0s 649us/step - loss: 0.0875 - accuracy: 0.9172\n",
      "Epoch 44/200\n",
      "29/29 [==============================] - 0s 647us/step - loss: 0.0851 - accuracy: 0.9034\n",
      "Epoch 45/200\n",
      "29/29 [==============================] - 0s 713us/step - loss: 0.0853 - accuracy: 0.9103\n",
      "Epoch 46/200\n",
      "29/29 [==============================] - 0s 633us/step - loss: 0.0818 - accuracy: 0.9241\n",
      "Epoch 47/200\n",
      "29/29 [==============================] - 0s 689us/step - loss: 0.0787 - accuracy: 0.9241\n",
      "Epoch 48/200\n",
      "29/29 [==============================] - 0s 725us/step - loss: 0.0810 - accuracy: 0.9172\n",
      "Epoch 49/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0750 - accuracy: 0.9310\n",
      "Epoch 50/200\n",
      "29/29 [==============================] - 0s 691us/step - loss: 0.0766 - accuracy: 0.9241\n",
      "Epoch 51/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0763 - accuracy: 0.9310\n",
      "Epoch 52/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 0.0743 - accuracy: 0.9172\n",
      "Epoch 53/200\n",
      "29/29 [==============================] - 0s 682us/step - loss: 0.0724 - accuracy: 0.9310\n",
      "Epoch 54/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0669 - accuracy: 0.9448\n",
      "Epoch 55/200\n",
      "29/29 [==============================] - 0s 673us/step - loss: 0.0647 - accuracy: 0.9586\n",
      "Epoch 56/200\n",
      "29/29 [==============================] - 0s 683us/step - loss: 0.0645 - accuracy: 0.9517\n",
      "Epoch 57/200\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.0654 - accuracy: 0.9310\n",
      "Epoch 58/200\n",
      "29/29 [==============================] - 0s 722us/step - loss: 0.0615 - accuracy: 0.9586\n",
      "Epoch 59/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0633 - accuracy: 0.9517\n",
      "Epoch 60/200\n",
      "29/29 [==============================] - 0s 675us/step - loss: 0.0589 - accuracy: 0.9517\n",
      "Epoch 61/200\n",
      "29/29 [==============================] - 0s 661us/step - loss: 0.0604 - accuracy: 0.9310\n",
      "Epoch 62/200\n",
      "29/29 [==============================] - 0s 666us/step - loss: 0.0565 - accuracy: 0.9586\n",
      "Epoch 63/200\n",
      "29/29 [==============================] - 0s 646us/step - loss: 0.0572 - accuracy: 0.9517\n",
      "Epoch 64/200\n",
      "29/29 [==============================] - 0s 645us/step - loss: 0.0517 - accuracy: 0.9724\n",
      "Epoch 65/200\n",
      "29/29 [==============================] - 0s 670us/step - loss: 0.0545 - accuracy: 0.9517\n",
      "Epoch 66/200\n",
      "29/29 [==============================] - 0s 606us/step - loss: 0.0491 - accuracy: 0.9655\n",
      "Epoch 67/200\n",
      "29/29 [==============================] - 0s 622us/step - loss: 0.0475 - accuracy: 0.9655\n",
      "Epoch 68/200\n",
      "29/29 [==============================] - 0s 668us/step - loss: 0.0460 - accuracy: 0.9724\n",
      "Epoch 69/200\n",
      "29/29 [==============================] - 0s 704us/step - loss: 0.0438 - accuracy: 0.9724\n",
      "Epoch 70/200\n",
      "29/29 [==============================] - 0s 682us/step - loss: 0.0422 - accuracy: 0.9586\n",
      "Epoch 71/200\n",
      "29/29 [==============================] - 0s 684us/step - loss: 0.0444 - accuracy: 0.9793\n",
      "Epoch 72/200\n",
      "29/29 [==============================] - 0s 644us/step - loss: 0.0403 - accuracy: 0.9655\n",
      "Epoch 73/200\n",
      "29/29 [==============================] - 0s 670us/step - loss: 0.0389 - accuracy: 0.9724\n",
      "Epoch 74/200\n",
      "29/29 [==============================] - 0s 701us/step - loss: 0.0390 - accuracy: 0.9655\n",
      "Epoch 75/200\n",
      "29/29 [==============================] - 0s 648us/step - loss: 0.0366 - accuracy: 0.9793\n",
      "Epoch 76/200\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.0366 - accuracy: 0.9724\n",
      "Epoch 77/200\n",
      "29/29 [==============================] - 0s 654us/step - loss: 0.0449 - accuracy: 0.9517\n",
      "Epoch 78/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0363 - accuracy: 0.9586\n",
      "Epoch 79/200\n",
      "29/29 [==============================] - 0s 679us/step - loss: 0.0346 - accuracy: 0.9793\n",
      "Epoch 80/200\n",
      "29/29 [==============================] - 0s 678us/step - loss: 0.0352 - accuracy: 0.9724\n",
      "Epoch 81/200\n",
      "29/29 [==============================] - 0s 676us/step - loss: 0.0343 - accuracy: 0.9724\n",
      "Epoch 82/200\n",
      "29/29 [==============================] - 0s 697us/step - loss: 0.0302 - accuracy: 0.9793\n",
      "Epoch 83/200\n",
      "29/29 [==============================] - 0s 716us/step - loss: 0.0297 - accuracy: 0.9793\n",
      "Epoch 84/200\n",
      "29/29 [==============================] - 0s 736us/step - loss: 0.0271 - accuracy: 0.9862\n",
      "Epoch 85/200\n",
      "29/29 [==============================] - 0s 659us/step - loss: 0.0269 - accuracy: 0.9862\n",
      "Epoch 86/200\n",
      "29/29 [==============================] - 0s 626us/step - loss: 0.0261 - accuracy: 0.9793\n",
      "Epoch 87/200\n",
      "29/29 [==============================] - 0s 711us/step - loss: 0.0245 - accuracy: 0.9862\n",
      "Epoch 88/200\n",
      "29/29 [==============================] - 0s 708us/step - loss: 0.0240 - accuracy: 0.9862\n",
      "Epoch 89/200\n",
      "29/29 [==============================] - 0s 719us/step - loss: 0.0229 - accuracy: 0.9862\n",
      "Epoch 90/200\n",
      "29/29 [==============================] - 0s 680us/step - loss: 0.0231 - accuracy: 0.9862\n",
      "Epoch 91/200\n",
      "29/29 [==============================] - 0s 645us/step - loss: 0.0213 - accuracy: 0.9862\n",
      "Epoch 92/200\n",
      "29/29 [==============================] - 0s 669us/step - loss: 0.0240 - accuracy: 0.9862\n",
      "Epoch 93/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.0200 - accuracy: 0.9793\n",
      "Epoch 94/200\n",
      "29/29 [==============================] - 0s 718us/step - loss: 0.0219 - accuracy: 0.9862\n",
      "Epoch 95/200\n",
      "29/29 [==============================] - 0s 698us/step - loss: 0.0197 - accuracy: 0.9862\n",
      "Epoch 96/200\n",
      "29/29 [==============================] - 0s 680us/step - loss: 0.0215 - accuracy: 0.9862\n",
      "Epoch 97/200\n",
      "29/29 [==============================] - 0s 662us/step - loss: 0.0172 - accuracy: 0.9862\n",
      "Epoch 98/200\n",
      "29/29 [==============================] - 0s 601us/step - loss: 0.0210 - accuracy: 0.9793\n",
      "Epoch 99/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0209 - accuracy: 0.9793\n",
      "Epoch 100/200\n",
      "29/29 [==============================] - 0s 630us/step - loss: 0.0215 - accuracy: 0.9862\n",
      "Epoch 101/200\n",
      "29/29 [==============================] - 0s 648us/step - loss: 0.0168 - accuracy: 0.9931\n",
      "Epoch 102/200\n",
      "29/29 [==============================] - 0s 666us/step - loss: 0.0200 - accuracy: 0.9862\n",
      "Epoch 103/200\n",
      "29/29 [==============================] - 0s 705us/step - loss: 0.0194 - accuracy: 0.9862\n",
      "Epoch 104/200\n",
      "29/29 [==============================] - 0s 680us/step - loss: 0.0136 - accuracy: 0.9862\n",
      "Epoch 105/200\n",
      "29/29 [==============================] - 0s 699us/step - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "29/29 [==============================] - 0s 664us/step - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "29/29 [==============================] - 0s 644us/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "29/29 [==============================] - 0s 679us/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "29/29 [==============================] - 0s 685us/step - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "29/29 [==============================] - 0s 646us/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "29/29 [==============================] - 0s 650us/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "29/29 [==============================] - 0s 717us/step - loss: 0.0143 - accuracy: 0.9931\n",
      "Epoch 114/200\n",
      "29/29 [==============================] - 0s 587us/step - loss: 0.0144 - accuracy: 0.9931\n",
      "Epoch 115/200\n",
      "29/29 [==============================] - 0s 520us/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "29/29 [==============================] - 0s 632us/step - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "29/29 [==============================] - 0s 706us/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "29/29 [==============================] - 0s 679us/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "29/29 [==============================] - 0s 648us/step - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "29/29 [==============================] - 0s 664us/step - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "29/29 [==============================] - 0s 681us/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "29/29 [==============================] - 0s 654us/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "29/29 [==============================] - 0s 718us/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "29/29 [==============================] - 0s 664us/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "29/29 [==============================] - 0s 720us/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "29/29 [==============================] - 0s 683us/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "29/29 [==============================] - 0s 672us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "29/29 [==============================] - 0s 649us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "29/29 [==============================] - 0s 697us/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "29/29 [==============================] - 0s 654us/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "29/29 [==============================] - 0s 676us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "29/29 [==============================] - 0s 743us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "29/29 [==============================] - 0s 714us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "29/29 [==============================] - 0s 649us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "29/29 [==============================] - 0s 625us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "29/29 [==============================] - 0s 640us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "29/29 [==============================] - 0s 695us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "29/29 [==============================] - 0s 663us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "29/29 [==============================] - 0s 656us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "29/29 [==============================] - 0s 678us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "29/29 [==============================] - 0s 644us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "29/29 [==============================] - 0s 605us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "29/29 [==============================] - 0s 676us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "29/29 [==============================] - 0s 656us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "29/29 [==============================] - 0s 648us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "29/29 [==============================] - 0s 666us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "29/29 [==============================] - 0s 682us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "29/29 [==============================] - 0s 722us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "29/29 [==============================] - 0s 678us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "29/29 [==============================] - 0s 640us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "29/29 [==============================] - 0s 687us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "29/29 [==============================] - 0s 661us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "29/29 [==============================] - 0s 714us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "29/29 [==============================] - 0s 674us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "29/29 [==============================] - 0s 711us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 711us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "29/29 [==============================] - 0s 695us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "29/29 [==============================] - 0s 676us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "29/29 [==============================] - 0s 694us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "29/29 [==============================] - 0s 697us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "29/29 [==============================] - 0s 675us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "29/29 [==============================] - 0s 643us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "29/29 [==============================] - 0s 648us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "29/29 [==============================] - 0s 701us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "29/29 [==============================] - 0s 643us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "29/29 [==============================] - 0s 674us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "29/29 [==============================] - 0s 657us/step - loss: 9.8150e-04 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "29/29 [==============================] - 0s 632us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "29/29 [==============================] - 0s 721us/step - loss: 9.2664e-04 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 9.0903e-04 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "29/29 [==============================] - 0s 743us/step - loss: 9.1012e-04 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 9.0154e-04 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "29/29 [==============================] - 0s 701us/step - loss: 9.0238e-04 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "29/29 [==============================] - 0s 689us/step - loss: 7.7795e-04 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "29/29 [==============================] - 0s 673us/step - loss: 7.6611e-04 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "29/29 [==============================] - 0s 659us/step - loss: 7.3659e-04 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "29/29 [==============================] - 0s 660us/step - loss: 7.5185e-04 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "29/29 [==============================] - 0s 716us/step - loss: 7.4481e-04 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "29/29 [==============================] - 0s 712us/step - loss: 7.1019e-04 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "29/29 [==============================] - 0s 676us/step - loss: 6.5110e-04 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "29/29 [==============================] - 0s 683us/step - loss: 7.0923e-04 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "29/29 [==============================] - 0s 647us/step - loss: 6.4145e-04 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "29/29 [==============================] - 0s 578us/step - loss: 6.6950e-04 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "29/29 [==============================] - 0s 631us/step - loss: 5.9872e-04 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "29/29 [==============================] - 0s 726us/step - loss: 6.0339e-04 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "29/29 [==============================] - 0s 686us/step - loss: 5.9942e-04 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "29/29 [==============================] - 0s 722us/step - loss: 5.5352e-04 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "29/29 [==============================] - 0s 761us/step - loss: 5.7512e-04 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "29/29 [==============================] - 0s 722us/step - loss: 5.8473e-04 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "29/29 [==============================] - ETA: 0s - loss: 3.0167e-04 - accuracy: 1.00 - 0s 721us/step - loss: 5.2940e-04 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "29/29 [==============================] - 0s 727us/step - loss: 5.0734e-04 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "29/29 [==============================] - 0s 720us/step - loss: 4.9229e-04 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "29/29 [==============================] - 0s 722us/step - loss: 4.6931e-04 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 4.7015e-04 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "29/29 [==============================] - 0s 755us/step - loss: 4.4436e-04 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 4.6188e-04 - accuracy: 1.0000\n",
      "2/2 [==============================] - 0s 498us/step - loss: 0.2021 - accuracy: 0.7460\n",
      "\n",
      " accuracy : 0.7460\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "numpy.random.seed(3)\n",
    "\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "df = pd.read_csv('./dataset/sonar.csv', header=None)\n",
    "\n",
    "dataset = df.values\n",
    "\n",
    "X = dataset[:, 0:60] # 0 ~ 59 독립변수\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X = np.asarray(X).astype(np.float32)\n",
    "\n",
    "Y_obj = dataset[:,60]\n",
    "\n",
    "e = LabelEncoder()\n",
    "\n",
    "e.fit(Y_obj)\n",
    "\n",
    "Y = e.transform(Y_obj)\n",
    "# train data 에 과하게 적합 되었을 때\n",
    " \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "\n",
    "X, Y, test_size=0.3, random_state=3)\n",
    "\n",
    " \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(24, input_dim=60, activation='relu')) # 60 * 24 + 24\n",
    "\n",
    "model.add(Dense(10, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "\n",
    "             optimizer='adam',\n",
    "\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=200, batch_size=5)\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model.save('my_model.h5')\n",
    "del model\n",
    "model = load_model('my_model.h5')\n",
    "\n",
    "print('\\n accuracy : %.4f' % (model.evaluate(X_test,Y_test)[1]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['R', 'M'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[60].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias - variance trade off "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " 1/29 [>.............................] - ETA: 0s - loss: 0.2481 - accuracy: 0.6000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0011s). Check your callbacks.\n",
      "29/29 [==============================] - 0s 580us/step - loss: 0.2513 - accuracy: 0.5310\n",
      "Epoch 2/200\n",
      "29/29 [==============================] - 0s 635us/step - loss: 0.2390 - accuracy: 0.5517\n",
      "Epoch 3/200\n",
      "29/29 [==============================] - 0s 588us/step - loss: 0.2289 - accuracy: 0.6414\n",
      "Epoch 4/200\n",
      "29/29 [==============================] - 0s 678us/step - loss: 0.2221 - accuracy: 0.6897\n",
      "Epoch 5/200\n",
      "29/29 [==============================] - 0s 663us/step - loss: 0.2177 - accuracy: 0.6690\n",
      "Epoch 6/200\n",
      "29/29 [==============================] - 0s 631us/step - loss: 0.2132 - accuracy: 0.7103\n",
      "Epoch 7/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.2019 - accuracy: 0.7241\n",
      "Epoch 8/200\n",
      "29/29 [==============================] - 0s 638us/step - loss: 0.1970 - accuracy: 0.7172\n",
      "Epoch 9/200\n",
      "29/29 [==============================] - 0s 601us/step - loss: 0.1917 - accuracy: 0.7172\n",
      "Epoch 10/200\n",
      "29/29 [==============================] - 0s 636us/step - loss: 0.1838 - accuracy: 0.7862\n",
      "Epoch 11/200\n",
      "29/29 [==============================] - 0s 620us/step - loss: 0.1762 - accuracy: 0.7448\n",
      "Epoch 12/200\n",
      "29/29 [==============================] - 0s 664us/step - loss: 0.1714 - accuracy: 0.7931\n",
      "Epoch 13/200\n",
      "29/29 [==============================] - 0s 623us/step - loss: 0.1684 - accuracy: 0.7448\n",
      "Epoch 14/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 0.1631 - accuracy: 0.7517\n",
      "Epoch 15/200\n",
      "29/29 [==============================] - 0s 572us/step - loss: 0.1551 - accuracy: 0.7862\n",
      "Epoch 16/200\n",
      "29/29 [==============================] - 0s 586us/step - loss: 0.1551 - accuracy: 0.7793\n",
      "Epoch 17/200\n",
      "29/29 [==============================] - 0s 588us/step - loss: 0.1486 - accuracy: 0.8000\n",
      "Epoch 18/200\n",
      "29/29 [==============================] - 0s 587us/step - loss: 0.1448 - accuracy: 0.7931\n",
      "Epoch 19/200\n",
      "29/29 [==============================] - 0s 629us/step - loss: 0.1406 - accuracy: 0.7862\n",
      "Epoch 20/200\n",
      "29/29 [==============================] - 0s 608us/step - loss: 0.1380 - accuracy: 0.8069\n",
      "Epoch 21/200\n",
      "29/29 [==============================] - 0s 599us/step - loss: 0.1361 - accuracy: 0.8000\n",
      "Epoch 22/200\n",
      "29/29 [==============================] - 0s 612us/step - loss: 0.1373 - accuracy: 0.8000\n",
      "Epoch 23/200\n",
      "29/29 [==============================] - 0s 616us/step - loss: 0.1460 - accuracy: 0.7724\n",
      "Epoch 24/200\n",
      "29/29 [==============================] - 0s 669us/step - loss: 0.1275 - accuracy: 0.8345\n",
      "Epoch 25/200\n",
      "29/29 [==============================] - 0s 661us/step - loss: 0.1252 - accuracy: 0.7931\n",
      "Epoch 26/200\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.1238 - accuracy: 0.8414\n",
      "Epoch 27/200\n",
      "29/29 [==============================] - 0s 700us/step - loss: 0.1189 - accuracy: 0.8483\n",
      "Epoch 28/200\n",
      "29/29 [==============================] - 0s 718us/step - loss: 0.1246 - accuracy: 0.8276\n",
      "Epoch 29/200\n",
      "29/29 [==============================] - 0s 700us/step - loss: 0.1156 - accuracy: 0.8276\n",
      "Epoch 30/200\n",
      "29/29 [==============================] - 0s 675us/step - loss: 0.1160 - accuracy: 0.8552\n",
      "Epoch 31/200\n",
      "29/29 [==============================] - 0s 711us/step - loss: 0.1123 - accuracy: 0.8345\n",
      "Epoch 32/200\n",
      "29/29 [==============================] - 0s 676us/step - loss: 0.1102 - accuracy: 0.8552\n",
      "Epoch 33/200\n",
      "29/29 [==============================] - 0s 719us/step - loss: 0.1084 - accuracy: 0.8690\n",
      "Epoch 34/200\n",
      "29/29 [==============================] - 0s 734us/step - loss: 0.1078 - accuracy: 0.8483\n",
      "Epoch 35/200\n",
      "29/29 [==============================] - 0s 649us/step - loss: 0.1020 - accuracy: 0.8621\n",
      "Epoch 36/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.1048 - accuracy: 0.8552\n",
      "Epoch 37/200\n",
      "29/29 [==============================] - 0s 697us/step - loss: 0.1015 - accuracy: 0.8621\n",
      "Epoch 38/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.1019 - accuracy: 0.8690\n",
      "Epoch 39/200\n",
      "29/29 [==============================] - 0s 749us/step - loss: 0.0956 - accuracy: 0.8759\n",
      "Epoch 40/200\n",
      "29/29 [==============================] - 0s 701us/step - loss: 0.0926 - accuracy: 0.8828\n",
      "Epoch 41/200\n",
      "29/29 [==============================] - 0s 671us/step - loss: 0.0931 - accuracy: 0.8759\n",
      "Epoch 42/200\n",
      "29/29 [==============================] - 0s 684us/step - loss: 0.0896 - accuracy: 0.9103\n",
      "Epoch 43/200\n",
      "29/29 [==============================] - 0s 690us/step - loss: 0.0857 - accuracy: 0.9034\n",
      "Epoch 44/200\n",
      "29/29 [==============================] - 0s 697us/step - loss: 0.0841 - accuracy: 0.8966\n",
      "Epoch 45/200\n",
      "29/29 [==============================] - 0s 681us/step - loss: 0.0865 - accuracy: 0.8966\n",
      "Epoch 46/200\n",
      "29/29 [==============================] - 0s 723us/step - loss: 0.0834 - accuracy: 0.9034\n",
      "Epoch 47/200\n",
      "29/29 [==============================] - 0s 687us/step - loss: 0.0787 - accuracy: 0.9103\n",
      "Epoch 48/200\n",
      "29/29 [==============================] - 0s 715us/step - loss: 0.0812 - accuracy: 0.8897\n",
      "Epoch 49/200\n",
      "29/29 [==============================] - 0s 694us/step - loss: 0.0744 - accuracy: 0.9241\n",
      "Epoch 50/200\n",
      "29/29 [==============================] - 0s 696us/step - loss: 0.0748 - accuracy: 0.9103\n",
      "Epoch 51/200\n",
      "29/29 [==============================] - 0s 674us/step - loss: 0.0749 - accuracy: 0.9103\n",
      "Epoch 52/200\n",
      "29/29 [==============================] - 0s 687us/step - loss: 0.0723 - accuracy: 0.9103\n",
      "Epoch 53/200\n",
      "29/29 [==============================] - 0s 668us/step - loss: 0.0708 - accuracy: 0.9448\n",
      "Epoch 54/200\n",
      "29/29 [==============================] - 0s 654us/step - loss: 0.0695 - accuracy: 0.9241\n",
      "Epoch 55/200\n",
      "29/29 [==============================] - 0s 637us/step - loss: 0.0644 - accuracy: 0.9448\n",
      "Epoch 56/200\n",
      "29/29 [==============================] - 0s 700us/step - loss: 0.0637 - accuracy: 0.9517\n",
      "Epoch 57/200\n",
      "29/29 [==============================] - 0s 638us/step - loss: 0.0623 - accuracy: 0.9379\n",
      "Epoch 58/200\n",
      "29/29 [==============================] - 0s 670us/step - loss: 0.0597 - accuracy: 0.9517\n",
      "Epoch 59/200\n",
      "29/29 [==============================] - 0s 714us/step - loss: 0.0629 - accuracy: 0.9448\n",
      "Epoch 60/200\n",
      "29/29 [==============================] - 0s 682us/step - loss: 0.0569 - accuracy: 0.9655\n",
      "Epoch 61/200\n",
      "29/29 [==============================] - 0s 748us/step - loss: 0.0610 - accuracy: 0.9172\n",
      "Epoch 62/200\n",
      "29/29 [==============================] - 0s 678us/step - loss: 0.0588 - accuracy: 0.9586\n",
      "Epoch 63/200\n",
      "29/29 [==============================] - 0s 746us/step - loss: 0.0592 - accuracy: 0.9310\n",
      "Epoch 64/200\n",
      "29/29 [==============================] - 0s 670us/step - loss: 0.0533 - accuracy: 0.9448\n",
      "Epoch 65/200\n",
      "29/29 [==============================] - 0s 668us/step - loss: 0.0552 - accuracy: 0.9241\n",
      "Epoch 66/200\n",
      "29/29 [==============================] - 0s 643us/step - loss: 0.0505 - accuracy: 0.9586\n",
      "Epoch 67/200\n",
      "29/29 [==============================] - 0s 648us/step - loss: 0.0476 - accuracy: 0.9517\n",
      "Epoch 68/200\n",
      "29/29 [==============================] - 0s 643us/step - loss: 0.0470 - accuracy: 0.9793\n",
      "Epoch 69/200\n",
      "29/29 [==============================] - 0s 697us/step - loss: 0.0461 - accuracy: 0.9655\n",
      "Epoch 70/200\n",
      "29/29 [==============================] - 0s 692us/step - loss: 0.0447 - accuracy: 0.9724\n",
      "Epoch 71/200\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.0461 - accuracy: 0.9586\n",
      "Epoch 72/200\n",
      "29/29 [==============================] - 0s 666us/step - loss: 0.0421 - accuracy: 0.9586\n",
      "Epoch 73/200\n",
      "29/29 [==============================] - 0s 681us/step - loss: 0.0388 - accuracy: 0.9793\n",
      "Epoch 74/200\n",
      "29/29 [==============================] - 0s 694us/step - loss: 0.0401 - accuracy: 0.9586\n",
      "Epoch 75/200\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.0393 - accuracy: 0.9724\n",
      "Epoch 76/200\n",
      "29/29 [==============================] - 0s 622us/step - loss: 0.0370 - accuracy: 0.9655\n",
      "Epoch 77/200\n",
      "29/29 [==============================] - 0s 671us/step - loss: 0.0504 - accuracy: 0.9448\n",
      "Epoch 78/200\n",
      "29/29 [==============================] - 0s 682us/step - loss: 0.0398 - accuracy: 0.9586\n",
      "Epoch 79/200\n",
      "29/29 [==============================] - 0s 660us/step - loss: 0.0384 - accuracy: 0.9724\n",
      "Epoch 80/200\n",
      "29/29 [==============================] - 0s 707us/step - loss: 0.0386 - accuracy: 0.9655\n",
      "Epoch 81/200\n",
      "29/29 [==============================] - 0s 633us/step - loss: 0.0339 - accuracy: 0.9724\n",
      "Epoch 82/200\n",
      "29/29 [==============================] - 0s 654us/step - loss: 0.0320 - accuracy: 0.9655\n",
      "Epoch 83/200\n",
      "29/29 [==============================] - 0s 226us/step - loss: 0.0361 - accuracy: 0.9517\n",
      "Epoch 84/200\n",
      "29/29 [==============================] - 0s 633us/step - loss: 0.0292 - accuracy: 0.9862\n",
      "Epoch 85/200\n",
      "29/29 [==============================] - 0s 626us/step - loss: 0.0293 - accuracy: 0.9862\n",
      "Epoch 86/200\n",
      "29/29 [==============================] - 0s 617us/step - loss: 0.0261 - accuracy: 0.9793\n",
      "Epoch 87/200\n",
      "29/29 [==============================] - 0s 133us/step - loss: 0.0260 - accuracy: 0.9862\n",
      "Epoch 88/200\n",
      "29/29 [==============================] - 0s 621us/step - loss: 0.0268 - accuracy: 0.9724\n",
      "Epoch 89/200\n",
      "29/29 [==============================] - 0s 696us/step - loss: 0.0229 - accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "29/29 [==============================] - 0s 675us/step - loss: 0.0237 - accuracy: 0.9931\n",
      "Epoch 91/200\n",
      "29/29 [==============================] - 0s 676us/step - loss: 0.0214 - accuracy: 0.9931\n",
      "Epoch 92/200\n",
      "29/29 [==============================] - 0s 680us/step - loss: 0.0248 - accuracy: 0.9931\n",
      "Epoch 93/200\n",
      "29/29 [==============================] - 0s 674us/step - loss: 0.0198 - accuracy: 0.9931\n",
      "Epoch 94/200\n",
      "29/29 [==============================] - 0s 692us/step - loss: 0.0223 - accuracy: 0.9862\n",
      "Epoch 95/200\n",
      "29/29 [==============================] - 0s 629us/step - loss: 0.0222 - accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "29/29 [==============================] - 0s 656us/step - loss: 0.0211 - accuracy: 0.9862\n",
      "Epoch 97/200\n",
      "29/29 [==============================] - 0s 223us/step - loss: 0.0188 - accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "29/29 [==============================] - 0s 608us/step - loss: 0.0209 - accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "29/29 [==============================] - 0s 686us/step - loss: 0.0199 - accuracy: 0.9862\n",
      "Epoch 100/200\n",
      "29/29 [==============================] - 0s 683us/step - loss: 0.0207 - accuracy: 0.9931\n",
      "Epoch 101/200\n",
      "29/29 [==============================] - 0s 650us/step - loss: 0.0163 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "29/29 [==============================] - 0s 657us/step - loss: 0.0165 - accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "29/29 [==============================] - 0s 657us/step - loss: 0.0155 - accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "29/29 [==============================] - 0s 694us/step - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "29/29 [==============================] - 0s 632us/step - loss: 0.0126 - accuracy: 0.9931\n",
      "Epoch 106/200\n",
      "29/29 [==============================] - 0s 662us/step - loss: 0.0183 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "29/29 [==============================] - 0s 683us/step - loss: 0.0151 - accuracy: 0.9931\n",
      "Epoch 108/200\n",
      "29/29 [==============================] - 0s 658us/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "29/29 [==============================] - 0s 666us/step - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "29/29 [==============================] - 0s 614us/step - loss: 0.0130 - accuracy: 0.9931\n",
      "Epoch 111/200\n",
      "29/29 [==============================] - 0s 465us/step - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "29/29 [==============================] - 0s 231us/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "29/29 [==============================] - 0s 660us/step - loss: 0.0193 - accuracy: 0.9862\n",
      "Epoch 114/200\n",
      "29/29 [==============================] - 0s 741us/step - loss: 0.0174 - accuracy: 0.9862\n",
      "Epoch 115/200\n",
      "29/29 [==============================] - 0s 631us/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "29/29 [==============================] - 0s 613us/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "29/29 [==============================] - 0s 596us/step - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "29/29 [==============================] - 0s 215us/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "29/29 [==============================] - 0s 775us/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "29/29 [==============================] - 0s 648us/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "29/29 [==============================] - 0s 616us/step - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "29/29 [==============================] - 0s 656us/step - loss: 0.0153 - accuracy: 0.9862\n",
      "Epoch 127/200\n",
      "29/29 [==============================] - 0s 549us/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "29/29 [==============================] - 0s 820us/step - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "29/29 [==============================] - 0s 124us/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "29/29 [==============================] - 0s 78us/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "29/29 [==============================] - 0s 147us/step - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "29/29 [==============================] - 0s 148us/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "29/29 [==============================] - 0s 645us/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "29/29 [==============================] - 0s 809us/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "29/29 [==============================] - 0s 659us/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "29/29 [==============================] - 0s 671us/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "29/29 [==============================] - 0s 626us/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "29/29 [==============================] - 0s 657us/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "29/29 [==============================] - 0s 662us/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "29/29 [==============================] - 0s 621us/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "29/29 [==============================] - 0s 650us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "29/29 [==============================] - 0s 645us/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "29/29 [==============================] - 0s 629us/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "29/29 [==============================] - 0s 662us/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "29/29 [==============================] - 0s 657us/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "29/29 [==============================] - 0s 699us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "29/29 [==============================] - 0s 140us/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "29/29 [==============================] - 0s 616us/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "29/29 [==============================] - 0s 724us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "29/29 [==============================] - 0s 684us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "29/29 [==============================] - 0s 684us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "29/29 [==============================] - 0s 648us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "29/29 [==============================] - 0s 710us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "29/29 [==============================] - 0s 629us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "29/29 [==============================] - 0s 678us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 548us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "29/29 [==============================] - 0s 350us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "29/29 [==============================] - 0s 659us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "29/29 [==============================] - 0s 698us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "29/29 [==============================] - 0s 686us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "29/29 [==============================] - 0s 683us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "29/29 [==============================] - 0s 654us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "29/29 [==============================] - 0s 670us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "29/29 [==============================] - 0s 744us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "29/29 [==============================] - 0s 648us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "29/29 [==============================] - 0s 647us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "29/29 [==============================] - 0s 663us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "29/29 [==============================] - 0s 624us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "29/29 [==============================] - 0s 137us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "29/29 [==============================] - 0s 703us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "29/29 [==============================] - 0s 604us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "29/29 [==============================] - 0s 820us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "29/29 [==============================] - 0s 654us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "29/29 [==============================] - 0s 260us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "29/29 [==============================] - 0s 609us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "29/29 [==============================] - 0s 277us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "29/29 [==============================] - 0s 604us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "29/29 [==============================] - 0s 685us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "29/29 [==============================] - 0s 691us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "29/29 [==============================] - 0s 684us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "29/29 [==============================] - 0s 616us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "29/29 [==============================] - 0s 629us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "29/29 [==============================] - 0s 610us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "29/29 [==============================] - 0s 672us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "29/29 [==============================] - 0s 646us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "29/29 [==============================] - 0s 622us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "29/29 [==============================] - 0s 698us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "29/29 [==============================] - 0s 625us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "29/29 [==============================] - 0s 659us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "29/29 [==============================] - 0s 606us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "29/29 [==============================] - 0s 598us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "29/29 [==============================] - 0s 658us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "29/29 [==============================] - 0s 613us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "29/29 [==============================] - 0s 678us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "29/29 [==============================] - 0s 644us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 9.9154e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001BBF923A3A8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 848us/step - loss: 0.1062 - accuracy: 0.8636\n",
      "Epoch 1/200\n",
      " 1/29 [>.............................] - ETA: 0s - loss: 0.2528 - accuracy: 0.6000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "29/29 [==============================] - 0s 665us/step - loss: 0.2611 - accuracy: 0.5586\n",
      "Epoch 2/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 0.2436 - accuracy: 0.5724\n",
      "Epoch 3/200\n",
      "29/29 [==============================] - 0s 595us/step - loss: 0.2319 - accuracy: 0.6276\n",
      "Epoch 4/200\n",
      "29/29 [==============================] - 0s 385us/step - loss: 0.2228 - accuracy: 0.6897\n",
      "Epoch 5/200\n",
      "29/29 [==============================] - 0s 978us/step - loss: 0.2151 - accuracy: 0.7241\n",
      "Epoch 6/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.2115 - accuracy: 0.7241\n",
      "Epoch 7/200\n",
      "29/29 [==============================] - 0s 486us/step - loss: 0.1972 - accuracy: 0.7379\n",
      "Epoch 8/200\n",
      "29/29 [==============================] - 0s 932us/step - loss: 0.1924 - accuracy: 0.7586\n",
      "Epoch 9/200\n",
      "29/29 [==============================] - 0s 654us/step - loss: 0.1870 - accuracy: 0.7586\n",
      "Epoch 10/200\n",
      "29/29 [==============================] - 0s 657us/step - loss: 0.1777 - accuracy: 0.7862\n",
      "Epoch 11/200\n",
      "29/29 [==============================] - 0s 627us/step - loss: 0.1692 - accuracy: 0.7793\n",
      "Epoch 12/200\n",
      "29/29 [==============================] - 0s 638us/step - loss: 0.1646 - accuracy: 0.8069\n",
      "Epoch 13/200\n",
      "29/29 [==============================] - 0s 605us/step - loss: 0.1603 - accuracy: 0.7724\n",
      "Epoch 14/200\n",
      "29/29 [==============================] - 0s 665us/step - loss: 0.1549 - accuracy: 0.7655\n",
      "Epoch 15/200\n",
      "29/29 [==============================] - 0s 695us/step - loss: 0.1497 - accuracy: 0.8276\n",
      "Epoch 16/200\n",
      "29/29 [==============================] - 0s 667us/step - loss: 0.1493 - accuracy: 0.8138\n",
      "Epoch 17/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.1422 - accuracy: 0.8207\n",
      "Epoch 18/200\n",
      "29/29 [==============================] - 0s 679us/step - loss: 0.1401 - accuracy: 0.8000\n",
      "Epoch 19/200\n",
      "29/29 [==============================] - 0s 611us/step - loss: 0.1358 - accuracy: 0.8414\n",
      "Epoch 20/200\n",
      "29/29 [==============================] - 0s 620us/step - loss: 0.1332 - accuracy: 0.8207\n",
      "Epoch 21/200\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.1316 - accuracy: 0.8276\n",
      "Epoch 22/200\n",
      "29/29 [==============================] - 0s 627us/step - loss: 0.1328 - accuracy: 0.8000\n",
      "Epoch 23/200\n",
      "29/29 [==============================] - 0s 671us/step - loss: 0.1408 - accuracy: 0.8000\n",
      "Epoch 24/200\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.1250 - accuracy: 0.8345\n",
      "Epoch 25/200\n",
      "29/29 [==============================] - 0s 658us/step - loss: 0.1222 - accuracy: 0.8690\n",
      "Epoch 26/200\n",
      "29/29 [==============================] - 0s 606us/step - loss: 0.1189 - accuracy: 0.8621\n",
      "Epoch 27/200\n",
      "29/29 [==============================] - 0s 673us/step - loss: 0.1161 - accuracy: 0.8690\n",
      "Epoch 28/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 0.1241 - accuracy: 0.8276\n",
      "Epoch 29/200\n",
      "29/29 [==============================] - 0s 643us/step - loss: 0.1134 - accuracy: 0.8759\n",
      "Epoch 30/200\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.1157 - accuracy: 0.8552\n",
      "Epoch 31/200\n",
      "29/29 [==============================] - 0s 622us/step - loss: 0.1113 - accuracy: 0.8759\n",
      "Epoch 32/200\n",
      "29/29 [==============================] - 0s 628us/step - loss: 0.1100 - accuracy: 0.8621\n",
      "Epoch 33/200\n",
      "29/29 [==============================] - 0s 730us/step - loss: 0.1067 - accuracy: 0.8828\n",
      "Epoch 34/200\n",
      "29/29 [==============================] - 0s 678us/step - loss: 0.1087 - accuracy: 0.8690\n",
      "Epoch 35/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.1030 - accuracy: 0.8897\n",
      "Epoch 36/200\n",
      "29/29 [==============================] - 0s 669us/step - loss: 0.1057 - accuracy: 0.8690\n",
      "Epoch 37/200\n",
      "29/29 [==============================] - 0s 689us/step - loss: 0.1063 - accuracy: 0.8690\n",
      "Epoch 38/200\n",
      "29/29 [==============================] - 0s 695us/step - loss: 0.1040 - accuracy: 0.8828\n",
      "Epoch 39/200\n",
      "29/29 [==============================] - 0s 622us/step - loss: 0.0997 - accuracy: 0.8966\n",
      "Epoch 40/200\n",
      "29/29 [==============================] - 0s 660us/step - loss: 0.0979 - accuracy: 0.8966\n",
      "Epoch 41/200\n",
      "29/29 [==============================] - 0s 671us/step - loss: 0.1016 - accuracy: 0.8690\n",
      "Epoch 42/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0949 - accuracy: 0.8966\n",
      "Epoch 43/200\n",
      "29/29 [==============================] - 0s 689us/step - loss: 0.0943 - accuracy: 0.9103\n",
      "Epoch 44/200\n",
      "29/29 [==============================] - 0s 664us/step - loss: 0.0907 - accuracy: 0.8759\n",
      "Epoch 45/200\n",
      "29/29 [==============================] - 0s 634us/step - loss: 0.0948 - accuracy: 0.8966\n",
      "Epoch 46/200\n",
      "29/29 [==============================] - 0s 684us/step - loss: 0.0902 - accuracy: 0.9034\n",
      "Epoch 47/200\n",
      "29/29 [==============================] - 0s 682us/step - loss: 0.0880 - accuracy: 0.9172\n",
      "Epoch 48/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0909 - accuracy: 0.8897\n",
      "Epoch 49/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.0845 - accuracy: 0.9172\n",
      "Epoch 50/200\n",
      "29/29 [==============================] - 0s 725us/step - loss: 0.0872 - accuracy: 0.8966\n",
      "Epoch 51/200\n",
      "29/29 [==============================] - 0s 643us/step - loss: 0.0889 - accuracy: 0.8966\n",
      "Epoch 52/200\n",
      "29/29 [==============================] - 0s 626us/step - loss: 0.0844 - accuracy: 0.9034\n",
      "Epoch 53/200\n",
      "29/29 [==============================] - 0s 625us/step - loss: 0.0866 - accuracy: 0.9034\n",
      "Epoch 54/200\n",
      "29/29 [==============================] - 0s 712us/step - loss: 0.0804 - accuracy: 0.9103\n",
      "Epoch 55/200\n",
      "29/29 [==============================] - 0s 721us/step - loss: 0.0779 - accuracy: 0.9241\n",
      "Epoch 56/200\n",
      "29/29 [==============================] - 0s 701us/step - loss: 0.0773 - accuracy: 0.9241\n",
      "Epoch 57/200\n",
      "29/29 [==============================] - 0s 692us/step - loss: 0.0766 - accuracy: 0.9172\n",
      "Epoch 58/200\n",
      "29/29 [==============================] - 0s 648us/step - loss: 0.0743 - accuracy: 0.9241\n",
      "Epoch 59/200\n",
      "29/29 [==============================] - 0s 585us/step - loss: 0.0732 - accuracy: 0.9310\n",
      "Epoch 60/200\n",
      "29/29 [==============================] - 0s 721us/step - loss: 0.0714 - accuracy: 0.9310\n",
      "Epoch 61/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0739 - accuracy: 0.9241\n",
      "Epoch 62/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0716 - accuracy: 0.9310\n",
      "Epoch 63/200\n",
      "29/29 [==============================] - 0s 714us/step - loss: 0.0696 - accuracy: 0.9310\n",
      "Epoch 64/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0668 - accuracy: 0.9379\n",
      "Epoch 65/200\n",
      "29/29 [==============================] - 0s 689us/step - loss: 0.0704 - accuracy: 0.9172\n",
      "Epoch 66/200\n",
      "29/29 [==============================] - 0s 692us/step - loss: 0.0654 - accuracy: 0.9448\n",
      "Epoch 67/200\n",
      "29/29 [==============================] - 0s 643us/step - loss: 0.0623 - accuracy: 0.9379\n",
      "Epoch 68/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0614 - accuracy: 0.9517\n",
      "Epoch 69/200\n",
      "29/29 [==============================] - 0s 646us/step - loss: 0.0596 - accuracy: 0.9517\n",
      "Epoch 70/200\n",
      "29/29 [==============================] - 0s 670us/step - loss: 0.0576 - accuracy: 0.9586\n",
      "Epoch 71/200\n",
      "29/29 [==============================] - 0s 673us/step - loss: 0.0605 - accuracy: 0.9586\n",
      "Epoch 72/200\n",
      "29/29 [==============================] - 0s 692us/step - loss: 0.0576 - accuracy: 0.9586\n",
      "Epoch 73/200\n",
      "29/29 [==============================] - 0s 656us/step - loss: 0.0538 - accuracy: 0.9655\n",
      "Epoch 74/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0579 - accuracy: 0.9172\n",
      "Epoch 75/200\n",
      "29/29 [==============================] - 0s 668us/step - loss: 0.0521 - accuracy: 0.9586\n",
      "Epoch 76/200\n",
      "29/29 [==============================] - 0s 637us/step - loss: 0.0527 - accuracy: 0.9655\n",
      "Epoch 77/200\n",
      "29/29 [==============================] - 0s 627us/step - loss: 0.0659 - accuracy: 0.9172\n",
      "Epoch 78/200\n",
      "29/29 [==============================] - 0s 638us/step - loss: 0.0532 - accuracy: 0.9517\n",
      "Epoch 79/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0527 - accuracy: 0.9448\n",
      "Epoch 80/200\n",
      "29/29 [==============================] - 0s 642us/step - loss: 0.0509 - accuracy: 0.9655\n",
      "Epoch 81/200\n",
      "29/29 [==============================] - 0s 659us/step - loss: 0.0465 - accuracy: 0.9586\n",
      "Epoch 82/200\n",
      "29/29 [==============================] - 0s 645us/step - loss: 0.0454 - accuracy: 0.9586\n",
      "Epoch 83/200\n",
      "29/29 [==============================] - 0s 632us/step - loss: 0.0490 - accuracy: 0.9448\n",
      "Epoch 84/200\n",
      "29/29 [==============================] - 0s 673us/step - loss: 0.0439 - accuracy: 0.9586\n",
      "Epoch 85/200\n",
      "29/29 [==============================] - 0s 667us/step - loss: 0.0402 - accuracy: 0.9724\n",
      "Epoch 86/200\n",
      "29/29 [==============================] - 0s 682us/step - loss: 0.0388 - accuracy: 0.9724\n",
      "Epoch 87/200\n",
      "29/29 [==============================] - 0s 635us/step - loss: 0.0384 - accuracy: 0.9655\n",
      "Epoch 88/200\n",
      "29/29 [==============================] - 0s 675us/step - loss: 0.0363 - accuracy: 0.9724\n",
      "Epoch 89/200\n",
      "29/29 [==============================] - 0s 681us/step - loss: 0.0353 - accuracy: 0.9793\n",
      "Epoch 90/200\n",
      "29/29 [==============================] - 0s 684us/step - loss: 0.0345 - accuracy: 0.9793\n",
      "Epoch 91/200\n",
      "29/29 [==============================] - 0s 675us/step - loss: 0.0330 - accuracy: 0.9724\n",
      "Epoch 92/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.0335 - accuracy: 0.9862\n",
      "Epoch 93/200\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.0295 - accuracy: 0.9862\n",
      "Epoch 94/200\n",
      "29/29 [==============================] - 0s 656us/step - loss: 0.0318 - accuracy: 0.9862\n",
      "Epoch 95/200\n",
      "29/29 [==============================] - 0s 640us/step - loss: 0.0294 - accuracy: 0.9931\n",
      "Epoch 96/200\n",
      "29/29 [==============================] - 0s 673us/step - loss: 0.0318 - accuracy: 0.9862\n",
      "Epoch 97/200\n",
      "29/29 [==============================] - 0s 706us/step - loss: 0.0241 - accuracy: 0.9931\n",
      "Epoch 98/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0327 - accuracy: 0.9793\n",
      "Epoch 99/200\n",
      "29/29 [==============================] - 0s 638us/step - loss: 0.0285 - accuracy: 0.9931\n",
      "Epoch 100/200\n",
      "29/29 [==============================] - 0s 660us/step - loss: 0.0293 - accuracy: 0.9931\n",
      "Epoch 101/200\n",
      "29/29 [==============================] - 0s 718us/step - loss: 0.0206 - accuracy: 0.9862\n",
      "Epoch 102/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0191 - accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "29/29 [==============================] - 0s 654us/step - loss: 0.0223 - accuracy: 0.9931\n",
      "Epoch 104/200\n",
      "29/29 [==============================] - 0s 550us/step - loss: 0.0184 - accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "29/29 [==============================] - 0s 669us/step - loss: 0.0166 - accuracy: 1.0000\n",
      "Epoch 106/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 694us/step - loss: 0.0198 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "29/29 [==============================] - 0s 629us/step - loss: 0.0185 - accuracy: 0.9931\n",
      "Epoch 108/200\n",
      "29/29 [==============================] - 0s 621us/step - loss: 0.0158 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "29/29 [==============================] - 0s 613us/step - loss: 0.0139 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "29/29 [==============================] - 0s 615us/step - loss: 0.0157 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "29/29 [==============================] - 0s 596us/step - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "29/29 [==============================] - 0s 703us/step - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "29/29 [==============================] - 0s 680us/step - loss: 0.0155 - accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "29/29 [==============================] - 0s 646us/step - loss: 0.0216 - accuracy: 0.9793\n",
      "Epoch 115/200\n",
      "29/29 [==============================] - 0s 632us/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "29/29 [==============================] - 0s 682us/step - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "29/29 [==============================] - 0s 657us/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "29/29 [==============================] - 0s 694us/step - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "29/29 [==============================] - 0s 510us/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "29/29 [==============================] - 0s 712us/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "29/29 [==============================] - 0s 638us/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "29/29 [==============================] - 0s 614us/step - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "29/29 [==============================] - 0s 646us/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "29/29 [==============================] - 0s 666us/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "29/29 [==============================] - 0s 647us/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "29/29 [==============================] - 0s 635us/step - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "29/29 [==============================] - 0s 631us/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "29/29 [==============================] - 0s 668us/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "29/29 [==============================] - 0s 625us/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "29/29 [==============================] - 0s 634us/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "29/29 [==============================] - 0s 657us/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "29/29 [==============================] - 0s 679us/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "29/29 [==============================] - 0s 661us/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "29/29 [==============================] - 0s 664us/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "29/29 [==============================] - 0s 680us/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "29/29 [==============================] - 0s 671us/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "29/29 [==============================] - 0s 636us/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "29/29 [==============================] - 0s 628us/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "29/29 [==============================] - 0s 640us/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "29/29 [==============================] - 0s 640us/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "29/29 [==============================] - 0s 685us/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "29/29 [==============================] - 0s 649us/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "29/29 [==============================] - 0s 654us/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "29/29 [==============================] - 0s 674us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "29/29 [==============================] - 0s 616us/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "29/29 [==============================] - 0s 643us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "29/29 [==============================] - 0s 658us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "29/29 [==============================] - 0s 658us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "29/29 [==============================] - 0s 672us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "29/29 [==============================] - 0s 674us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "29/29 [==============================] - 0s 645us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "29/29 [==============================] - 0s 634us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "29/29 [==============================] - 0s 644us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "29/29 [==============================] - 0s 684us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "29/29 [==============================] - 0s 644us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "29/29 [==============================] - 0s 640us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "29/29 [==============================] - 0s 707us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "29/29 [==============================] - 0s 632us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "29/29 [==============================] - 0s 657us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "29/29 [==============================] - 0s 718us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "29/29 [==============================] - 0s 669us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "29/29 [==============================] - 0s 682us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "29/29 [==============================] - 0s 646us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "29/29 [==============================] - 0s 681us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "29/29 [==============================] - 0s 609us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "29/29 [==============================] - 0s 673us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "29/29 [==============================] - 0s 749us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "29/29 [==============================] - 0s 706us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "29/29 [==============================] - 0s 695us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "29/29 [==============================] - 0s 664us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "29/29 [==============================] - 0s 675us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "29/29 [==============================] - 0s 650us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "29/29 [==============================] - 0s 635us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "29/29 [==============================] - 0s 594us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "29/29 [==============================] - 0s 647us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "29/29 [==============================] - 0s 678us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "29/29 [==============================] - 0s 696us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "29/29 [==============================] - 0s 696us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "29/29 [==============================] - 0s 699us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "29/29 [==============================] - 0s 685us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "29/29 [==============================] - 0s 675us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 9.7562e-04 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "29/29 [==============================] - 0s 613us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "29/29 [==============================] - 0s 656us/step - loss: 9.1842e-04 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 8.7004e-04 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "29/29 [==============================] - 0s 671us/step - loss: 8.6020e-04 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "29/29 [==============================] - 0s 716us/step - loss: 8.7356e-04 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "29/29 [==============================] - 0s 702us/step - loss: 9.6032e-04 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "29/29 [==============================] - 0s 662us/step - loss: 8.2910e-04 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "29/29 [==============================] - 0s 674us/step - loss: 8.2023e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001BBFB3083A8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0624 - accuracy: 0.9048\n",
      "Epoch 1/200\n",
      "29/29 [==============================] - 0s 566us/step - loss: 0.2577 - accuracy: 0.4621\n",
      "Epoch 2/200\n",
      "29/29 [==============================] - 0s 548us/step - loss: 0.2486 - accuracy: 0.4828\n",
      "Epoch 3/200\n",
      "29/29 [==============================] - 0s 567us/step - loss: 0.2452 - accuracy: 0.6759\n",
      "Epoch 4/200\n",
      "29/29 [==============================] - 0s 627us/step - loss: 0.2421 - accuracy: 0.6621\n",
      "Epoch 5/200\n",
      "29/29 [==============================] - 0s 571us/step - loss: 0.2399 - accuracy: 0.6414\n",
      "Epoch 6/200\n",
      "29/29 [==============================] - 0s 600us/step - loss: 0.2364 - accuracy: 0.6690\n",
      "Epoch 7/200\n",
      "29/29 [==============================] - 0s 678us/step - loss: 0.2309 - accuracy: 0.6414\n",
      "Epoch 8/200\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.2261 - accuracy: 0.6759\n",
      "Epoch 9/200\n",
      "29/29 [==============================] - 0s 627us/step - loss: 0.2232 - accuracy: 0.6345\n",
      "Epoch 10/200\n",
      "29/29 [==============================] - 0s 640us/step - loss: 0.2144 - accuracy: 0.7034\n",
      "Epoch 11/200\n",
      "29/29 [==============================] - 0s 699us/step - loss: 0.2064 - accuracy: 0.7103\n",
      "Epoch 12/200\n",
      "29/29 [==============================] - 0s 685us/step - loss: 0.2000 - accuracy: 0.7586\n",
      "Epoch 13/200\n",
      "29/29 [==============================] - 0s 625us/step - loss: 0.1886 - accuracy: 0.7586\n",
      "Epoch 14/200\n",
      "29/29 [==============================] - 0s 675us/step - loss: 0.1770 - accuracy: 0.7241\n",
      "Epoch 15/200\n",
      "29/29 [==============================] - 0s 673us/step - loss: 0.1687 - accuracy: 0.7862\n",
      "Epoch 16/200\n",
      "29/29 [==============================] - 0s 668us/step - loss: 0.1654 - accuracy: 0.7448\n",
      "Epoch 17/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.1565 - accuracy: 0.7793\n",
      "Epoch 18/200\n",
      "29/29 [==============================] - 0s 644us/step - loss: 0.1516 - accuracy: 0.7724\n",
      "Epoch 19/200\n",
      "29/29 [==============================] - 0s 662us/step - loss: 0.1468 - accuracy: 0.8000\n",
      "Epoch 20/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 0.1432 - accuracy: 0.8069\n",
      "Epoch 21/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 0.1391 - accuracy: 0.7931\n",
      "Epoch 22/200\n",
      "29/29 [==============================] - 0s 646us/step - loss: 0.1391 - accuracy: 0.8069\n",
      "Epoch 23/200\n",
      "29/29 [==============================] - 0s 662us/step - loss: 0.1466 - accuracy: 0.7931\n",
      "Epoch 24/200\n",
      "29/29 [==============================] - 0s 636us/step - loss: 0.1307 - accuracy: 0.8552\n",
      "Epoch 25/200\n",
      "29/29 [==============================] - 0s 658us/step - loss: 0.1258 - accuracy: 0.8483\n",
      "Epoch 26/200\n",
      "29/29 [==============================] - 0s 620us/step - loss: 0.1246 - accuracy: 0.8552\n",
      "Epoch 27/200\n",
      "29/29 [==============================] - 0s 667us/step - loss: 0.1187 - accuracy: 0.8483\n",
      "Epoch 28/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.1252 - accuracy: 0.8345\n",
      "Epoch 29/200\n",
      "29/29 [==============================] - 0s 630us/step - loss: 0.1145 - accuracy: 0.8552\n",
      "Epoch 30/200\n",
      "29/29 [==============================] - 0s 661us/step - loss: 0.1165 - accuracy: 0.8690\n",
      "Epoch 31/200\n",
      "29/29 [==============================] - 0s 634us/step - loss: 0.1123 - accuracy: 0.8690\n",
      "Epoch 32/200\n",
      "29/29 [==============================] - 0s 702us/step - loss: 0.1104 - accuracy: 0.8621\n",
      "Epoch 33/200\n",
      "29/29 [==============================] - 0s 672us/step - loss: 0.1083 - accuracy: 0.8621\n",
      "Epoch 34/200\n",
      "29/29 [==============================] - 0s 662us/step - loss: 0.1071 - accuracy: 0.8621\n",
      "Epoch 35/200\n",
      "29/29 [==============================] - 0s 682us/step - loss: 0.1031 - accuracy: 0.8828\n",
      "Epoch 36/200\n",
      "29/29 [==============================] - 0s 635us/step - loss: 0.1045 - accuracy: 0.8759\n",
      "Epoch 37/200\n",
      "29/29 [==============================] - 0s 668us/step - loss: 0.1008 - accuracy: 0.8828\n",
      "Epoch 38/200\n",
      "29/29 [==============================] - 0s 645us/step - loss: 0.1019 - accuracy: 0.8897\n",
      "Epoch 39/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.0976 - accuracy: 0.8828\n",
      "Epoch 40/200\n",
      "29/29 [==============================] - 0s 660us/step - loss: 0.0951 - accuracy: 0.8759\n",
      "Epoch 41/200\n",
      "29/29 [==============================] - 0s 722us/step - loss: 0.0961 - accuracy: 0.8690\n",
      "Epoch 42/200\n",
      "29/29 [==============================] - 0s 689us/step - loss: 0.0913 - accuracy: 0.8966\n",
      "Epoch 43/200\n",
      "29/29 [==============================] - 0s 694us/step - loss: 0.0911 - accuracy: 0.8966\n",
      "Epoch 44/200\n",
      "29/29 [==============================] - 0s 647us/step - loss: 0.0878 - accuracy: 0.9034\n",
      "Epoch 45/200\n",
      "29/29 [==============================] - 0s 646us/step - loss: 0.0892 - accuracy: 0.8966\n",
      "Epoch 46/200\n",
      "29/29 [==============================] - 0s 628us/step - loss: 0.0848 - accuracy: 0.8966\n",
      "Epoch 47/200\n",
      "29/29 [==============================] - 0s 703us/step - loss: 0.0824 - accuracy: 0.9103\n",
      "Epoch 48/200\n",
      "29/29 [==============================] - 0s 697us/step - loss: 0.0838 - accuracy: 0.9034\n",
      "Epoch 49/200\n",
      "29/29 [==============================] - 0s 686us/step - loss: 0.0797 - accuracy: 0.9103\n",
      "Epoch 50/200\n",
      "29/29 [==============================] - 0s 643us/step - loss: 0.0806 - accuracy: 0.9172\n",
      "Epoch 51/200\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.0803 - accuracy: 0.9172\n",
      "Epoch 52/200\n",
      "29/29 [==============================] - 0s 696us/step - loss: 0.0789 - accuracy: 0.9034\n",
      "Epoch 53/200\n",
      "29/29 [==============================] - 0s 689us/step - loss: 0.0811 - accuracy: 0.8966\n",
      "Epoch 54/200\n",
      "29/29 [==============================] - 0s 645us/step - loss: 0.0743 - accuracy: 0.9103\n",
      "Epoch 55/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0717 - accuracy: 0.9103\n",
      "Epoch 56/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 690us/step - loss: 0.0702 - accuracy: 0.9310\n",
      "Epoch 57/200\n",
      "29/29 [==============================] - 0s 724us/step - loss: 0.0700 - accuracy: 0.9448\n",
      "Epoch 58/200\n",
      "29/29 [==============================] - 0s 654us/step - loss: 0.0661 - accuracy: 0.9448\n",
      "Epoch 59/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0666 - accuracy: 0.9310\n",
      "Epoch 60/200\n",
      "29/29 [==============================] - 0s 657us/step - loss: 0.0651 - accuracy: 0.9241\n",
      "Epoch 61/200\n",
      "29/29 [==============================] - 0s 681us/step - loss: 0.0687 - accuracy: 0.9310\n",
      "Epoch 62/200\n",
      "29/29 [==============================] - 0s 629us/step - loss: 0.0629 - accuracy: 0.9586\n",
      "Epoch 63/200\n",
      "29/29 [==============================] - 0s 666us/step - loss: 0.0619 - accuracy: 0.9448\n",
      "Epoch 64/200\n",
      "29/29 [==============================] - 0s 663us/step - loss: 0.0593 - accuracy: 0.9379\n",
      "Epoch 65/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0623 - accuracy: 0.9379\n",
      "Epoch 66/200\n",
      "29/29 [==============================] - 0s 664us/step - loss: 0.0575 - accuracy: 0.9517\n",
      "Epoch 67/200\n",
      "29/29 [==============================] - 0s 675us/step - loss: 0.0572 - accuracy: 0.9517\n",
      "Epoch 68/200\n",
      "29/29 [==============================] - 0s 695us/step - loss: 0.0556 - accuracy: 0.9517\n",
      "Epoch 69/200\n",
      "29/29 [==============================] - 0s 636us/step - loss: 0.0528 - accuracy: 0.9517\n",
      "Epoch 70/200\n",
      "29/29 [==============================] - 0s 634us/step - loss: 0.0508 - accuracy: 0.9586\n",
      "Epoch 71/200\n",
      "29/29 [==============================] - 0s 642us/step - loss: 0.0555 - accuracy: 0.9448\n",
      "Epoch 72/200\n",
      "29/29 [==============================] - 0s 625us/step - loss: 0.0496 - accuracy: 0.9586\n",
      "Epoch 73/200\n",
      "29/29 [==============================] - 0s 636us/step - loss: 0.0478 - accuracy: 0.9448\n",
      "Epoch 74/200\n",
      "29/29 [==============================] - 0s 638us/step - loss: 0.0485 - accuracy: 0.9448\n",
      "Epoch 75/200\n",
      "29/29 [==============================] - 0s 699us/step - loss: 0.0466 - accuracy: 0.9655\n",
      "Epoch 76/200\n",
      "29/29 [==============================] - 0s 714us/step - loss: 0.0461 - accuracy: 0.9586\n",
      "Epoch 77/200\n",
      "29/29 [==============================] - 0s 666us/step - loss: 0.0508 - accuracy: 0.9517\n",
      "Epoch 78/200\n",
      "29/29 [==============================] - 0s 624us/step - loss: 0.0477 - accuracy: 0.9655\n",
      "Epoch 79/200\n",
      "29/29 [==============================] - 0s 638us/step - loss: 0.0456 - accuracy: 0.9517\n",
      "Epoch 80/200\n",
      "29/29 [==============================] - 0s 665us/step - loss: 0.0446 - accuracy: 0.9586\n",
      "Epoch 81/200\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.0433 - accuracy: 0.9517\n",
      "Epoch 82/200\n",
      "29/29 [==============================] - 0s 672us/step - loss: 0.0404 - accuracy: 0.9586\n",
      "Epoch 83/200\n",
      "29/29 [==============================] - 0s 630us/step - loss: 0.0447 - accuracy: 0.9517\n",
      "Epoch 84/200\n",
      "29/29 [==============================] - 0s 656us/step - loss: 0.0380 - accuracy: 0.9655\n",
      "Epoch 85/200\n",
      "29/29 [==============================] - 0s 657us/step - loss: 0.0364 - accuracy: 0.9724\n",
      "Epoch 86/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0376 - accuracy: 0.9724\n",
      "Epoch 87/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 0.0347 - accuracy: 0.9724\n",
      "Epoch 88/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0327 - accuracy: 0.9793\n",
      "Epoch 89/200\n",
      "29/29 [==============================] - 0s 690us/step - loss: 0.0324 - accuracy: 0.9793\n",
      "Epoch 90/200\n",
      "29/29 [==============================] - 0s 720us/step - loss: 0.0317 - accuracy: 0.9724\n",
      "Epoch 91/200\n",
      "29/29 [==============================] - 0s 689us/step - loss: 0.0309 - accuracy: 0.9793\n",
      "Epoch 92/200\n",
      "29/29 [==============================] - 0s 685us/step - loss: 0.0332 - accuracy: 0.9724\n",
      "Epoch 93/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0281 - accuracy: 0.9931\n",
      "Epoch 94/200\n",
      "29/29 [==============================] - 0s 766us/step - loss: 0.0319 - accuracy: 0.9724\n",
      "Epoch 95/200\n",
      "29/29 [==============================] - 0s 720us/step - loss: 0.0287 - accuracy: 0.9793\n",
      "Epoch 96/200\n",
      "29/29 [==============================] - 0s 722us/step - loss: 0.0299 - accuracy: 0.9793\n",
      "Epoch 97/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0273 - accuracy: 0.9793\n",
      "Epoch 98/200\n",
      "29/29 [==============================] - 0s 720us/step - loss: 0.0307 - accuracy: 0.9793\n",
      "Epoch 99/200\n",
      "29/29 [==============================] - 0s 721us/step - loss: 0.0276 - accuracy: 0.9793\n",
      "Epoch 100/200\n",
      "29/29 [==============================] - 0s 719us/step - loss: 0.0280 - accuracy: 0.9724\n",
      "Epoch 101/200\n",
      "29/29 [==============================] - 0s 755us/step - loss: 0.0235 - accuracy: 0.9793\n",
      "Epoch 102/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0243 - accuracy: 0.9862\n",
      "Epoch 103/200\n",
      "29/29 [==============================] - 0s 721us/step - loss: 0.0232 - accuracy: 0.9724\n",
      "Epoch 104/200\n",
      "29/29 [==============================] - 0s 720us/step - loss: 0.0245 - accuracy: 0.9724\n",
      "Epoch 105/200\n",
      "29/29 [==============================] - 0s 654us/step - loss: 0.0207 - accuracy: 0.9862\n",
      "Epoch 106/200\n",
      "29/29 [==============================] - 0s 707us/step - loss: 0.0214 - accuracy: 0.9793\n",
      "Epoch 107/200\n",
      "29/29 [==============================] - 0s 616us/step - loss: 0.0218 - accuracy: 0.9793\n",
      "Epoch 108/200\n",
      "29/29 [==============================] - 0s 649us/step - loss: 0.0192 - accuracy: 0.9931\n",
      "Epoch 109/200\n",
      "29/29 [==============================] - 0s 632us/step - loss: 0.0184 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "29/29 [==============================] - 0s 691us/step - loss: 0.0208 - accuracy: 0.9931\n",
      "Epoch 111/200\n",
      "29/29 [==============================] - 0s 621us/step - loss: 0.0211 - accuracy: 0.9862\n",
      "Epoch 112/200\n",
      "29/29 [==============================] - 0s 636us/step - loss: 0.0205 - accuracy: 0.9862\n",
      "Epoch 113/200\n",
      "29/29 [==============================] - 0s 678us/step - loss: 0.0321 - accuracy: 0.9724\n",
      "Epoch 114/200\n",
      "29/29 [==============================] - 0s 660us/step - loss: 0.0248 - accuracy: 0.9793\n",
      "Epoch 115/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0185 - accuracy: 0.9931\n",
      "Epoch 116/200\n",
      "29/29 [==============================] - 0s 649us/step - loss: 0.0159 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "29/29 [==============================] - 0s 719us/step - loss: 0.0162 - accuracy: 0.9931\n",
      "Epoch 118/200\n",
      "29/29 [==============================] - 0s 668us/step - loss: 0.0178 - accuracy: 0.9862\n",
      "Epoch 119/200\n",
      "29/29 [==============================] - 0s 698us/step - loss: 0.0150 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "29/29 [==============================] - 0s 725us/step - loss: 0.0166 - accuracy: 0.9862\n",
      "Epoch 121/200\n",
      "29/29 [==============================] - 0s 624us/step - loss: 0.0179 - accuracy: 0.9862\n",
      "Epoch 122/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0145 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "29/29 [==============================] - 0s 672us/step - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "29/29 [==============================] - 0s 680us/step - loss: 0.0136 - accuracy: 0.9931\n",
      "Epoch 125/200\n",
      "29/29 [==============================] - 0s 632us/step - loss: 0.0177 - accuracy: 0.9931\n",
      "Epoch 126/200\n",
      "29/29 [==============================] - 0s 516us/step - loss: 0.0160 - accuracy: 0.9931\n",
      "Epoch 127/200\n",
      "29/29 [==============================] - 0s 673us/step - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.0115 - accuracy: 0.9931\n",
      "Epoch 129/200\n",
      "29/29 [==============================] - 0s 706us/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "29/29 [==============================] - 0s 711us/step - loss: 0.0115 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "29/29 [==============================] - 0s 640us/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "29/29 [==============================] - 0s 675us/step - loss: 0.0103 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "29/29 [==============================] - 0s 659us/step - loss: 0.0122 - accuracy: 0.9931\n",
      "Epoch 135/200\n",
      "29/29 [==============================] - 0s 659us/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 136/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 682us/step - loss: 0.0087 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "29/29 [==============================] - 0s 640us/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "29/29 [==============================] - 0s 639us/step - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "29/29 [==============================] - 0s 620us/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "29/29 [==============================] - 0s 632us/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "29/29 [==============================] - 0s 680us/step - loss: 0.0087 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "29/29 [==============================] - 0s 665us/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "29/29 [==============================] - 0s 668us/step - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "29/29 [==============================] - 0s 645us/step - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "29/29 [==============================] - 0s 682us/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "29/29 [==============================] - 0s 620us/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "29/29 [==============================] - 0s 630us/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "29/29 [==============================] - ETA: 0s - loss: 7.3061e-04 - accuracy: 1.00 - 0s 674us/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "29/29 [==============================] - 0s 593us/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "29/29 [==============================] - 0s 727us/step - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "29/29 [==============================] - 0s 713us/step - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "29/29 [==============================] - 0s 679us/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "29/29 [==============================] - 0s 636us/step - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "29/29 [==============================] - 0s 643us/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "29/29 [==============================] - 0s 643us/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "29/29 [==============================] - 0s 639us/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "29/29 [==============================] - 0s 115us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "29/29 [==============================] - 0s 137us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "29/29 [==============================] - 0s 157us/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "29/29 [==============================] - 0s 604us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "29/29 [==============================] - 0s 64us/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "29/29 [==============================] - 0s 701us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "29/29 [==============================] - 0s 141us/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "29/29 [==============================] - 0s 607us/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "29/29 [==============================] - 0s 162us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "29/29 [==============================] - 0s 141us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "29/29 [==============================] - 0s 59us/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "29/29 [==============================] - 0s 64us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "29/29 [==============================] - 0s 566us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "29/29 [==============================] - 0s 686us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "29/29 [==============================] - 0s 656us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "29/29 [==============================] - 0s 704us/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "29/29 [==============================] - 0s 668us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "29/29 [==============================] - 0s 689us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "29/29 [==============================] - 0s 644us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "29/29 [==============================] - 0s 672us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "29/29 [==============================] - 0s 664us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "29/29 [==============================] - 0s 721us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "29/29 [==============================] - 0s 650us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "29/29 [==============================] - 0s 702us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "29/29 [==============================] - 0s 702us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "29/29 [==============================] - 0s 692us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "29/29 [==============================] - 0s 618us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "29/29 [==============================] - 0s 659us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "29/29 [==============================] - 0s 656us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "29/29 [==============================] - 0s 626us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "29/29 [==============================] - 0s 683us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "29/29 [==============================] - 0s 670us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "29/29 [==============================] - 0s 664us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "29/29 [==============================] - 0s 671us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "29/29 [==============================] - 0s 647us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "29/29 [==============================] - 0s 624us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "29/29 [==============================] - 0s 680us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "29/29 [==============================] - 0s 628us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001BBF92358B8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.1699 - accuracy: 0.8095\n",
      "Epoch 1/200\n",
      "29/29 [==============================] - 0s 569us/step - loss: 0.2552 - accuracy: 0.4621\n",
      "Epoch 2/200\n",
      "29/29 [==============================] - 0s 566us/step - loss: 0.2462 - accuracy: 0.5586\n",
      "Epoch 3/200\n",
      "29/29 [==============================] - 0s 617us/step - loss: 0.2389 - accuracy: 0.5655\n",
      "Epoch 4/200\n",
      "29/29 [==============================] - 0s 599us/step - loss: 0.2339 - accuracy: 0.6138\n",
      "Epoch 5/200\n",
      "29/29 [==============================] - 0s 621us/step - loss: 0.2318 - accuracy: 0.6000\n",
      "Epoch 6/200\n",
      "29/29 [==============================] - 0s 708us/step - loss: 0.2269 - accuracy: 0.6690\n",
      "Epoch 7/200\n",
      "29/29 [==============================] - 0s 617us/step - loss: 0.2199 - accuracy: 0.6621\n",
      "Epoch 8/200\n",
      "29/29 [==============================] - 0s 670us/step - loss: 0.2166 - accuracy: 0.6690\n",
      "Epoch 9/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.2128 - accuracy: 0.6483\n",
      "Epoch 10/200\n",
      "29/29 [==============================] - 0s 643us/step - loss: 0.2054 - accuracy: 0.7310\n",
      "Epoch 11/200\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.1979 - accuracy: 0.7379\n",
      "Epoch 12/200\n",
      "29/29 [==============================] - 0s 666us/step - loss: 0.1934 - accuracy: 0.7586\n",
      "Epoch 13/200\n",
      "29/29 [==============================] - 0s 656us/step - loss: 0.1877 - accuracy: 0.7034\n",
      "Epoch 14/200\n",
      "29/29 [==============================] - 0s 689us/step - loss: 0.1791 - accuracy: 0.7517\n",
      "Epoch 15/200\n",
      "29/29 [==============================] - 0s 638us/step - loss: 0.1699 - accuracy: 0.7793\n",
      "Epoch 16/200\n",
      "29/29 [==============================] - 0s 672us/step - loss: 0.1692 - accuracy: 0.7586\n",
      "Epoch 17/200\n",
      "29/29 [==============================] - 0s 621us/step - loss: 0.1612 - accuracy: 0.7793\n",
      "Epoch 18/200\n",
      "29/29 [==============================] - 0s 640us/step - loss: 0.1580 - accuracy: 0.7862\n",
      "Epoch 19/200\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.1525 - accuracy: 0.7724\n",
      "Epoch 20/200\n",
      "29/29 [==============================] - 0s 691us/step - loss: 0.1508 - accuracy: 0.7862\n",
      "Epoch 21/200\n",
      "29/29 [==============================] - 0s 700us/step - loss: 0.1471 - accuracy: 0.8000\n",
      "Epoch 22/200\n",
      "29/29 [==============================] - 0s 656us/step - loss: 0.1492 - accuracy: 0.8000\n",
      "Epoch 23/200\n",
      "29/29 [==============================] - 0s 678us/step - loss: 0.1505 - accuracy: 0.7793\n",
      "Epoch 24/200\n",
      "29/29 [==============================] - 0s 669us/step - loss: 0.1387 - accuracy: 0.8000\n",
      "Epoch 25/200\n",
      "29/29 [==============================] - 0s 678us/step - loss: 0.1359 - accuracy: 0.8207\n",
      "Epoch 26/200\n",
      "29/29 [==============================] - 0s 708us/step - loss: 0.1333 - accuracy: 0.8138\n",
      "Epoch 27/200\n",
      "29/29 [==============================] - 0s 673us/step - loss: 0.1307 - accuracy: 0.8345\n",
      "Epoch 28/200\n",
      "29/29 [==============================] - 0s 719us/step - loss: 0.1346 - accuracy: 0.8069\n",
      "Epoch 29/200\n",
      "29/29 [==============================] - 0s 712us/step - loss: 0.1269 - accuracy: 0.8345\n",
      "Epoch 30/200\n",
      "29/29 [==============================] - 0s 673us/step - loss: 0.1280 - accuracy: 0.8069\n",
      "Epoch 31/200\n",
      "29/29 [==============================] - 0s 632us/step - loss: 0.1245 - accuracy: 0.8207\n",
      "Epoch 32/200\n",
      "29/29 [==============================] - 0s 684us/step - loss: 0.1221 - accuracy: 0.8207\n",
      "Epoch 33/200\n",
      "29/29 [==============================] - 0s 679us/step - loss: 0.1214 - accuracy: 0.8138\n",
      "Epoch 34/200\n",
      "29/29 [==============================] - 0s 757us/step - loss: 0.1204 - accuracy: 0.8207\n",
      "Epoch 35/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.1166 - accuracy: 0.8483\n",
      "Epoch 36/200\n",
      "29/29 [==============================] - 0s 668us/step - loss: 0.1176 - accuracy: 0.8138\n",
      "Epoch 37/200\n",
      "29/29 [==============================] - 0s 673us/step - loss: 0.1163 - accuracy: 0.8414\n",
      "Epoch 38/200\n",
      "29/29 [==============================] - 0s 672us/step - loss: 0.1152 - accuracy: 0.8483\n",
      "Epoch 39/200\n",
      "29/29 [==============================] - 0s 691us/step - loss: 0.1111 - accuracy: 0.8552\n",
      "Epoch 40/200\n",
      "29/29 [==============================] - 0s 712us/step - loss: 0.1101 - accuracy: 0.8345\n",
      "Epoch 41/200\n",
      "29/29 [==============================] - 0s 695us/step - loss: 0.1101 - accuracy: 0.8345\n",
      "Epoch 42/200\n",
      "29/29 [==============================] - 0s 715us/step - loss: 0.1057 - accuracy: 0.8414\n",
      "Epoch 43/200\n",
      "29/29 [==============================] - 0s 701us/step - loss: 0.1068 - accuracy: 0.8483\n",
      "Epoch 44/200\n",
      "29/29 [==============================] - 0s 728us/step - loss: 0.1047 - accuracy: 0.8414\n",
      "Epoch 45/200\n",
      "29/29 [==============================] - 0s 660us/step - loss: 0.1089 - accuracy: 0.8690\n",
      "Epoch 46/200\n",
      "29/29 [==============================] - 0s 679us/step - loss: 0.1003 - accuracy: 0.8621\n",
      "Epoch 47/200\n",
      "29/29 [==============================] - 0s 640us/step - loss: 0.0987 - accuracy: 0.8552\n",
      "Epoch 48/200\n",
      "29/29 [==============================] - 0s 689us/step - loss: 0.0988 - accuracy: 0.8552\n",
      "Epoch 49/200\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.0957 - accuracy: 0.8759\n",
      "Epoch 50/200\n",
      "29/29 [==============================] - 0s 659us/step - loss: 0.0967 - accuracy: 0.8690\n",
      "Epoch 51/200\n",
      "29/29 [==============================] - 0s 676us/step - loss: 0.0969 - accuracy: 0.8690\n",
      "Epoch 52/200\n",
      "29/29 [==============================] - 0s 633us/step - loss: 0.0947 - accuracy: 0.8552\n",
      "Epoch 53/200\n",
      "29/29 [==============================] - 0s 608us/step - loss: 0.0964 - accuracy: 0.8690\n",
      "Epoch 54/200\n",
      "29/29 [==============================] - 0s 686us/step - loss: 0.0936 - accuracy: 0.8621\n",
      "Epoch 55/200\n",
      "29/29 [==============================] - 0s 639us/step - loss: 0.0869 - accuracy: 0.8897\n",
      "Epoch 56/200\n",
      "29/29 [==============================] - 0s 691us/step - loss: 0.0875 - accuracy: 0.8690\n",
      "Epoch 57/200\n",
      "29/29 [==============================] - 0s 623us/step - loss: 0.0870 - accuracy: 0.8966\n",
      "Epoch 58/200\n",
      "29/29 [==============================] - 0s 645us/step - loss: 0.0817 - accuracy: 0.9034\n",
      "Epoch 59/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.0835 - accuracy: 0.9034\n",
      "Epoch 60/200\n",
      "29/29 [==============================] - 0s 695us/step - loss: 0.0835 - accuracy: 0.9103\n",
      "Epoch 61/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0912 - accuracy: 0.8759\n",
      "Epoch 62/200\n",
      "29/29 [==============================] - 0s 673us/step - loss: 0.0801 - accuracy: 0.9172\n",
      "Epoch 63/200\n",
      "29/29 [==============================] - 0s 669us/step - loss: 0.0790 - accuracy: 0.9310\n",
      "Epoch 64/200\n",
      "29/29 [==============================] - 0s 676us/step - loss: 0.0769 - accuracy: 0.9172\n",
      "Epoch 65/200\n",
      "29/29 [==============================] - 0s 668us/step - loss: 0.0779 - accuracy: 0.8897\n",
      "Epoch 66/200\n",
      "29/29 [==============================] - 0s 673us/step - loss: 0.0733 - accuracy: 0.9241\n",
      "Epoch 67/200\n",
      "29/29 [==============================] - 0s 674us/step - loss: 0.0713 - accuracy: 0.9241\n",
      "Epoch 68/200\n",
      "29/29 [==============================] - 0s 629us/step - loss: 0.0719 - accuracy: 0.9172\n",
      "Epoch 69/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 0.0675 - accuracy: 0.9310\n",
      "Epoch 70/200\n",
      "29/29 [==============================] - 0s 703us/step - loss: 0.0666 - accuracy: 0.9379\n",
      "Epoch 71/200\n",
      "29/29 [==============================] - 0s 756us/step - loss: 0.0707 - accuracy: 0.9034\n",
      "Epoch 72/200\n",
      "29/29 [==============================] - 0s 757us/step - loss: 0.0666 - accuracy: 0.9241\n",
      "Epoch 73/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 0.0608 - accuracy: 0.9517\n",
      "Epoch 74/200\n",
      "29/29 [==============================] - 0s 585us/step - loss: 0.0647 - accuracy: 0.9172\n",
      "Epoch 75/200\n",
      "29/29 [==============================] - 0s 550us/step - loss: 0.0611 - accuracy: 0.9448\n",
      "Epoch 76/200\n",
      "29/29 [==============================] - 0s 481us/step - loss: 0.0607 - accuracy: 0.9379\n",
      "Epoch 77/200\n",
      "29/29 [==============================] - 0s 499us/step - loss: 0.0658 - accuracy: 0.9241\n",
      "Epoch 78/200\n",
      "29/29 [==============================] - 0s 733us/step - loss: 0.0605 - accuracy: 0.9448\n",
      "Epoch 79/200\n",
      "29/29 [==============================] - 0s 689us/step - loss: 0.0602 - accuracy: 0.9310\n",
      "Epoch 80/200\n",
      "29/29 [==============================] - 0s 671us/step - loss: 0.0573 - accuracy: 0.9517\n",
      "Epoch 81/200\n",
      "29/29 [==============================] - 0s 625us/step - loss: 0.0555 - accuracy: 0.9586\n",
      "Epoch 82/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 0.0536 - accuracy: 0.9586\n",
      "Epoch 83/200\n",
      "29/29 [==============================] - 0s 791us/step - loss: 0.0537 - accuracy: 0.9517\n",
      "Epoch 84/200\n",
      "29/29 [==============================] - 0s 722us/step - loss: 0.0493 - accuracy: 0.9517\n",
      "Epoch 85/200\n",
      "29/29 [==============================] - 0s 654us/step - loss: 0.0490 - accuracy: 0.9586\n",
      "Epoch 86/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0477 - accuracy: 0.9724\n",
      "Epoch 87/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 722us/step - loss: 0.0465 - accuracy: 0.9517\n",
      "Epoch 88/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 0.0445 - accuracy: 0.9793\n",
      "Epoch 89/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 0.0432 - accuracy: 0.9724\n",
      "Epoch 90/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 0.0434 - accuracy: 0.9793\n",
      "Epoch 91/200\n",
      "29/29 [==============================] - 0s 585us/step - loss: 0.0428 - accuracy: 0.9586\n",
      "Epoch 92/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0442 - accuracy: 0.9517\n",
      "Epoch 93/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0389 - accuracy: 0.9793\n",
      "Epoch 94/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0423 - accuracy: 0.9655\n",
      "Epoch 95/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0412 - accuracy: 0.9724\n",
      "Epoch 96/200\n",
      "29/29 [==============================] - 0s 584us/step - loss: 0.0442 - accuracy: 0.9586\n",
      "Epoch 97/200\n",
      "29/29 [==============================] - 0s 585us/step - loss: 0.0363 - accuracy: 0.9724\n",
      "Epoch 98/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 0.0390 - accuracy: 0.9586\n",
      "Epoch 99/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0353 - accuracy: 0.9793\n",
      "Epoch 100/200\n",
      "29/29 [==============================] - 0s 585us/step - loss: 0.0365 - accuracy: 0.9724\n",
      "Epoch 101/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0326 - accuracy: 0.9793\n",
      "Epoch 102/200\n",
      "29/29 [==============================] - 0s 550us/step - loss: 0.0315 - accuracy: 0.9793\n",
      "Epoch 103/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 0.0324 - accuracy: 0.9793\n",
      "Epoch 104/200\n",
      "29/29 [==============================] - 0s 617us/step - loss: 0.0307 - accuracy: 0.9793\n",
      "Epoch 105/200\n",
      "29/29 [==============================] - 0s 587us/step - loss: 0.0288 - accuracy: 0.9862\n",
      "Epoch 106/200\n",
      "29/29 [==============================] - 0s 550us/step - loss: 0.0293 - accuracy: 0.9793\n",
      "Epoch 107/200\n",
      "29/29 [==============================] - 0s 585us/step - loss: 0.0285 - accuracy: 0.9793\n",
      "Epoch 108/200\n",
      "29/29 [==============================] - 0s 550us/step - loss: 0.0265 - accuracy: 0.9862\n",
      "Epoch 109/200\n",
      "29/29 [==============================] - 0s 617us/step - loss: 0.0271 - accuracy: 0.9793\n",
      "Epoch 110/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0286 - accuracy: 0.9931\n",
      "Epoch 111/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0266 - accuracy: 0.9862\n",
      "Epoch 112/200\n",
      "29/29 [==============================] - 0s 585us/step - loss: 0.0242 - accuracy: 0.9862\n",
      "Epoch 113/200\n",
      "29/29 [==============================] - 0s 621us/step - loss: 0.0361 - accuracy: 0.9586\n",
      "Epoch 114/200\n",
      "29/29 [==============================] - 0s 620us/step - loss: 0.0295 - accuracy: 0.9724\n",
      "Epoch 115/200\n",
      "29/29 [==============================] - 0s 616us/step - loss: 0.0239 - accuracy: 0.9862\n",
      "Epoch 116/200\n",
      "29/29 [==============================] - 0s 722us/step - loss: 0.0224 - accuracy: 0.9862\n",
      "Epoch 117/200\n",
      "29/29 [==============================] - 0s 584us/step - loss: 0.0211 - accuracy: 0.9931\n",
      "Epoch 118/200\n",
      "29/29 [==============================] - 0s 585us/step - loss: 0.0263 - accuracy: 0.9793\n",
      "Epoch 119/200\n",
      "29/29 [==============================] - 0s 723us/step - loss: 0.0214 - accuracy: 0.9931\n",
      "Epoch 120/200\n",
      "29/29 [==============================] - 0s 719us/step - loss: 0.0220 - accuracy: 0.9862\n",
      "Epoch 121/200\n",
      "29/29 [==============================] - 0s 685us/step - loss: 0.0263 - accuracy: 0.9793\n",
      "Epoch 122/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 0.0192 - accuracy: 0.9931\n",
      "Epoch 123/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0180 - accuracy: 0.9931\n",
      "Epoch 124/200\n",
      "29/29 [==============================] - 0s 583us/step - loss: 0.0207 - accuracy: 0.9931\n",
      "Epoch 125/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0261 - accuracy: 0.9862\n",
      "Epoch 126/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0210 - accuracy: 0.9862\n",
      "Epoch 127/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0174 - accuracy: 0.9931\n",
      "Epoch 128/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 0.0161 - accuracy: 0.9931\n",
      "Epoch 129/200\n",
      "29/29 [==============================] - 0s 585us/step - loss: 0.0171 - accuracy: 0.9931\n",
      "Epoch 130/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 0.0165 - accuracy: 0.9931\n",
      "Epoch 131/200\n",
      "29/29 [==============================] - 0s 585us/step - loss: 0.0157 - accuracy: 0.9862\n",
      "Epoch 132/200\n",
      "29/29 [==============================] - 0s 516us/step - loss: 0.0149 - accuracy: 0.9931\n",
      "Epoch 133/200\n",
      "29/29 [==============================] - 0s 551us/step - loss: 0.0141 - accuracy: 0.9931\n",
      "Epoch 134/200\n",
      "29/29 [==============================] - 0s 504us/step - loss: 0.0154 - accuracy: 0.9931\n",
      "Epoch 135/200\n",
      "29/29 [==============================] - 0s 481us/step - loss: 0.0131 - accuracy: 0.9931\n",
      "Epoch 136/200\n",
      "29/29 [==============================] - 0s 549us/step - loss: 0.0124 - accuracy: 0.9931\n",
      "Epoch 137/200\n",
      "29/29 [==============================] - 0s 513us/step - loss: 0.0115 - accuracy: 0.9931\n",
      "Epoch 138/200\n",
      "29/29 [==============================] - 0s 516us/step - loss: 0.0130 - accuracy: 0.9931\n",
      "Epoch 139/200\n",
      "29/29 [==============================] - 0s 582us/step - loss: 0.0188 - accuracy: 0.9931\n",
      "Epoch 140/200\n",
      "29/29 [==============================] - 0s 536us/step - loss: 0.0142 - accuracy: 0.9931\n",
      "Epoch 141/200\n",
      "29/29 [==============================] - 0s 550us/step - loss: 0.0113 - accuracy: 0.9931\n",
      "Epoch 142/200\n",
      "29/29 [==============================] - 0s 585us/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "29/29 [==============================] - 0s 550us/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "29/29 [==============================] - 0s 543us/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "29/29 [==============================] - 0s 506us/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "29/29 [==============================] - 0s 481us/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "29/29 [==============================] - 0s 535us/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "29/29 [==============================] - 0s 516us/step - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "29/29 [==============================] - 0s 550us/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "29/29 [==============================] - 0s 552us/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "29/29 [==============================] - 0s 493us/step - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "29/29 [==============================] - 0s 515us/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "29/29 [==============================] - 0s 587us/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "29/29 [==============================] - 0s 479us/step - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "29/29 [==============================] - 0s 482us/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "29/29 [==============================] - 0s 544us/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "29/29 [==============================] - 0s 577us/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "29/29 [==============================] - 0s 519us/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "29/29 [==============================] - 0s 503us/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "29/29 [==============================] - 0s 531us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "29/29 [==============================] - 0s 669us/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "29/29 [==============================] - 0s 647us/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "29/29 [==============================] - 0s 686us/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "29/29 [==============================] - 0s 683us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "29/29 [==============================] - 0s 643us/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "29/29 [==============================] - 0s 615us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "29/29 [==============================] - 0s 676us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "29/29 [==============================] - 0s 691us/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "29/29 [==============================] - 0s 757us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "29/29 [==============================] - 0s 706us/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "29/29 [==============================] - 0s 715us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "29/29 [==============================] - 0s 722us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "29/29 [==============================] - 0s 680us/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "29/29 [==============================] - 0s 685us/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "29/29 [==============================] - 0s 624us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "29/29 [==============================] - 0s 636us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "29/29 [==============================] - 0s 629us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "29/29 [==============================] - 0s 681us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "29/29 [==============================] - 0s 639us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "29/29 [==============================] - 0s 689us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "29/29 [==============================] - 0s 699us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "29/29 [==============================] - 0s 647us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "29/29 [==============================] - 0s 654us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "29/29 [==============================] - 0s 725us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "29/29 [==============================] - 0s 640us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "29/29 [==============================] - 0s 671us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "29/29 [==============================] - 0s 684us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "29/29 [==============================] - 0s 689us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "29/29 [==============================] - 0s 791us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "29/29 [==============================] - 0s 697us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "29/29 [==============================] - 0s 725us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "29/29 [==============================] - 0s 680us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "29/29 [==============================] - 0s 674us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001BBF794B318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0796 - accuracy: 0.9048\n",
      "Epoch 1/200\n",
      " 1/29 [>.............................] - ETA: 0s - loss: 0.2349 - accuracy: 0.6000WARNING:tensorflow:Callbacks method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_begin` time: 0.0010s). Check your callbacks.\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.2481 - accuracy: 0.5310\n",
      "Epoch 2/200\n",
      "29/29 [==============================] - 0s 585us/step - loss: 0.2351 - accuracy: 0.5586\n",
      "Epoch 3/200\n",
      "29/29 [==============================] - 0s 550us/step - loss: 0.2296 - accuracy: 0.5793\n",
      "Epoch 4/200\n",
      "29/29 [==============================] - 0s 620us/step - loss: 0.2235 - accuracy: 0.6483\n",
      "Epoch 5/200\n",
      "29/29 [==============================] - 0s 723us/step - loss: 0.2212 - accuracy: 0.6207\n",
      "Epoch 6/200\n",
      "29/29 [==============================] - 0s 719us/step - loss: 0.2168 - accuracy: 0.7241\n",
      "Epoch 7/200\n",
      "29/29 [==============================] - 0s 705us/step - loss: 0.2071 - accuracy: 0.6966\n",
      "Epoch 8/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.2032 - accuracy: 0.7172\n",
      "Epoch 9/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.1998 - accuracy: 0.6897\n",
      "Epoch 10/200\n",
      "29/29 [==============================] - 0s 757us/step - loss: 0.1928 - accuracy: 0.7310\n",
      "Epoch 11/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.1868 - accuracy: 0.7379\n",
      "Epoch 12/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.1839 - accuracy: 0.7586\n",
      "Epoch 13/200\n",
      "29/29 [==============================] - 0s 722us/step - loss: 0.1790 - accuracy: 0.7724\n",
      "Epoch 14/200\n",
      "29/29 [==============================] - 0s 756us/step - loss: 0.1739 - accuracy: 0.7448\n",
      "Epoch 15/200\n",
      "29/29 [==============================] - 0s 771us/step - loss: 0.1664 - accuracy: 0.7793\n",
      "Epoch 16/200\n",
      "29/29 [==============================] - 0s 706us/step - loss: 0.1651 - accuracy: 0.7862\n",
      "Epoch 17/200\n",
      "29/29 [==============================] - 0s 736us/step - loss: 0.1561 - accuracy: 0.7931\n",
      "Epoch 18/200\n",
      "29/29 [==============================] - 0s 721us/step - loss: 0.1524 - accuracy: 0.8207\n",
      "Epoch 19/200\n",
      "29/29 [==============================] - 0s 727us/step - loss: 0.1461 - accuracy: 0.8069\n",
      "Epoch 20/200\n",
      "29/29 [==============================] - 0s 722us/step - loss: 0.1413 - accuracy: 0.8207\n",
      "Epoch 21/200\n",
      "29/29 [==============================] - 0s 756us/step - loss: 0.1381 - accuracy: 0.8276\n",
      "Epoch 22/200\n",
      "29/29 [==============================] - 0s 815us/step - loss: 0.1385 - accuracy: 0.8345\n",
      "Epoch 23/200\n",
      "29/29 [==============================] - 0s 560us/step - loss: 0.1481 - accuracy: 0.7793\n",
      "Epoch 24/200\n",
      "29/29 [==============================] - 0s 500us/step - loss: 0.1291 - accuracy: 0.8552\n",
      "Epoch 25/200\n",
      "29/29 [==============================] - 0s 494us/step - loss: 0.1246 - accuracy: 0.8552\n",
      "Epoch 26/200\n",
      "29/29 [==============================] - 0s 501us/step - loss: 0.1216 - accuracy: 0.8483\n",
      "Epoch 27/200\n",
      "29/29 [==============================] - 0s 522us/step - loss: 0.1164 - accuracy: 0.8690\n",
      "Epoch 28/200\n",
      "29/29 [==============================] - 0s 486us/step - loss: 0.1213 - accuracy: 0.8552\n",
      "Epoch 29/200\n",
      "29/29 [==============================] - 0s 609us/step - loss: 0.1128 - accuracy: 0.8759\n",
      "Epoch 30/200\n",
      "29/29 [==============================] - 0s 659us/step - loss: 0.1144 - accuracy: 0.8621\n",
      "Epoch 31/200\n",
      "29/29 [==============================] - 0s 660us/step - loss: 0.1099 - accuracy: 0.8828\n",
      "Epoch 32/200\n",
      "29/29 [==============================] - 0s 644us/step - loss: 0.1073 - accuracy: 0.8828\n",
      "Epoch 33/200\n",
      "29/29 [==============================] - 0s 726us/step - loss: 0.1056 - accuracy: 0.8897\n",
      "Epoch 34/200\n",
      "29/29 [==============================] - 0s 694us/step - loss: 0.1041 - accuracy: 0.8828\n",
      "Epoch 35/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 684us/step - loss: 0.0988 - accuracy: 0.8897\n",
      "Epoch 36/200\n",
      "29/29 [==============================] - 0s 667us/step - loss: 0.1008 - accuracy: 0.8828\n",
      "Epoch 37/200\n",
      "29/29 [==============================] - 0s 661us/step - loss: 0.0976 - accuracy: 0.9034\n",
      "Epoch 38/200\n",
      "29/29 [==============================] - 0s 710us/step - loss: 0.1001 - accuracy: 0.8828\n",
      "Epoch 39/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0924 - accuracy: 0.9034\n",
      "Epoch 40/200\n",
      "29/29 [==============================] - 0s 708us/step - loss: 0.0898 - accuracy: 0.9034\n",
      "Epoch 41/200\n",
      "29/29 [==============================] - 0s 698us/step - loss: 0.0899 - accuracy: 0.9034\n",
      "Epoch 42/200\n",
      "29/29 [==============================] - 0s 723us/step - loss: 0.0851 - accuracy: 0.9103\n",
      "Epoch 43/200\n",
      "29/29 [==============================] - 0s 648us/step - loss: 0.0831 - accuracy: 0.9241\n",
      "Epoch 44/200\n",
      "29/29 [==============================] - 0s 746us/step - loss: 0.0818 - accuracy: 0.8966\n",
      "Epoch 45/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 0.0794 - accuracy: 0.9172\n",
      "Epoch 46/200\n",
      "29/29 [==============================] - 0s 671us/step - loss: 0.0789 - accuracy: 0.9241\n",
      "Epoch 47/200\n",
      "29/29 [==============================] - 0s 667us/step - loss: 0.0750 - accuracy: 0.9241\n",
      "Epoch 48/200\n",
      "29/29 [==============================] - 0s 691us/step - loss: 0.0780 - accuracy: 0.9172\n",
      "Epoch 49/200\n",
      "29/29 [==============================] - 0s 626us/step - loss: 0.0723 - accuracy: 0.9379\n",
      "Epoch 50/200\n",
      "29/29 [==============================] - 0s 676us/step - loss: 0.0714 - accuracy: 0.9310\n",
      "Epoch 51/200\n",
      "29/29 [==============================] - 0s 658us/step - loss: 0.0729 - accuracy: 0.9172\n",
      "Epoch 52/200\n",
      "29/29 [==============================] - 0s 648us/step - loss: 0.0702 - accuracy: 0.9310\n",
      "Epoch 53/200\n",
      "29/29 [==============================] - 0s 663us/step - loss: 0.0701 - accuracy: 0.9241\n",
      "Epoch 54/200\n",
      "29/29 [==============================] - 0s 744us/step - loss: 0.0657 - accuracy: 0.9517\n",
      "Epoch 55/200\n",
      "29/29 [==============================] - 0s 640us/step - loss: 0.0628 - accuracy: 0.9448\n",
      "Epoch 56/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0612 - accuracy: 0.9517\n",
      "Epoch 57/200\n",
      "29/29 [==============================] - 0s 680us/step - loss: 0.0616 - accuracy: 0.9379\n",
      "Epoch 58/200\n",
      "29/29 [==============================] - 0s 666us/step - loss: 0.0577 - accuracy: 0.9586\n",
      "Epoch 59/200\n",
      "29/29 [==============================] - 0s 665us/step - loss: 0.0633 - accuracy: 0.9379\n",
      "Epoch 60/200\n",
      "29/29 [==============================] - 0s 687us/step - loss: 0.0557 - accuracy: 0.9448\n",
      "Epoch 61/200\n",
      "29/29 [==============================] - 0s 668us/step - loss: 0.0560 - accuracy: 0.9379\n",
      "Epoch 62/200\n",
      "29/29 [==============================] - 0s 609us/step - loss: 0.0561 - accuracy: 0.9586\n",
      "Epoch 63/200\n",
      "29/29 [==============================] - 0s 657us/step - loss: 0.0539 - accuracy: 0.9517\n",
      "Epoch 64/200\n",
      "29/29 [==============================] - 0s 712us/step - loss: 0.0512 - accuracy: 0.9517\n",
      "Epoch 65/200\n",
      "29/29 [==============================] - 0s 692us/step - loss: 0.0560 - accuracy: 0.9517\n",
      "Epoch 66/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0477 - accuracy: 0.9586\n",
      "Epoch 67/200\n",
      "29/29 [==============================] - 0s 671us/step - loss: 0.0459 - accuracy: 0.9448\n",
      "Epoch 68/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0453 - accuracy: 0.9586\n",
      "Epoch 69/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0432 - accuracy: 0.9586\n",
      "Epoch 70/200\n",
      "29/29 [==============================] - 0s 686us/step - loss: 0.0418 - accuracy: 0.9586\n",
      "Epoch 71/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0448 - accuracy: 0.9724\n",
      "Epoch 72/200\n",
      "29/29 [==============================] - 0s 650us/step - loss: 0.0393 - accuracy: 0.9724\n",
      "Epoch 73/200\n",
      "29/29 [==============================] - 0s 639us/step - loss: 0.0371 - accuracy: 0.9724\n",
      "Epoch 74/200\n",
      "29/29 [==============================] - 0s 642us/step - loss: 0.0381 - accuracy: 0.9724\n",
      "Epoch 75/200\n",
      "29/29 [==============================] - 0s 660us/step - loss: 0.0371 - accuracy: 0.9793\n",
      "Epoch 76/200\n",
      "29/29 [==============================] - 0s 722us/step - loss: 0.0361 - accuracy: 0.9655\n",
      "Epoch 77/200\n",
      "29/29 [==============================] - 0s 721us/step - loss: 0.0389 - accuracy: 0.9724\n",
      "Epoch 78/200\n",
      "29/29 [==============================] - 0s 722us/step - loss: 0.0343 - accuracy: 0.9793\n",
      "Epoch 79/200\n",
      "29/29 [==============================] - 0s 689us/step - loss: 0.0348 - accuracy: 0.9862\n",
      "Epoch 80/200\n",
      "29/29 [==============================] - 0s 684us/step - loss: 0.0352 - accuracy: 0.9793\n",
      "Epoch 81/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0352 - accuracy: 0.9931\n",
      "Epoch 82/200\n",
      "29/29 [==============================] - 0s 773us/step - loss: 0.0315 - accuracy: 0.9724\n",
      "Epoch 83/200\n",
      "29/29 [==============================] - 0s 711us/step - loss: 0.0350 - accuracy: 0.9724\n",
      "Epoch 84/200\n",
      "29/29 [==============================] - 0s 660us/step - loss: 0.0274 - accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "29/29 [==============================] - 0s 638us/step - loss: 0.0259 - accuracy: 0.9931\n",
      "Epoch 86/200\n",
      "29/29 [==============================] - 0s 663us/step - loss: 0.0257 - accuracy: 0.9931\n",
      "Epoch 87/200\n",
      "29/29 [==============================] - 0s 667us/step - loss: 0.0242 - accuracy: 0.9931\n",
      "Epoch 88/200\n",
      "29/29 [==============================] - 0s 682us/step - loss: 0.0224 - accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "29/29 [==============================] - 0s 635us/step - loss: 0.0221 - accuracy: 0.9931\n",
      "Epoch 90/200\n",
      "29/29 [==============================] - 0s 640us/step - loss: 0.0218 - accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "29/29 [==============================] - 0s 668us/step - loss: 0.0207 - accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "29/29 [==============================] - 0s 689us/step - loss: 0.0217 - accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "29/29 [==============================] - 0s 687us/step - loss: 0.0191 - accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0190 - accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "29/29 [==============================] - 0s 648us/step - loss: 0.0198 - accuracy: 0.9931\n",
      "Epoch 96/200\n",
      "29/29 [==============================] - 0s 676us/step - loss: 0.0191 - accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "29/29 [==============================] - 0s 681us/step - loss: 0.0161 - accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 0.0222 - accuracy: 0.9931\n",
      "Epoch 99/200\n",
      "29/29 [==============================] - 0s 681us/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "29/29 [==============================] - 0s 659us/step - loss: 0.0177 - accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "29/29 [==============================] - 0s 633us/step - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "29/29 [==============================] - 0s 637us/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "29/29 [==============================] - 0s 703us/step - loss: 0.0160 - accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "29/29 [==============================] - 0s 701us/step - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "29/29 [==============================] - 0s 675us/step - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "29/29 [==============================] - 0s 633us/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "29/29 [==============================] - 0s 679us/step - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "29/29 [==============================] - 0s 683us/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "29/29 [==============================] - 0s 664us/step - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "29/29 [==============================] - 0s 704us/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "29/29 [==============================] - 0s 691us/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "29/29 [==============================] - 0s 636us/step - loss: 0.0148 - accuracy: 0.9931\n",
      "Epoch 114/200\n",
      "29/29 [==============================] - 0s 626us/step - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "29/29 [==============================] - 0s 650us/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "29/29 [==============================] - 0s 648us/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "29/29 [==============================] - 0s 675us/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "29/29 [==============================] - 0s 695us/step - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "29/29 [==============================] - 0s 674us/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "29/29 [==============================] - 0s 639us/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "29/29 [==============================] - 0s 629us/step - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "29/29 [==============================] - 0s 669us/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "29/29 [==============================] - 0s 618us/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "29/29 [==============================] - 0s 620us/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "29/29 [==============================] - 0s 708us/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "29/29 [==============================] - 0s 663us/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "29/29 [==============================] - 0s 618us/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "29/29 [==============================] - 0s 654us/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "29/29 [==============================] - 0s 659us/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "29/29 [==============================] - 0s 687us/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "29/29 [==============================] - 0s 621us/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "29/29 [==============================] - 0s 631us/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "29/29 [==============================] - 0s 668us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "29/29 [==============================] - 0s 661us/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "29/29 [==============================] - 0s 640us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "29/29 [==============================] - 0s 642us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "29/29 [==============================] - 0s 638us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "29/29 [==============================] - 0s 661us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "29/29 [==============================] - 0s 664us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "29/29 [==============================] - 0s 639us/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "29/29 [==============================] - 0s 611us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "29/29 [==============================] - 0s 663us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "29/29 [==============================] - 0s 603us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "29/29 [==============================] - 0s 718us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "29/29 [==============================] - 0s 694us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "29/29 [==============================] - 0s 684us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "29/29 [==============================] - 0s 723us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "29/29 [==============================] - 0s 662us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "29/29 [==============================] - 0s 716us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "29/29 [==============================] - 0s 702us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "29/29 [==============================] - 0s 714us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "29/29 [==============================] - 0s 690us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "29/29 [==============================] - 0s 656us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "29/29 [==============================] - ETA: 0s - loss: 8.7374e-05 - accuracy: 1.00 - 0s 691us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "29/29 [==============================] - 0s 736us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "29/29 [==============================] - 0s 708us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "29/29 [==============================] - 0s 640us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "29/29 [==============================] - 0s 624us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "29/29 [==============================] - 0s 690us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "29/29 [==============================] - 0s 695us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "29/29 [==============================] - 0s 675us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "29/29 [==============================] - 0s 670us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "29/29 [==============================] - 0s 697us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "29/29 [==============================] - 0s 707us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "29/29 [==============================] - 0s 644us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "29/29 [==============================] - 0s 683us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "29/29 [==============================] - 0s 700us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "29/29 [==============================] - 0s 679us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "29/29 [==============================] - 0s 603us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "29/29 [==============================] - 0s 674us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "29/29 [==============================] - 0s 631us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "29/29 [==============================] - 0s 630us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "29/29 [==============================] - 0s 694us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "29/29 [==============================] - 0s 649us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "29/29 [==============================] - 0s 668us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "29/29 [==============================] - 0s 622us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "29/29 [==============================] - 0s 668us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 9.9927e-04 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "29/29 [==============================] - 0s 631us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "29/29 [==============================] - 0s 705us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "29/29 [==============================] - 0s 643us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "29/29 [==============================] - 0s 719us/step - loss: 9.3234e-04 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "29/29 [==============================] - 0s 748us/step - loss: 9.2607e-04 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "29/29 [==============================] - 0s 772us/step - loss: 8.8894e-04 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "29/29 [==============================] - 0s 674us/step - loss: 8.6281e-04 - accuracy: 1.0000\n",
      "Epoch 194/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 642us/step - loss: 8.4363e-04 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "29/29 [==============================] - 0s 631us/step - loss: 8.3824e-04 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "29/29 [==============================] - 0s 660us/step - loss: 7.8174e-04 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 7.4838e-04 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 7.7418e-04 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "29/29 [==============================] - 0s 654us/step - loss: 7.6283e-04 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 7.2582e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001BBF933C318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 605us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 1/200\n",
      " 1/29 [>.............................] - ETA: 0s - loss: 0.2480 - accuracy: 0.6000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "29/29 [==============================] - 0s 603us/step - loss: 0.2514 - accuracy: 0.5310\n",
      "Epoch 2/200\n",
      "29/29 [==============================] - 0s 631us/step - loss: 0.2417 - accuracy: 0.5310\n",
      "Epoch 3/200\n",
      "29/29 [==============================] - 0s 544us/step - loss: 0.2354 - accuracy: 0.5448\n",
      "Epoch 4/200\n",
      "29/29 [==============================] - 0s 634us/step - loss: 0.2299 - accuracy: 0.5793\n",
      "Epoch 5/200\n",
      "29/29 [==============================] - 0s 616us/step - loss: 0.2270 - accuracy: 0.5862\n",
      "Epoch 6/200\n",
      "29/29 [==============================] - 0s 582us/step - loss: 0.2213 - accuracy: 0.6483\n",
      "Epoch 7/200\n",
      "29/29 [==============================] - 0s 591us/step - loss: 0.2118 - accuracy: 0.6483\n",
      "Epoch 8/200\n",
      "29/29 [==============================] - 0s 616us/step - loss: 0.2091 - accuracy: 0.6966\n",
      "Epoch 9/200\n",
      "29/29 [==============================] - 0s 656us/step - loss: 0.2038 - accuracy: 0.7172\n",
      "Epoch 10/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.1955 - accuracy: 0.7310\n",
      "Epoch 11/200\n",
      "29/29 [==============================] - 0s 684us/step - loss: 0.1899 - accuracy: 0.7172\n",
      "Epoch 12/200\n",
      "29/29 [==============================] - 0s 606us/step - loss: 0.1878 - accuracy: 0.7517\n",
      "Epoch 13/200\n",
      "29/29 [==============================] - 0s 606us/step - loss: 0.1812 - accuracy: 0.7517\n",
      "Epoch 14/200\n",
      "29/29 [==============================] - 0s 583us/step - loss: 0.1774 - accuracy: 0.7310\n",
      "Epoch 15/200\n",
      "29/29 [==============================] - 0s 656us/step - loss: 0.1692 - accuracy: 0.8000\n",
      "Epoch 16/200\n",
      "29/29 [==============================] - 0s 613us/step - loss: 0.1682 - accuracy: 0.7793\n",
      "Epoch 17/200\n",
      "29/29 [==============================] - 0s 661us/step - loss: 0.1608 - accuracy: 0.7793\n",
      "Epoch 18/200\n",
      "29/29 [==============================] - 0s 695us/step - loss: 0.1576 - accuracy: 0.7862\n",
      "Epoch 19/200\n",
      "29/29 [==============================] - 0s 646us/step - loss: 0.1519 - accuracy: 0.8000\n",
      "Epoch 20/200\n",
      "29/29 [==============================] - 0s 647us/step - loss: 0.1473 - accuracy: 0.7931\n",
      "Epoch 21/200\n",
      "29/29 [==============================] - 0s 601us/step - loss: 0.1459 - accuracy: 0.8138\n",
      "Epoch 22/200\n",
      "29/29 [==============================] - 0s 491us/step - loss: 0.1463 - accuracy: 0.7724\n",
      "Epoch 23/200\n",
      "29/29 [==============================] - 0s 412us/step - loss: 0.1512 - accuracy: 0.8000\n",
      "Epoch 24/200\n",
      "29/29 [==============================] - 0s 447us/step - loss: 0.1357 - accuracy: 0.8414\n",
      "Epoch 25/200\n",
      "29/29 [==============================] - 0s 498us/step - loss: 0.1323 - accuracy: 0.8414\n",
      "Epoch 26/200\n",
      "29/29 [==============================] - 0s 446us/step - loss: 0.1290 - accuracy: 0.8276\n",
      "Epoch 27/200\n",
      "29/29 [==============================] - 0s 561us/step - loss: 0.1266 - accuracy: 0.8207\n",
      "Epoch 28/200\n",
      "29/29 [==============================] - 0s 533us/step - loss: 0.1310 - accuracy: 0.8138\n",
      "Epoch 29/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 0.1215 - accuracy: 0.8345\n",
      "Epoch 30/200\n",
      "29/29 [==============================] - 0s 672us/step - loss: 0.1233 - accuracy: 0.8276\n",
      "Epoch 31/200\n",
      "29/29 [==============================] - 0s 693us/step - loss: 0.1193 - accuracy: 0.8552\n",
      "Epoch 32/200\n",
      "29/29 [==============================] - 0s 665us/step - loss: 0.1166 - accuracy: 0.8552\n",
      "Epoch 33/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 0.1146 - accuracy: 0.8690\n",
      "Epoch 34/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.1145 - accuracy: 0.8690\n",
      "Epoch 35/200\n",
      "29/29 [==============================] - 0s 665us/step - loss: 0.1088 - accuracy: 0.8759\n",
      "Epoch 36/200\n",
      "29/29 [==============================] - 0s 642us/step - loss: 0.1110 - accuracy: 0.8621\n",
      "Epoch 37/200\n",
      "29/29 [==============================] - 0s 680us/step - loss: 0.1096 - accuracy: 0.8828\n",
      "Epoch 38/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.1094 - accuracy: 0.8621\n",
      "Epoch 39/200\n",
      "29/29 [==============================] - 0s 674us/step - loss: 0.1043 - accuracy: 0.9103\n",
      "Epoch 40/200\n",
      "29/29 [==============================] - 0s 659us/step - loss: 0.1017 - accuracy: 0.8759\n",
      "Epoch 41/200\n",
      "29/29 [==============================] - 0s 630us/step - loss: 0.1036 - accuracy: 0.8552\n",
      "Epoch 42/200\n",
      "29/29 [==============================] - 0s 656us/step - loss: 0.0975 - accuracy: 0.8828\n",
      "Epoch 43/200\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.0956 - accuracy: 0.8759\n",
      "Epoch 44/200\n",
      "29/29 [==============================] - 0s 645us/step - loss: 0.0939 - accuracy: 0.8966\n",
      "Epoch 45/200\n",
      "29/29 [==============================] - 0s 647us/step - loss: 0.0966 - accuracy: 0.8966\n",
      "Epoch 46/200\n",
      "29/29 [==============================] - 0s 678us/step - loss: 0.0917 - accuracy: 0.9034\n",
      "Epoch 47/200\n",
      "29/29 [==============================] - 0s 682us/step - loss: 0.0882 - accuracy: 0.9172\n",
      "Epoch 48/200\n",
      "29/29 [==============================] - 0s 686us/step - loss: 0.0903 - accuracy: 0.8828\n",
      "Epoch 49/200\n",
      "29/29 [==============================] - 0s 788us/step - loss: 0.0851 - accuracy: 0.9034\n",
      "Epoch 50/200\n",
      "29/29 [==============================] - 0s 636us/step - loss: 0.0876 - accuracy: 0.8966\n",
      "Epoch 51/200\n",
      "29/29 [==============================] - 0s 724us/step - loss: 0.0867 - accuracy: 0.8828\n",
      "Epoch 52/200\n",
      "29/29 [==============================] - 0s 675us/step - loss: 0.0838 - accuracy: 0.8966\n",
      "Epoch 53/200\n",
      "29/29 [==============================] - 0s 660us/step - loss: 0.0834 - accuracy: 0.9310\n",
      "Epoch 54/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 0.0806 - accuracy: 0.9034\n",
      "Epoch 55/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0760 - accuracy: 0.9103\n",
      "Epoch 56/200\n",
      "29/29 [==============================] - 0s 625us/step - loss: 0.0756 - accuracy: 0.9310\n",
      "Epoch 57/200\n",
      "29/29 [==============================] - 0s 656us/step - loss: 0.0749 - accuracy: 0.9034\n",
      "Epoch 58/200\n",
      "29/29 [==============================] - 0s 671us/step - loss: 0.0701 - accuracy: 0.9241\n",
      "Epoch 59/200\n",
      "29/29 [==============================] - 0s 680us/step - loss: 0.0708 - accuracy: 0.9379\n",
      "Epoch 60/200\n",
      "29/29 [==============================] - 0s 649us/step - loss: 0.0708 - accuracy: 0.9241\n",
      "Epoch 61/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 0.0704 - accuracy: 0.9172\n",
      "Epoch 62/200\n",
      "29/29 [==============================] - 0s 550us/step - loss: 0.0754 - accuracy: 0.9310\n",
      "Epoch 63/200\n",
      "29/29 [==============================] - 0s 689us/step - loss: 0.0694 - accuracy: 0.9241\n",
      "Epoch 64/200\n",
      "29/29 [==============================] - 0s 657us/step - loss: 0.0657 - accuracy: 0.9448\n",
      "Epoch 65/200\n",
      "29/29 [==============================] - 0s 666us/step - loss: 0.0679 - accuracy: 0.9241\n",
      "Epoch 66/200\n",
      "29/29 [==============================] - 0s 659us/step - loss: 0.0609 - accuracy: 0.9517\n",
      "Epoch 67/200\n",
      "29/29 [==============================] - 0s 643us/step - loss: 0.0596 - accuracy: 0.9586\n",
      "Epoch 68/200\n",
      "29/29 [==============================] - 0s 650us/step - loss: 0.0590 - accuracy: 0.9448\n",
      "Epoch 69/200\n",
      "29/29 [==============================] - 0s 620us/step - loss: 0.0565 - accuracy: 0.9517\n",
      "Epoch 70/200\n",
      "29/29 [==============================] - 0s 606us/step - loss: 0.0539 - accuracy: 0.9448\n",
      "Epoch 71/200\n",
      "29/29 [==============================] - 0s 645us/step - loss: 0.0591 - accuracy: 0.9310\n",
      "Epoch 72/200\n",
      "29/29 [==============================] - 0s 665us/step - loss: 0.0521 - accuracy: 0.9517\n",
      "Epoch 73/200\n",
      "29/29 [==============================] - 0s 701us/step - loss: 0.0511 - accuracy: 0.9586\n",
      "Epoch 74/200\n",
      "29/29 [==============================] - 0s 670us/step - loss: 0.0526 - accuracy: 0.9448\n",
      "Epoch 75/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0503 - accuracy: 0.9655\n",
      "Epoch 76/200\n",
      "29/29 [==============================] - 0s 694us/step - loss: 0.0496 - accuracy: 0.9448\n",
      "Epoch 77/200\n",
      "29/29 [==============================] - 0s 705us/step - loss: 0.0573 - accuracy: 0.9448\n",
      "Epoch 78/200\n",
      "29/29 [==============================] - 0s 762us/step - loss: 0.0475 - accuracy: 0.9586\n",
      "Epoch 79/200\n",
      "29/29 [==============================] - 0s 703us/step - loss: 0.0465 - accuracy: 0.9724\n",
      "Epoch 80/200\n",
      "29/29 [==============================] - 0s 656us/step - loss: 0.0508 - accuracy: 0.9655\n",
      "Epoch 81/200\n",
      "29/29 [==============================] - 0s 700us/step - loss: 0.0482 - accuracy: 0.9586\n",
      "Epoch 82/200\n",
      "29/29 [==============================] - 0s 723us/step - loss: 0.0427 - accuracy: 0.9724\n",
      "Epoch 83/200\n",
      "29/29 [==============================] - 0s 717us/step - loss: 0.0416 - accuracy: 0.9655\n",
      "Epoch 84/200\n",
      "29/29 [==============================] - 0s 639us/step - loss: 0.0408 - accuracy: 0.9724\n",
      "Epoch 85/200\n",
      "29/29 [==============================] - 0s 670us/step - loss: 0.0368 - accuracy: 0.9793\n",
      "Epoch 86/200\n",
      "29/29 [==============================] - 0s 645us/step - loss: 0.0370 - accuracy: 0.9655\n",
      "Epoch 87/200\n",
      "29/29 [==============================] - 0s 680us/step - loss: 0.0347 - accuracy: 0.9793\n",
      "Epoch 88/200\n",
      "29/29 [==============================] - 0s 673us/step - loss: 0.0337 - accuracy: 0.9793\n",
      "Epoch 89/200\n",
      "29/29 [==============================] - 0s 601us/step - loss: 0.0328 - accuracy: 0.9724\n",
      "Epoch 90/200\n",
      "29/29 [==============================] - 0s 695us/step - loss: 0.0322 - accuracy: 0.9724\n",
      "Epoch 91/200\n",
      "29/29 [==============================] - 0s 643us/step - loss: 0.0315 - accuracy: 0.9793\n",
      "Epoch 92/200\n",
      "29/29 [==============================] - 0s 662us/step - loss: 0.0331 - accuracy: 0.9793\n",
      "Epoch 93/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0275 - accuracy: 0.9793\n",
      "Epoch 94/200\n",
      "29/29 [==============================] - 0s 681us/step - loss: 0.0303 - accuracy: 0.9793\n",
      "Epoch 95/200\n",
      "29/29 [==============================] - 0s 668us/step - loss: 0.0294 - accuracy: 0.9931\n",
      "Epoch 96/200\n",
      "29/29 [==============================] - 0s 713us/step - loss: 0.0295 - accuracy: 0.9862\n",
      "Epoch 97/200\n",
      "29/29 [==============================] - 0s 649us/step - loss: 0.0249 - accuracy: 0.9862\n",
      "Epoch 98/200\n",
      "29/29 [==============================] - 0s 681us/step - loss: 0.0258 - accuracy: 0.9931\n",
      "Epoch 99/200\n",
      "29/29 [==============================] - 0s 647us/step - loss: 0.0252 - accuracy: 0.9862\n",
      "Epoch 100/200\n",
      "29/29 [==============================] - 0s 628us/step - loss: 0.0296 - accuracy: 0.9793\n",
      "Epoch 101/200\n",
      "29/29 [==============================] - 0s 693us/step - loss: 0.0224 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "29/29 [==============================] - 0s 686us/step - loss: 0.0237 - accuracy: 0.9931\n",
      "Epoch 103/200\n",
      "29/29 [==============================] - 0s 649us/step - loss: 0.0253 - accuracy: 0.9862\n",
      "Epoch 104/200\n",
      "29/29 [==============================] - 0s 585us/step - loss: 0.0210 - accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "29/29 [==============================] - 0s 704us/step - loss: 0.0196 - accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "29/29 [==============================] - 0s 669us/step - loss: 0.0204 - accuracy: 0.9931\n",
      "Epoch 107/200\n",
      "29/29 [==============================] - 0s 624us/step - loss: 0.0229 - accuracy: 0.9931\n",
      "Epoch 108/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0192 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "29/29 [==============================] - 0s 693us/step - loss: 0.0178 - accuracy: 0.9931\n",
      "Epoch 110/200\n",
      "29/29 [==============================] - 0s 664us/step - loss: 0.0184 - accuracy: 0.9931\n",
      "Epoch 111/200\n",
      "29/29 [==============================] - 0s 628us/step - loss: 0.0160 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0212 - accuracy: 0.9862\n",
      "Epoch 114/200\n",
      "29/29 [==============================] - 0s 697us/step - loss: 0.0198 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "29/29 [==============================] - 0s 646us/step - loss: 0.0152 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "29/29 [==============================] - 0s 661us/step - loss: 0.0141 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "29/29 [==============================] - 0s 709us/step - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 0.0165 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "29/29 [==============================] - 0s 626us/step - loss: 0.0149 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "29/29 [==============================] - 0s 612us/step - loss: 0.0145 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "29/29 [==============================] - 0s 678us/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "29/29 [==============================] - 0s 643us/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "29/29 [==============================] - 0s 625us/step - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "29/29 [==============================] - 0s 695us/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "29/29 [==============================] - 0s 649us/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "29/29 [==============================] - 0s 680us/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "29/29 [==============================] - 0s 659us/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "29/29 [==============================] - 0s 686us/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "29/29 [==============================] - 0s 659us/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "29/29 [==============================] - 0s 649us/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "29/29 [==============================] - 0s 607us/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "29/29 [==============================] - 0s 614us/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "29/29 [==============================] - 0s 638us/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "29/29 [==============================] - 0s 632us/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "29/29 [==============================] - 0s 638us/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "29/29 [==============================] - 0s 669us/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "29/29 [==============================] - 0s 681us/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "29/29 [==============================] - 0s 661us/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 143/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 666us/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "29/29 [==============================] - 0s 668us/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "29/29 [==============================] - 0s 710us/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "29/29 [==============================] - 0s 608us/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "29/29 [==============================] - 0s 721us/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "29/29 [==============================] - 0s 639us/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "29/29 [==============================] - 0s 650us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "29/29 [==============================] - 0s 681us/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "29/29 [==============================] - 0s 646us/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "29/29 [==============================] - 0s 724us/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "29/29 [==============================] - 0s 663us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "29/29 [==============================] - 0s 691us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "29/29 [==============================] - 0s 712us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "29/29 [==============================] - 0s 673us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "29/29 [==============================] - 0s 695us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "29/29 [==============================] - 0s 647us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "29/29 [==============================] - 0s 670us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "29/29 [==============================] - 0s 690us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "29/29 [==============================] - 0s 582us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "29/29 [==============================] - 0s 564us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "29/29 [==============================] - 0s 576us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "29/29 [==============================] - 0s 585us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "29/29 [==============================] - 0s 596us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "29/29 [==============================] - 0s 551us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "29/29 [==============================] - 0s 588us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "29/29 [==============================] - 0s 649us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "29/29 [==============================] - 0s 650us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "29/29 [==============================] - 0s 659us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "29/29 [==============================] - 0s 669us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "29/29 [==============================] - 0s 588us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "29/29 [==============================] - 0s 694us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "29/29 [==============================] - 0s 687us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "29/29 [==============================] - 0s 615us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "29/29 [==============================] - 0s 754us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "29/29 [==============================] - 0s 719us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "29/29 [==============================] - 0s 722us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "29/29 [==============================] - 0s 741us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "29/29 [==============================] - 0s 718us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "29/29 [==============================] - 0s 747us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "29/29 [==============================] - 0s 719us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "29/29 [==============================] - 0s 727us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "29/29 [==============================] - 0s 685us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "29/29 [==============================] - 0s 694us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "29/29 [==============================] - 0s 681us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "29/29 [==============================] - 0s 627us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "29/29 [==============================] - 0s 623us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "29/29 [==============================] - 0s 678us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "29/29 [==============================] - 0s 685us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "29/29 [==============================] - 0s 674us/step - loss: 9.9204e-04 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "29/29 [==============================] - 0s 621us/step - loss: 9.8039e-04 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "29/29 [==============================] - 0s 694us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 9.4537e-04 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "29/29 [==============================] - 0s 672us/step - loss: 9.5379e-04 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "29/29 [==============================] - 0s 681us/step - loss: 8.7499e-04 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "29/29 [==============================] - 0s 735us/step - loss: 9.0897e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001BBF9867EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0797 - accuracy: 0.9048\n",
      "Epoch 1/200\n",
      "29/29 [==============================] - 0s 584us/step - loss: 0.2552 - accuracy: 0.5034\n",
      "Epoch 2/200\n",
      "29/29 [==============================] - 0s 550us/step - loss: 0.2422 - accuracy: 0.6000\n",
      "Epoch 3/200\n",
      "29/29 [==============================] - 0s 585us/step - loss: 0.2321 - accuracy: 0.6345\n",
      "Epoch 4/200\n",
      "29/29 [==============================] - 0s 550us/step - loss: 0.2234 - accuracy: 0.6897\n",
      "Epoch 5/200\n",
      "29/29 [==============================] - 0s 585us/step - loss: 0.2176 - accuracy: 0.6759\n",
      "Epoch 6/200\n",
      "29/29 [==============================] - 0s 551us/step - loss: 0.2119 - accuracy: 0.6483\n",
      "Epoch 7/200\n",
      "29/29 [==============================] - 0s 516us/step - loss: 0.1942 - accuracy: 0.7517\n",
      "Epoch 8/200\n",
      "29/29 [==============================] - 0s 516us/step - loss: 0.1881 - accuracy: 0.7241\n",
      "Epoch 9/200\n",
      "29/29 [==============================] - 0s 550us/step - loss: 0.1834 - accuracy: 0.7310\n",
      "Epoch 10/200\n",
      "29/29 [==============================] - 0s 550us/step - loss: 0.1737 - accuracy: 0.7586\n",
      "Epoch 11/200\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.2224 - accuracy: 0.60 - 0s 550us/step - loss: 0.1615 - accuracy: 0.7448\n",
      "Epoch 12/200\n",
      "29/29 [==============================] - 0s 517us/step - loss: 0.1593 - accuracy: 0.7586\n",
      "Epoch 13/200\n",
      "29/29 [==============================] - 0s 511us/step - loss: 0.1545 - accuracy: 0.7931\n",
      "Epoch 14/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 534us/step - loss: 0.1531 - accuracy: 0.7793\n",
      "Epoch 15/200\n",
      "29/29 [==============================] - 0s 505us/step - loss: 0.1484 - accuracy: 0.8000\n",
      "Epoch 16/200\n",
      "29/29 [==============================] - 0s 450us/step - loss: 0.1489 - accuracy: 0.7793\n",
      "Epoch 17/200\n",
      "29/29 [==============================] - 0s 567us/step - loss: 0.1414 - accuracy: 0.7931\n",
      "Epoch 18/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.1401 - accuracy: 0.8138\n",
      "Epoch 19/200\n",
      "29/29 [==============================] - 0s 626us/step - loss: 0.1381 - accuracy: 0.8000\n",
      "Epoch 20/200\n",
      "29/29 [==============================] - 0s 645us/step - loss: 0.1363 - accuracy: 0.8138\n",
      "Epoch 21/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.1331 - accuracy: 0.8207\n",
      "Epoch 22/200\n",
      "29/29 [==============================] - 0s 707us/step - loss: 0.1340 - accuracy: 0.8345\n",
      "Epoch 23/200\n",
      "29/29 [==============================] - 0s 678us/step - loss: 0.1445 - accuracy: 0.7793\n",
      "Epoch 24/200\n",
      "29/29 [==============================] - 0s 638us/step - loss: 0.1286 - accuracy: 0.8138\n",
      "Epoch 25/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.1248 - accuracy: 0.8207\n",
      "Epoch 26/200\n",
      "29/29 [==============================] - 0s 654us/step - loss: 0.1229 - accuracy: 0.8345\n",
      "Epoch 27/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 0.1196 - accuracy: 0.8414\n",
      "Epoch 28/200\n",
      "29/29 [==============================] - 0s 693us/step - loss: 0.1272 - accuracy: 0.8207\n",
      "Epoch 29/200\n",
      "29/29 [==============================] - 0s 650us/step - loss: 0.1155 - accuracy: 0.8483\n",
      "Epoch 30/200\n",
      "29/29 [==============================] - 0s 626us/step - loss: 0.1201 - accuracy: 0.8414\n",
      "Epoch 31/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.1151 - accuracy: 0.8552\n",
      "Epoch 32/200\n",
      "29/29 [==============================] - 0s 638us/step - loss: 0.1117 - accuracy: 0.8759\n",
      "Epoch 33/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 0.1122 - accuracy: 0.8345\n",
      "Epoch 34/200\n",
      "29/29 [==============================] - 0s 666us/step - loss: 0.1130 - accuracy: 0.8483\n",
      "Epoch 35/200\n",
      "29/29 [==============================] - 0s 687us/step - loss: 0.1044 - accuracy: 0.8690\n",
      "Epoch 36/200\n",
      "29/29 [==============================] - 0s 648us/step - loss: 0.1086 - accuracy: 0.8690\n",
      "Epoch 37/200\n",
      "29/29 [==============================] - 0s 583us/step - loss: 0.1075 - accuracy: 0.8897\n",
      "Epoch 38/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.1055 - accuracy: 0.8690\n",
      "Epoch 39/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0988 - accuracy: 0.8897\n",
      "Epoch 40/200\n",
      "29/29 [==============================] - 0s 689us/step - loss: 0.0976 - accuracy: 0.8828\n",
      "Epoch 41/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 0.0981 - accuracy: 0.8897\n",
      "Epoch 42/200\n",
      "29/29 [==============================] - 0s 712us/step - loss: 0.0919 - accuracy: 0.9034\n",
      "Epoch 43/200\n",
      "29/29 [==============================] - 0s 692us/step - loss: 0.0921 - accuracy: 0.8897\n",
      "Epoch 44/200\n",
      "29/29 [==============================] - 0s 685us/step - loss: 0.0887 - accuracy: 0.8897\n",
      "Epoch 45/200\n",
      "29/29 [==============================] - 0s 671us/step - loss: 0.0895 - accuracy: 0.8897\n",
      "Epoch 46/200\n",
      "29/29 [==============================] - 0s 649us/step - loss: 0.0909 - accuracy: 0.8966\n",
      "Epoch 47/200\n",
      "29/29 [==============================] - 0s 647us/step - loss: 0.0844 - accuracy: 0.9034\n",
      "Epoch 48/200\n",
      "29/29 [==============================] - 0s 598us/step - loss: 0.0848 - accuracy: 0.8897\n",
      "Epoch 49/200\n",
      "29/29 [==============================] - 0s 700us/step - loss: 0.0794 - accuracy: 0.9172\n",
      "Epoch 50/200\n",
      "29/29 [==============================] - 0s 696us/step - loss: 0.0821 - accuracy: 0.8897\n",
      "Epoch 51/200\n",
      "29/29 [==============================] - 0s 542us/step - loss: 0.0811 - accuracy: 0.9034\n",
      "Epoch 52/200\n",
      "29/29 [==============================] - 0s 519us/step - loss: 0.0791 - accuracy: 0.9172\n",
      "Epoch 53/200\n",
      "29/29 [==============================] - 0s 525us/step - loss: 0.0824 - accuracy: 0.9172\n",
      "Epoch 54/200\n",
      "29/29 [==============================] - 0s 720us/step - loss: 0.0755 - accuracy: 0.8966\n",
      "Epoch 55/200\n",
      "29/29 [==============================] - 0s 670us/step - loss: 0.0703 - accuracy: 0.9310\n",
      "Epoch 56/200\n",
      "29/29 [==============================] - 0s 607us/step - loss: 0.0691 - accuracy: 0.9379\n",
      "Epoch 57/200\n",
      "29/29 [==============================] - 0s 696us/step - loss: 0.0688 - accuracy: 0.9103\n",
      "Epoch 58/200\n",
      "29/29 [==============================] - 0s 692us/step - loss: 0.0671 - accuracy: 0.9310\n",
      "Epoch 59/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0664 - accuracy: 0.9241\n",
      "Epoch 60/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.0634 - accuracy: 0.9448\n",
      "Epoch 61/200\n",
      "29/29 [==============================] - 0s 648us/step - loss: 0.0693 - accuracy: 0.9172\n",
      "Epoch 62/200\n",
      "29/29 [==============================] - 0s 671us/step - loss: 0.0695 - accuracy: 0.9172\n",
      "Epoch 63/200\n",
      "29/29 [==============================] - 0s 623us/step - loss: 0.0637 - accuracy: 0.9379\n",
      "Epoch 64/200\n",
      "29/29 [==============================] - 0s 671us/step - loss: 0.0594 - accuracy: 0.9448\n",
      "Epoch 65/200\n",
      "29/29 [==============================] - 0s 669us/step - loss: 0.0657 - accuracy: 0.9172\n",
      "Epoch 66/200\n",
      "29/29 [==============================] - 0s 676us/step - loss: 0.0563 - accuracy: 0.9586\n",
      "Epoch 67/200\n",
      "29/29 [==============================] - 0s 629us/step - loss: 0.0561 - accuracy: 0.9517\n",
      "Epoch 68/200\n",
      "29/29 [==============================] - 0s 645us/step - loss: 0.0541 - accuracy: 0.9586\n",
      "Epoch 69/200\n",
      "29/29 [==============================] - 0s 672us/step - loss: 0.0537 - accuracy: 0.9517\n",
      "Epoch 70/200\n",
      "29/29 [==============================] - 0s 646us/step - loss: 0.0502 - accuracy: 0.9586\n",
      "Epoch 71/200\n",
      "29/29 [==============================] - 0s 620us/step - loss: 0.0537 - accuracy: 0.9517\n",
      "Epoch 72/200\n",
      "29/29 [==============================] - 0s 627us/step - loss: 0.0517 - accuracy: 0.9517\n",
      "Epoch 73/200\n",
      "29/29 [==============================] - 0s 631us/step - loss: 0.0471 - accuracy: 0.9724\n",
      "Epoch 74/200\n",
      "29/29 [==============================] - 0s 728us/step - loss: 0.0515 - accuracy: 0.9586\n",
      "Epoch 75/200\n",
      "29/29 [==============================] - 0s 769us/step - loss: 0.0460 - accuracy: 0.9655\n",
      "Epoch 76/200\n",
      "29/29 [==============================] - 0s 718us/step - loss: 0.0473 - accuracy: 0.9655\n",
      "Epoch 77/200\n",
      "29/29 [==============================] - 0s 762us/step - loss: 0.0608 - accuracy: 0.9310\n",
      "Epoch 78/200\n",
      "29/29 [==============================] - 0s 700us/step - loss: 0.0457 - accuracy: 0.9655\n",
      "Epoch 79/200\n",
      "29/29 [==============================] - 0s 661us/step - loss: 0.0483 - accuracy: 0.9586\n",
      "Epoch 80/200\n",
      "29/29 [==============================] - 0s 675us/step - loss: 0.0547 - accuracy: 0.9448\n",
      "Epoch 81/200\n",
      "29/29 [==============================] - 0s 687us/step - loss: 0.0459 - accuracy: 0.9655\n",
      "Epoch 82/200\n",
      "29/29 [==============================] - 0s 745us/step - loss: 0.0424 - accuracy: 0.9655\n",
      "Epoch 83/200\n",
      "29/29 [==============================] - 0s 679us/step - loss: 0.0455 - accuracy: 0.9448\n",
      "Epoch 84/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0407 - accuracy: 0.9724\n",
      "Epoch 85/200\n",
      "29/29 [==============================] - 0s 828us/step - loss: 0.0382 - accuracy: 0.9724\n",
      "Epoch 86/200\n",
      "29/29 [==============================] - 0s 686us/step - loss: 0.0361 - accuracy: 0.9862\n",
      "Epoch 87/200\n",
      "29/29 [==============================] - 0s 733us/step - loss: 0.0367 - accuracy: 0.9724\n",
      "Epoch 88/200\n",
      "29/29 [==============================] - 0s 657us/step - loss: 0.0353 - accuracy: 0.9655\n",
      "Epoch 89/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0335 - accuracy: 0.9793\n",
      "Epoch 90/200\n",
      "29/29 [==============================] - 0s 739us/step - loss: 0.0342 - accuracy: 0.9793\n",
      "Epoch 91/200\n",
      "29/29 [==============================] - 0s 685us/step - loss: 0.0308 - accuracy: 0.9931\n",
      "Epoch 92/200\n",
      "29/29 [==============================] - 0s 687us/step - loss: 0.0343 - accuracy: 0.9862\n",
      "Epoch 93/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0297 - accuracy: 0.9793\n",
      "Epoch 94/200\n",
      "29/29 [==============================] - 0s 671us/step - loss: 0.0330 - accuracy: 0.9862\n",
      "Epoch 95/200\n",
      "29/29 [==============================] - 0s 695us/step - loss: 0.0302 - accuracy: 0.9931\n",
      "Epoch 96/200\n",
      "29/29 [==============================] - 0s 721us/step - loss: 0.0361 - accuracy: 0.9724\n",
      "Epoch 97/200\n",
      "29/29 [==============================] - 0s 693us/step - loss: 0.0261 - accuracy: 0.9862\n",
      "Epoch 98/200\n",
      "29/29 [==============================] - 0s 836us/step - loss: 0.0330 - accuracy: 0.9793\n",
      "Epoch 99/200\n",
      "29/29 [==============================] - 0s 676us/step - loss: 0.0353 - accuracy: 0.9655\n",
      "Epoch 100/200\n",
      "29/29 [==============================] - 0s 666us/step - loss: 0.0383 - accuracy: 0.9724\n",
      "Epoch 101/200\n",
      "29/29 [==============================] - 0s 621us/step - loss: 0.0267 - accuracy: 0.9793\n",
      "Epoch 102/200\n",
      "29/29 [==============================] - 0s 494us/step - loss: 0.0237 - accuracy: 0.9931\n",
      "Epoch 103/200\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0259 - accuracy: 0.9862\n",
      "Epoch 104/200\n",
      "29/29 [==============================] - 0s 669us/step - loss: 0.0238 - accuracy: 0.9862\n",
      "Epoch 105/200\n",
      "29/29 [==============================] - 0s 662us/step - loss: 0.0236 - accuracy: 0.9793\n",
      "Epoch 106/200\n",
      "29/29 [==============================] - 0s 738us/step - loss: 0.0266 - accuracy: 0.9862\n",
      "Epoch 107/200\n",
      "29/29 [==============================] - 0s 618us/step - loss: 0.0234 - accuracy: 0.9931\n",
      "Epoch 108/200\n",
      "29/29 [==============================] - 0s 745us/step - loss: 0.0224 - accuracy: 0.9931\n",
      "Epoch 109/200\n",
      "29/29 [==============================] - 0s 709us/step - loss: 0.0225 - accuracy: 0.9862\n",
      "Epoch 110/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 0.0234 - accuracy: 0.9862\n",
      "Epoch 111/200\n",
      "29/29 [==============================] - 0s 686us/step - loss: 0.0207 - accuracy: 0.9862\n",
      "Epoch 112/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0194 - accuracy: 0.9862\n",
      "Epoch 113/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 0.0346 - accuracy: 0.9724\n",
      "Epoch 114/200\n",
      "29/29 [==============================] - 0s 697us/step - loss: 0.0290 - accuracy: 0.9655\n",
      "Epoch 115/200\n",
      "29/29 [==============================] - 0s 711us/step - loss: 0.0199 - accuracy: 0.9931\n",
      "Epoch 116/200\n",
      "29/29 [==============================] - 0s 684us/step - loss: 0.0189 - accuracy: 0.9931\n",
      "Epoch 117/200\n",
      "29/29 [==============================] - 0s 672us/step - loss: 0.0192 - accuracy: 0.9931\n",
      "Epoch 118/200\n",
      "29/29 [==============================] - 0s 725us/step - loss: 0.0198 - accuracy: 0.9931\n",
      "Epoch 119/200\n",
      "29/29 [==============================] - 0s 679us/step - loss: 0.0178 - accuracy: 0.9931\n",
      "Epoch 120/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0197 - accuracy: 0.9931\n",
      "Epoch 121/200\n",
      "29/29 [==============================] - 0s 675us/step - loss: 0.0190 - accuracy: 0.9931\n",
      "Epoch 122/200\n",
      "29/29 [==============================] - 0s 673us/step - loss: 0.0168 - accuracy: 0.9931\n",
      "Epoch 123/200\n",
      "29/29 [==============================] - 0s 666us/step - loss: 0.0174 - accuracy: 0.9862\n",
      "Epoch 124/200\n",
      "29/29 [==============================] - 0s 678us/step - loss: 0.0162 - accuracy: 0.9931\n",
      "Epoch 125/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0203 - accuracy: 0.9931\n",
      "Epoch 126/200\n",
      "29/29 [==============================] - 0s 647us/step - loss: 0.0223 - accuracy: 0.9793\n",
      "Epoch 127/200\n",
      "29/29 [==============================] - 0s 713us/step - loss: 0.0181 - accuracy: 0.9931\n",
      "Epoch 128/200\n",
      "29/29 [==============================] - 0s 638us/step - loss: 0.0157 - accuracy: 0.9931\n",
      "Epoch 129/200\n",
      "29/29 [==============================] - 0s 669us/step - loss: 0.0150 - accuracy: 0.9931\n",
      "Epoch 130/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0150 - accuracy: 0.9931\n",
      "Epoch 131/200\n",
      "29/29 [==============================] - 0s 644us/step - loss: 0.0143 - accuracy: 0.9931\n",
      "Epoch 132/200\n",
      "29/29 [==============================] - 0s 648us/step - loss: 0.0150 - accuracy: 0.9931\n",
      "Epoch 133/200\n",
      "29/29 [==============================] - 0s 657us/step - loss: 0.0147 - accuracy: 0.9931\n",
      "Epoch 134/200\n",
      "29/29 [==============================] - 0s 643us/step - loss: 0.0177 - accuracy: 0.9862\n",
      "Epoch 135/200\n",
      "29/29 [==============================] - 0s 638us/step - loss: 0.0145 - accuracy: 0.9931\n",
      "Epoch 136/200\n",
      "29/29 [==============================] - 0s 650us/step - loss: 0.0135 - accuracy: 0.9931\n",
      "Epoch 137/200\n",
      "29/29 [==============================] - 0s 654us/step - loss: 0.0131 - accuracy: 0.9931\n",
      "Epoch 138/200\n",
      "29/29 [==============================] - 0s 708us/step - loss: 0.0129 - accuracy: 0.9931\n",
      "Epoch 139/200\n",
      "29/29 [==============================] - 0s 660us/step - loss: 0.0137 - accuracy: 0.9931\n",
      "Epoch 140/200\n",
      "29/29 [==============================] - 0s 679us/step - loss: 0.0127 - accuracy: 0.9931\n",
      "Epoch 141/200\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.0036 - accuracy: 1.00 - 0s 626us/step - loss: 0.0138 - accuracy: 0.9931\n",
      "Epoch 142/200\n",
      "29/29 [==============================] - 0s 617us/step - loss: 0.0126 - accuracy: 0.9931\n",
      "Epoch 143/200\n",
      "29/29 [==============================] - 0s 631us/step - loss: 0.0126 - accuracy: 0.9931\n",
      "Epoch 144/200\n",
      "29/29 [==============================] - 0s 626us/step - loss: 0.0119 - accuracy: 0.9931\n",
      "Epoch 145/200\n",
      "29/29 [==============================] - 0s 622us/step - loss: 0.0125 - accuracy: 0.9931\n",
      "Epoch 146/200\n",
      "29/29 [==============================] - 0s 614us/step - loss: 0.0124 - accuracy: 0.9931\n",
      "Epoch 147/200\n",
      "29/29 [==============================] - 0s 680us/step - loss: 0.0118 - accuracy: 0.9931\n",
      "Epoch 148/200\n",
      "29/29 [==============================] - 0s 661us/step - loss: 0.0108 - accuracy: 0.9931\n",
      "Epoch 149/200\n",
      "29/29 [==============================] - 0s 680us/step - loss: 0.0125 - accuracy: 0.9931\n",
      "Epoch 150/200\n",
      "29/29 [==============================] - 0s 675us/step - loss: 0.0117 - accuracy: 0.9931\n",
      "Epoch 151/200\n",
      "29/29 [==============================] - 0s 635us/step - loss: 0.0108 - accuracy: 0.9931\n",
      "Epoch 152/200\n",
      "29/29 [==============================] - 0s 687us/step - loss: 0.0097 - accuracy: 0.9931\n",
      "Epoch 153/200\n",
      "29/29 [==============================] - 0s 626us/step - loss: 0.0105 - accuracy: 0.9862\n",
      "Epoch 154/200\n",
      "29/29 [==============================] - 0s 676us/step - loss: 0.0146 - accuracy: 0.9931\n",
      "Epoch 155/200\n",
      "29/29 [==============================] - 0s 642us/step - loss: 0.0132 - accuracy: 0.9862\n",
      "Epoch 156/200\n",
      "29/29 [==============================] - 0s 657us/step - loss: 0.0077 - accuracy: 0.9931\n",
      "Epoch 157/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "29/29 [==============================] - 0s 638us/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "29/29 [==============================] - 0s 668us/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "29/29 [==============================] - 0s 678us/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "29/29 [==============================] - 0s 586us/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "29/29 [==============================] - 0s 646us/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "29/29 [==============================] - 0s 639us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "29/29 [==============================] - 0s 654us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "29/29 [==============================] - 0s 673us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "29/29 [==============================] - 0s 639us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "29/29 [==============================] - 0s 680us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "29/29 [==============================] - 0s 654us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "29/29 [==============================] - 0s 663us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "29/29 [==============================] - 0s 659us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 173/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 655us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "29/29 [==============================] - 0s 607us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "29/29 [==============================] - 0s 644us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "29/29 [==============================] - 0s 679us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "29/29 [==============================] - 0s 625us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "29/29 [==============================] - 0s 661us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "29/29 [==============================] - 0s 662us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "29/29 [==============================] - 0s 635us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "29/29 [==============================] - 0s 671us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "29/29 [==============================] - 0s 661us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "29/29 [==============================] - 0s 644us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "29/29 [==============================] - 0s 599us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "29/29 [==============================] - 0s 690us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "29/29 [==============================] - 0s 639us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "29/29 [==============================] - 0s 670us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "29/29 [==============================] - 0s 633us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "29/29 [==============================] - 0s 662us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "29/29 [==============================] - 0s 687us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "29/29 [==============================] - 0s 633us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "29/29 [==============================] - 0s 644us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "29/29 [==============================] - 0s 662us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "29/29 [==============================] - 0s 604us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "29/29 [==============================] - 0s 670us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "29/29 [==============================] - 0s 628us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "29/29 [==============================] - 0s 678us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "29/29 [==============================] - 0s 671us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001BBFB189B88> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.1259 - accuracy: 0.8571\n",
      "Epoch 1/200\n",
      "29/29 [==============================] - 0s 544us/step - loss: 0.2493 - accuracy: 0.5241\n",
      "Epoch 2/200\n",
      "29/29 [==============================] - 0s 600us/step - loss: 0.2405 - accuracy: 0.5586\n",
      "Epoch 3/200\n",
      "29/29 [==============================] - 0s 509us/step - loss: 0.2348 - accuracy: 0.5793\n",
      "Epoch 4/200\n",
      "29/29 [==============================] - 0s 664us/step - loss: 0.2291 - accuracy: 0.6552\n",
      "Epoch 5/200\n",
      "29/29 [==============================] - 0s 617us/step - loss: 0.2255 - accuracy: 0.6483\n",
      "Epoch 6/200\n",
      "29/29 [==============================] - 0s 635us/step - loss: 0.2204 - accuracy: 0.6759\n",
      "Epoch 7/200\n",
      "29/29 [==============================] - 0s 585us/step - loss: 0.2082 - accuracy: 0.7103\n",
      "Epoch 8/200\n",
      "29/29 [==============================] - 0s 601us/step - loss: 0.2020 - accuracy: 0.7448\n",
      "Epoch 9/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 0.1979 - accuracy: 0.7172\n",
      "Epoch 10/200\n",
      "29/29 [==============================] - 0s 618us/step - loss: 0.1872 - accuracy: 0.7655\n",
      "Epoch 11/200\n",
      "29/29 [==============================] - 0s 617us/step - loss: 0.1784 - accuracy: 0.7724\n",
      "Epoch 12/200\n",
      "29/29 [==============================] - 0s 635us/step - loss: 0.1713 - accuracy: 0.8069\n",
      "Epoch 13/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 0.1674 - accuracy: 0.7931\n",
      "Epoch 14/200\n",
      "29/29 [==============================] - 0s 665us/step - loss: 0.1610 - accuracy: 0.7931\n",
      "Epoch 15/200\n",
      "29/29 [==============================] - 0s 672us/step - loss: 0.1515 - accuracy: 0.8276\n",
      "Epoch 16/200\n",
      "29/29 [==============================] - 0s 637us/step - loss: 0.1504 - accuracy: 0.7931\n",
      "Epoch 17/200\n",
      "29/29 [==============================] - 0s 664us/step - loss: 0.1416 - accuracy: 0.8414\n",
      "Epoch 18/200\n",
      "29/29 [==============================] - 0s 633us/step - loss: 0.1373 - accuracy: 0.8414\n",
      "Epoch 19/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.1339 - accuracy: 0.8276\n",
      "Epoch 20/200\n",
      "29/29 [==============================] - 0s 627us/step - loss: 0.1313 - accuracy: 0.7931\n",
      "Epoch 21/200\n",
      "29/29 [==============================] - 0s 656us/step - loss: 0.1292 - accuracy: 0.8345\n",
      "Epoch 22/200\n",
      "29/29 [==============================] - 0s 612us/step - loss: 0.1295 - accuracy: 0.8345\n",
      "Epoch 23/200\n",
      "29/29 [==============================] - 0s 602us/step - loss: 0.1398 - accuracy: 0.8069\n",
      "Epoch 24/200\n",
      "29/29 [==============================] - 0s 671us/step - loss: 0.1219 - accuracy: 0.8345\n",
      "Epoch 25/200\n",
      "29/29 [==============================] - 0s 636us/step - loss: 0.1171 - accuracy: 0.8828\n",
      "Epoch 26/200\n",
      "29/29 [==============================] - 0s 639us/step - loss: 0.1152 - accuracy: 0.8759\n",
      "Epoch 27/200\n",
      "29/29 [==============================] - 0s 670us/step - loss: 0.1115 - accuracy: 0.8759\n",
      "Epoch 28/200\n",
      "29/29 [==============================] - 0s 669us/step - loss: 0.1192 - accuracy: 0.8345\n",
      "Epoch 29/200\n",
      "29/29 [==============================] - 0s 679us/step - loss: 0.1079 - accuracy: 0.8966\n",
      "Epoch 30/200\n",
      "29/29 [==============================] - 0s 632us/step - loss: 0.1112 - accuracy: 0.8759\n",
      "Epoch 31/200\n",
      "29/29 [==============================] - 0s 634us/step - loss: 0.1057 - accuracy: 0.8759\n",
      "Epoch 32/200\n",
      "29/29 [==============================] - 0s 636us/step - loss: 0.1038 - accuracy: 0.8828\n",
      "Epoch 33/200\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.1021 - accuracy: 0.8966\n",
      "Epoch 34/200\n",
      "29/29 [==============================] - 0s 673us/step - loss: 0.1016 - accuracy: 0.8828\n",
      "Epoch 35/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0975 - accuracy: 0.8897\n",
      "Epoch 36/200\n",
      "29/29 [==============================] - 0s 611us/step - loss: 0.0990 - accuracy: 0.9034\n",
      "Epoch 37/200\n",
      "29/29 [==============================] - 0s 705us/step - loss: 0.0980 - accuracy: 0.9034\n",
      "Epoch 38/200\n",
      "29/29 [==============================] - 0s 654us/step - loss: 0.0981 - accuracy: 0.9103\n",
      "Epoch 39/200\n",
      "29/29 [==============================] - 0s 654us/step - loss: 0.0917 - accuracy: 0.8966\n",
      "Epoch 40/200\n",
      "29/29 [==============================] - 0s 672us/step - loss: 0.0894 - accuracy: 0.9034\n",
      "Epoch 41/200\n",
      "29/29 [==============================] - 0s 649us/step - loss: 0.0909 - accuracy: 0.8828\n",
      "Epoch 42/200\n",
      "29/29 [==============================] - 0s 618us/step - loss: 0.0845 - accuracy: 0.9310\n",
      "Epoch 43/200\n",
      "29/29 [==============================] - 0s 671us/step - loss: 0.0841 - accuracy: 0.8966\n",
      "Epoch 44/200\n",
      "29/29 [==============================] - 0s 669us/step - loss: 0.0817 - accuracy: 0.8966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/200\n",
      "29/29 [==============================] - 0s 660us/step - loss: 0.0821 - accuracy: 0.9241\n",
      "Epoch 46/200\n",
      "29/29 [==============================] - 0s 623us/step - loss: 0.0802 - accuracy: 0.9103\n",
      "Epoch 47/200\n",
      "29/29 [==============================] - 0s 550us/step - loss: 0.0769 - accuracy: 0.9172\n",
      "Epoch 48/200\n",
      "29/29 [==============================] - 0s 481us/step - loss: 0.0792 - accuracy: 0.9103\n",
      "Epoch 49/200\n",
      "29/29 [==============================] - 0s 488us/step - loss: 0.0722 - accuracy: 0.9241\n",
      "Epoch 50/200\n",
      "29/29 [==============================] - 0s 498us/step - loss: 0.0723 - accuracy: 0.9241\n",
      "Epoch 51/200\n",
      "29/29 [==============================] - 0s 574us/step - loss: 0.0740 - accuracy: 0.9103\n",
      "Epoch 52/200\n",
      "29/29 [==============================] - 0s 692us/step - loss: 0.0713 - accuracy: 0.9172\n",
      "Epoch 53/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0724 - accuracy: 0.9172\n",
      "Epoch 54/200\n",
      "29/29 [==============================] - 0s 661us/step - loss: 0.0652 - accuracy: 0.9379\n",
      "Epoch 55/200\n",
      "29/29 [==============================] - 0s 615us/step - loss: 0.0617 - accuracy: 0.9517\n",
      "Epoch 56/200\n",
      "29/29 [==============================] - 0s 654us/step - loss: 0.0608 - accuracy: 0.9379\n",
      "Epoch 57/200\n",
      "29/29 [==============================] - 0s 659us/step - loss: 0.0613 - accuracy: 0.9379\n",
      "Epoch 58/200\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.0584 - accuracy: 0.9379\n",
      "Epoch 59/200\n",
      "29/29 [==============================] - 0s 649us/step - loss: 0.0585 - accuracy: 0.9379\n",
      "Epoch 60/200\n",
      "29/29 [==============================] - 0s 620us/step - loss: 0.0554 - accuracy: 0.9379\n",
      "Epoch 61/200\n",
      "29/29 [==============================] - 0s 647us/step - loss: 0.0545 - accuracy: 0.9448\n",
      "Epoch 62/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.0526 - accuracy: 0.9586\n",
      "Epoch 63/200\n",
      "29/29 [==============================] - 0s 618us/step - loss: 0.0519 - accuracy: 0.9379\n",
      "Epoch 64/200\n",
      "29/29 [==============================] - 0s 631us/step - loss: 0.0473 - accuracy: 0.9655\n",
      "Epoch 65/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0527 - accuracy: 0.9241\n",
      "Epoch 66/200\n",
      "29/29 [==============================] - 0s 616us/step - loss: 0.0428 - accuracy: 0.9724\n",
      "Epoch 67/200\n",
      "29/29 [==============================] - 0s 636us/step - loss: 0.0439 - accuracy: 0.9586\n",
      "Epoch 68/200\n",
      "29/29 [==============================] - 0s 671us/step - loss: 0.0418 - accuracy: 0.9724\n",
      "Epoch 69/200\n",
      "29/29 [==============================] - 0s 610us/step - loss: 0.0377 - accuracy: 0.9793\n",
      "Epoch 70/200\n",
      "29/29 [==============================] - 0s 628us/step - loss: 0.0360 - accuracy: 0.9655\n",
      "Epoch 71/200\n",
      "29/29 [==============================] - 0s 687us/step - loss: 0.0373 - accuracy: 0.9724\n",
      "Epoch 72/200\n",
      "29/29 [==============================] - 0s 635us/step - loss: 0.0353 - accuracy: 0.9793\n",
      "Epoch 73/200\n",
      "29/29 [==============================] - 0s 634us/step - loss: 0.0322 - accuracy: 0.9793\n",
      "Epoch 74/200\n",
      "29/29 [==============================] - 0s 635us/step - loss: 0.0317 - accuracy: 0.9724\n",
      "Epoch 75/200\n",
      "29/29 [==============================] - 0s 656us/step - loss: 0.0316 - accuracy: 0.9862\n",
      "Epoch 76/200\n",
      "29/29 [==============================] - 0s 659us/step - loss: 0.0303 - accuracy: 0.9793\n",
      "Epoch 77/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.0352 - accuracy: 0.9793\n",
      "Epoch 78/200\n",
      "29/29 [==============================] - 0s 675us/step - loss: 0.0279 - accuracy: 0.9793\n",
      "Epoch 79/200\n",
      "29/29 [==============================] - 0s 656us/step - loss: 0.0291 - accuracy: 0.9724\n",
      "Epoch 80/200\n",
      "29/29 [==============================] - 0s 647us/step - loss: 0.0269 - accuracy: 0.9931\n",
      "Epoch 81/200\n",
      "29/29 [==============================] - 0s 644us/step - loss: 0.0287 - accuracy: 0.9793\n",
      "Epoch 82/200\n",
      "29/29 [==============================] - 0s 643us/step - loss: 0.0240 - accuracy: 0.9862\n",
      "Epoch 83/200\n",
      "29/29 [==============================] - 0s 669us/step - loss: 0.0263 - accuracy: 0.9862\n",
      "Epoch 84/200\n",
      "29/29 [==============================] - 0s 662us/step - loss: 0.0210 - accuracy: 0.9931\n",
      "Epoch 85/200\n",
      "29/29 [==============================] - 0s 666us/step - loss: 0.0191 - accuracy: 0.9931\n",
      "Epoch 86/200\n",
      "29/29 [==============================] - 0s 635us/step - loss: 0.0191 - accuracy: 0.9931\n",
      "Epoch 87/200\n",
      "29/29 [==============================] - 0s 664us/step - loss: 0.0184 - accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0171 - accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "29/29 [==============================] - 0s 647us/step - loss: 0.0164 - accuracy: 0.9931\n",
      "Epoch 90/200\n",
      "29/29 [==============================] - 0s 673us/step - loss: 0.0175 - accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "29/29 [==============================] - 0s 643us/step - loss: 0.0157 - accuracy: 0.9931\n",
      "Epoch 92/200\n",
      "29/29 [==============================] - 0s 648us/step - loss: 0.0158 - accuracy: 0.9931\n",
      "Epoch 93/200\n",
      "29/29 [==============================] - 0s 644us/step - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "29/29 [==============================] - 0s 656us/step - loss: 0.0153 - accuracy: 0.9931\n",
      "Epoch 95/200\n",
      "29/29 [==============================] - 0s 658us/step - loss: 0.0152 - accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "29/29 [==============================] - 0s 644us/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "29/29 [==============================] - 0s 635us/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "29/29 [==============================] - 0s 661us/step - loss: 0.0158 - accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "29/29 [==============================] - 0s 634us/step - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "29/29 [==============================] - 0s 658us/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "29/29 [==============================] - 0s 672us/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "29/29 [==============================] - 0s 700us/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "29/29 [==============================] - 0s 620us/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "29/29 [==============================] - 0s 633us/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "29/29 [==============================] - 0s 678us/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "29/29 [==============================] - 0s 640us/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "29/29 [==============================] - 0s 650us/step - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "29/29 [==============================] - 0s 620us/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "29/29 [==============================] - 0s 636us/step - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.0118 - accuracy: 0.9931\n",
      "Epoch 114/200\n",
      "29/29 [==============================] - 0s 659us/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "29/29 [==============================] - 0s 643us/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "29/29 [==============================] - 0s 670us/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "29/29 [==============================] - 0s 676us/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "29/29 [==============================] - 0s 625us/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "29/29 [==============================] - 0s 660us/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "29/29 [==============================] - 0s 671us/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "29/29 [==============================] - 0s 692us/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "29/29 [==============================] - 0s 637us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "29/29 [==============================] - 0s 658us/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 125/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 608us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "29/29 [==============================] - 0s 672us/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "29/29 [==============================] - 0s 636us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "29/29 [==============================] - 0s 614us/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "29/29 [==============================] - 0s 633us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "29/29 [==============================] - 0s 686us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "29/29 [==============================] - 0s 710us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "29/29 [==============================] - 0s 669us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "29/29 [==============================] - 0s 675us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "29/29 [==============================] - 0s 752us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "29/29 [==============================] - 0s 701us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "29/29 [==============================] - 0s 624us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "29/29 [==============================] - 0s 689us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "29/29 [==============================] - 0s 638us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "29/29 [==============================] - 0s 637us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "29/29 [==============================] - 0s 637us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "29/29 [==============================] - 0s 637us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "29/29 [==============================] - 0s 670us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "29/29 [==============================] - 0s 631us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "29/29 [==============================] - 0s 656us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "29/29 [==============================] - 0s 657us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "29/29 [==============================] - 0s 623us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "29/29 [==============================] - 0s 657us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "29/29 [==============================] - 0s 647us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "29/29 [==============================] - 0s 702us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "29/29 [==============================] - 0s 666us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "29/29 [==============================] - 0s 690us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "29/29 [==============================] - 0s 636us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "29/29 [==============================] - 0s 610us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "29/29 [==============================] - 0s 692us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "29/29 [==============================] - 0s 676us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "29/29 [==============================] - 0s 649us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "29/29 [==============================] - 0s 654us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "29/29 [==============================] - 0s 611us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "29/29 [==============================] - 0s 634us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "29/29 [==============================] - 0s 659us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "29/29 [==============================] - 0s 723us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "29/29 [==============================] - 0s 642us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "29/29 [==============================] - 0s 660us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "29/29 [==============================] - 0s 670us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "29/29 [==============================] - 0s 694us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "29/29 [==============================] - 0s 644us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "29/29 [==============================] - 0s 602us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "29/29 [==============================] - 0s 650us/step - loss: 8.9757e-04 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "29/29 [==============================] - 0s 637us/step - loss: 8.5509e-04 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "29/29 [==============================] - 0s 647us/step - loss: 8.6546e-04 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "29/29 [==============================] - 0s 639us/step - loss: 8.2519e-04 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "29/29 [==============================] - 0s 657us/step - loss: 8.1797e-04 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "29/29 [==============================] - 0s 650us/step - loss: 8.4175e-04 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "29/29 [==============================] - 0s 692us/step - loss: 7.6665e-04 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "29/29 [==============================] - 0s 718us/step - loss: 7.6102e-04 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "29/29 [==============================] - 0s 672us/step - loss: 7.1487e-04 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 7.6700e-04 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "29/29 [==============================] - 0s 702us/step - loss: 7.8652e-04 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "29/29 [==============================] - 0s 665us/step - loss: 8.1549e-04 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "29/29 [==============================] - 0s 661us/step - loss: 6.7291e-04 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "29/29 [==============================] - 0s 671us/step - loss: 6.2745e-04 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "29/29 [==============================] - 0s 696us/step - loss: 6.6131e-04 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "29/29 [==============================] - 0s 693us/step - loss: 6.2028e-04 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "29/29 [==============================] - 0s 643us/step - loss: 6.4530e-04 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "29/29 [==============================] - 0s 615us/step - loss: 6.0553e-04 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 5.5310e-04 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "29/29 [==============================] - 0s 642us/step - loss: 5.5439e-04 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "29/29 [==============================] - 0s 673us/step - loss: 5.5709e-04 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "29/29 [==============================] - 0s 662us/step - loss: 5.4903e-04 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "29/29 [==============================] - 0s 621us/step - loss: 5.7718e-04 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "29/29 [==============================] - 0s 626us/step - loss: 5.0782e-04 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "29/29 [==============================] - 0s 616us/step - loss: 4.7826e-04 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "29/29 [==============================] - 0s 677us/step - loss: 4.8592e-04 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "29/29 [==============================] - 0s 690us/step - loss: 4.4825e-04 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "29/29 [==============================] - 0s 621us/step - loss: 4.9095e-04 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 4.7960e-04 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "29/29 [==============================] - 0s 676us/step - loss: 4.6204e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001BBF778DB88> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0407 - accuracy: 0.9500\n",
      "Epoch 1/200\n",
      " 1/29 [>.............................] - ETA: 0s - loss: 0.3365 - accuracy: 0.4000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "29/29 [==============================] - 0s 648us/step - loss: 0.2731 - accuracy: 0.4621\n",
      "Epoch 2/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.2449 - accuracy: 0.5310\n",
      "Epoch 3/200\n",
      "29/29 [==============================] - 0s 590us/step - loss: 0.2374 - accuracy: 0.5724\n",
      "Epoch 4/200\n",
      "29/29 [==============================] - 0s 689us/step - loss: 0.2301 - accuracy: 0.6690\n",
      "Epoch 5/200\n",
      "29/29 [==============================] - 0s 694us/step - loss: 0.2250 - accuracy: 0.6759\n",
      "Epoch 6/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 0.2222 - accuracy: 0.6414\n",
      "Epoch 7/200\n",
      "29/29 [==============================] - 0s 642us/step - loss: 0.2113 - accuracy: 0.7172\n",
      "Epoch 8/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.2049 - accuracy: 0.7862\n",
      "Epoch 9/200\n",
      "29/29 [==============================] - 0s 657us/step - loss: 0.2006 - accuracy: 0.7379\n",
      "Epoch 10/200\n",
      "29/29 [==============================] - 0s 660us/step - loss: 0.1922 - accuracy: 0.7724\n",
      "Epoch 11/200\n",
      "29/29 [==============================] - 0s 695us/step - loss: 0.1817 - accuracy: 0.7517\n",
      "Epoch 12/200\n",
      "29/29 [==============================] - 0s 676us/step - loss: 0.1758 - accuracy: 0.8069\n",
      "Epoch 13/200\n",
      "29/29 [==============================] - 0s 622us/step - loss: 0.1720 - accuracy: 0.8000\n",
      "Epoch 14/200\n",
      "29/29 [==============================] - 0s 671us/step - loss: 0.1641 - accuracy: 0.7931\n",
      "Epoch 15/200\n",
      "29/29 [==============================] - 0s 658us/step - loss: 0.1574 - accuracy: 0.8069\n",
      "Epoch 16/200\n",
      "29/29 [==============================] - 0s 685us/step - loss: 0.1562 - accuracy: 0.8138\n",
      "Epoch 17/200\n",
      "29/29 [==============================] - 0s 687us/step - loss: 0.1493 - accuracy: 0.8345\n",
      "Epoch 18/200\n",
      "29/29 [==============================] - 0s 630us/step - loss: 0.1443 - accuracy: 0.8276\n",
      "Epoch 19/200\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.1399 - accuracy: 0.8276\n",
      "Epoch 20/200\n",
      "29/29 [==============================] - 0s 694us/step - loss: 0.1369 - accuracy: 0.8207\n",
      "Epoch 21/200\n",
      "29/29 [==============================] - 0s 650us/step - loss: 0.1332 - accuracy: 0.8138\n",
      "Epoch 22/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.1346 - accuracy: 0.8345\n",
      "Epoch 23/200\n",
      "29/29 [==============================] - 0s 664us/step - loss: 0.1431 - accuracy: 0.8000\n",
      "Epoch 24/200\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.1260 - accuracy: 0.8552\n",
      "Epoch 25/200\n",
      "29/29 [==============================] - 0s 624us/step - loss: 0.1213 - accuracy: 0.8690\n",
      "Epoch 26/200\n",
      "29/29 [==============================] - 0s 650us/step - loss: 0.1191 - accuracy: 0.8828\n",
      "Epoch 27/200\n",
      "29/29 [==============================] - 0s 679us/step - loss: 0.1141 - accuracy: 0.8690\n",
      "Epoch 28/200\n",
      "29/29 [==============================] - 0s 670us/step - loss: 0.1223 - accuracy: 0.8345\n",
      "Epoch 29/200\n",
      "29/29 [==============================] - 0s 648us/step - loss: 0.1098 - accuracy: 0.8897\n",
      "Epoch 30/200\n",
      "29/29 [==============================] - 0s 611us/step - loss: 0.1108 - accuracy: 0.8759\n",
      "Epoch 31/200\n",
      "29/29 [==============================] - 0s 674us/step - loss: 0.1083 - accuracy: 0.8759\n",
      "Epoch 32/200\n",
      "29/29 [==============================] - 0s 668us/step - loss: 0.1052 - accuracy: 0.8828\n",
      "Epoch 33/200\n",
      "29/29 [==============================] - 0s 643us/step - loss: 0.1020 - accuracy: 0.8759\n",
      "Epoch 34/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.1027 - accuracy: 0.8828\n",
      "Epoch 35/200\n",
      "29/29 [==============================] - 0s 634us/step - loss: 0.0967 - accuracy: 0.9034\n",
      "Epoch 36/200\n",
      "29/29 [==============================] - 0s 635us/step - loss: 0.0988 - accuracy: 0.8966\n",
      "Epoch 37/200\n",
      "29/29 [==============================] - 0s 642us/step - loss: 0.0966 - accuracy: 0.9172\n",
      "Epoch 38/200\n",
      "29/29 [==============================] - 0s 648us/step - loss: 0.0947 - accuracy: 0.9034\n",
      "Epoch 39/200\n",
      "29/29 [==============================] - 0s 692us/step - loss: 0.0906 - accuracy: 0.9103\n",
      "Epoch 40/200\n",
      "29/29 [==============================] - 0s 633us/step - loss: 0.0877 - accuracy: 0.9172\n",
      "Epoch 41/200\n",
      "29/29 [==============================] - 0s 648us/step - loss: 0.0891 - accuracy: 0.9103\n",
      "Epoch 42/200\n",
      "29/29 [==============================] - 0s 644us/step - loss: 0.0851 - accuracy: 0.9241\n",
      "Epoch 43/200\n",
      "29/29 [==============================] - 0s 648us/step - loss: 0.0859 - accuracy: 0.9310\n",
      "Epoch 44/200\n",
      "29/29 [==============================] - 0s 621us/step - loss: 0.0807 - accuracy: 0.9172\n",
      "Epoch 45/200\n",
      "29/29 [==============================] - 0s 650us/step - loss: 0.0829 - accuracy: 0.9034\n",
      "Epoch 46/200\n",
      "29/29 [==============================] - 0s 683us/step - loss: 0.0804 - accuracy: 0.9241\n",
      "Epoch 47/200\n",
      "29/29 [==============================] - 0s 632us/step - loss: 0.0757 - accuracy: 0.9310\n",
      "Epoch 48/200\n",
      "29/29 [==============================] - 0s 670us/step - loss: 0.0778 - accuracy: 0.9034\n",
      "Epoch 49/200\n",
      "29/29 [==============================] - 0s 682us/step - loss: 0.0732 - accuracy: 0.9241\n",
      "Epoch 50/200\n",
      "29/29 [==============================] - 0s 666us/step - loss: 0.0742 - accuracy: 0.9310\n",
      "Epoch 51/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0757 - accuracy: 0.9103\n",
      "Epoch 52/200\n",
      "29/29 [==============================] - 0s 620us/step - loss: 0.0720 - accuracy: 0.9310\n",
      "Epoch 53/200\n",
      "29/29 [==============================] - 0s 735us/step - loss: 0.0748 - accuracy: 0.9310\n",
      "Epoch 54/200\n",
      "29/29 [==============================] - 0s 606us/step - loss: 0.0699 - accuracy: 0.9241\n",
      "Epoch 55/200\n",
      "29/29 [==============================] - 0s 617us/step - loss: 0.0646 - accuracy: 0.9586\n",
      "Epoch 56/200\n",
      "29/29 [==============================] - 0s 665us/step - loss: 0.0647 - accuracy: 0.9379\n",
      "Epoch 57/200\n",
      "29/29 [==============================] - 0s 687us/step - loss: 0.0638 - accuracy: 0.9379\n",
      "Epoch 58/200\n",
      "29/29 [==============================] - 0s 683us/step - loss: 0.0603 - accuracy: 0.9448\n",
      "Epoch 59/200\n",
      "29/29 [==============================] - 0s 681us/step - loss: 0.0607 - accuracy: 0.9517\n",
      "Epoch 60/200\n",
      "29/29 [==============================] - 0s 675us/step - loss: 0.0624 - accuracy: 0.9310\n",
      "Epoch 61/200\n",
      "29/29 [==============================] - 0s 723us/step - loss: 0.0636 - accuracy: 0.9103\n",
      "Epoch 62/200\n",
      "29/29 [==============================] - 0s 697us/step - loss: 0.0619 - accuracy: 0.9517\n",
      "Epoch 63/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 0.0608 - accuracy: 0.9379\n",
      "Epoch 64/200\n",
      "29/29 [==============================] - 0s 679us/step - loss: 0.0570 - accuracy: 0.9448\n",
      "Epoch 65/200\n",
      "29/29 [==============================] - 0s 676us/step - loss: 0.0595 - accuracy: 0.9379\n",
      "Epoch 66/200\n",
      "29/29 [==============================] - 0s 671us/step - loss: 0.0534 - accuracy: 0.9517\n",
      "Epoch 67/200\n",
      "29/29 [==============================] - 0s 665us/step - loss: 0.0518 - accuracy: 0.9586\n",
      "Epoch 68/200\n",
      "29/29 [==============================] - 0s 674us/step - loss: 0.0535 - accuracy: 0.9517\n",
      "Epoch 69/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 0.0506 - accuracy: 0.9586\n",
      "Epoch 70/200\n",
      "29/29 [==============================] - 0s 697us/step - loss: 0.0482 - accuracy: 0.9655\n",
      "Epoch 71/200\n",
      "29/29 [==============================] - 0s 640us/step - loss: 0.0501 - accuracy: 0.9379\n",
      "Epoch 72/200\n",
      "29/29 [==============================] - 0s 673us/step - loss: 0.0487 - accuracy: 0.9517\n",
      "Epoch 73/200\n",
      "29/29 [==============================] - 0s 678us/step - loss: 0.0440 - accuracy: 0.9586\n",
      "Epoch 74/200\n",
      "29/29 [==============================] - 0s 721us/step - loss: 0.0486 - accuracy: 0.9379\n",
      "Epoch 75/200\n",
      "29/29 [==============================] - 0s 679us/step - loss: 0.0466 - accuracy: 0.9724\n",
      "Epoch 76/200\n",
      "29/29 [==============================] - 0s 672us/step - loss: 0.0450 - accuracy: 0.9517\n",
      "Epoch 77/200\n",
      "29/29 [==============================] - 0s 678us/step - loss: 0.0531 - accuracy: 0.9241\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 707us/step - loss: 0.0443 - accuracy: 0.9586\n",
      "Epoch 79/200\n",
      "29/29 [==============================] - 0s 689us/step - loss: 0.0463 - accuracy: 0.9586\n",
      "Epoch 80/200\n",
      "29/29 [==============================] - 0s 640us/step - loss: 0.0447 - accuracy: 0.9724\n",
      "Epoch 81/200\n",
      "29/29 [==============================] - 0s 650us/step - loss: 0.0433 - accuracy: 0.9586\n",
      "Epoch 82/200\n",
      "29/29 [==============================] - 0s 488us/step - loss: 0.0401 - accuracy: 0.9793\n",
      "Epoch 83/200\n",
      "29/29 [==============================] - 0s 952us/step - loss: 0.0479 - accuracy: 0.9586\n",
      "Epoch 84/200\n",
      "29/29 [==============================] - 0s 684us/step - loss: 0.0371 - accuracy: 0.9724\n",
      "Epoch 85/200\n",
      "29/29 [==============================] - 0s 620us/step - loss: 0.0353 - accuracy: 0.9724\n",
      "Epoch 86/200\n",
      "29/29 [==============================] - 0s 687us/step - loss: 0.0371 - accuracy: 0.9793\n",
      "Epoch 87/200\n",
      "29/29 [==============================] - 0s 670us/step - loss: 0.0346 - accuracy: 0.9793\n",
      "Epoch 88/200\n",
      "29/29 [==============================] - 0s 635us/step - loss: 0.0320 - accuracy: 0.9793\n",
      "Epoch 89/200\n",
      "29/29 [==============================] - 0s 649us/step - loss: 0.0321 - accuracy: 0.9655\n",
      "Epoch 90/200\n",
      "29/29 [==============================] - 0s 649us/step - loss: 0.0314 - accuracy: 0.9793\n",
      "Epoch 91/200\n",
      "29/29 [==============================] - 0s 709us/step - loss: 0.0297 - accuracy: 0.9793\n",
      "Epoch 92/200\n",
      "29/29 [==============================] - 0s 662us/step - loss: 0.0308 - accuracy: 0.9862\n",
      "Epoch 93/200\n",
      "29/29 [==============================] - 0s 665us/step - loss: 0.0267 - accuracy: 0.9862\n",
      "Epoch 94/200\n",
      "29/29 [==============================] - 0s 704us/step - loss: 0.0298 - accuracy: 0.9655\n",
      "Epoch 95/200\n",
      "29/29 [==============================] - 0s 658us/step - loss: 0.0302 - accuracy: 0.9862\n",
      "Epoch 96/200\n",
      "29/29 [==============================] - 0s 654us/step - loss: 0.0300 - accuracy: 0.9862\n",
      "Epoch 97/200\n",
      "29/29 [==============================] - 0s 665us/step - loss: 0.0255 - accuracy: 0.9862\n",
      "Epoch 98/200\n",
      "29/29 [==============================] - 0s 664us/step - loss: 0.0252 - accuracy: 0.9862\n",
      "Epoch 99/200\n",
      "29/29 [==============================] - 0s 700us/step - loss: 0.0249 - accuracy: 0.9862\n",
      "Epoch 100/200\n",
      "29/29 [==============================] - 0s 669us/step - loss: 0.0285 - accuracy: 0.9793\n",
      "Epoch 101/200\n",
      "29/29 [==============================] - 0s 630us/step - loss: 0.0226 - accuracy: 0.9862\n",
      "Epoch 102/200\n",
      "29/29 [==============================] - 0s 647us/step - loss: 0.0220 - accuracy: 0.9862\n",
      "Epoch 103/200\n",
      "29/29 [==============================] - 0s 696us/step - loss: 0.0222 - accuracy: 0.9862\n",
      "Epoch 104/200\n",
      "29/29 [==============================] - 0s 715us/step - loss: 0.0226 - accuracy: 0.9862\n",
      "Epoch 105/200\n",
      "29/29 [==============================] - 0s 665us/step - loss: 0.0209 - accuracy: 0.9862\n",
      "Epoch 106/200\n",
      "29/29 [==============================] - 0s 681us/step - loss: 0.0211 - accuracy: 0.9862\n",
      "Epoch 107/200\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.0215 - accuracy: 0.9862\n",
      "Epoch 108/200\n",
      "29/29 [==============================] - 0s 664us/step - loss: 0.0183 - accuracy: 0.9862\n",
      "Epoch 109/200\n",
      "29/29 [==============================] - 0s 656us/step - loss: 0.0183 - accuracy: 0.9862\n",
      "Epoch 110/200\n",
      "29/29 [==============================] - 0s 643us/step - loss: 0.0194 - accuracy: 0.9931\n",
      "Epoch 111/200\n",
      "29/29 [==============================] - 0s 686us/step - loss: 0.0214 - accuracy: 0.9931\n",
      "Epoch 112/200\n",
      "29/29 [==============================] - 0s 699us/step - loss: 0.0220 - accuracy: 0.9724\n",
      "Epoch 113/200\n",
      "29/29 [==============================] - 0s 675us/step - loss: 0.0312 - accuracy: 0.9724\n",
      "Epoch 114/200\n",
      "29/29 [==============================] - 0s 672us/step - loss: 0.0250 - accuracy: 0.9793\n",
      "Epoch 115/200\n",
      "29/29 [==============================] - 0s 663us/step - loss: 0.0173 - accuracy: 0.9862\n",
      "Epoch 116/200\n",
      "29/29 [==============================] - 0s 645us/step - loss: 0.0164 - accuracy: 0.9862\n",
      "Epoch 117/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0137 - accuracy: 0.9862\n",
      "Epoch 118/200\n",
      "29/29 [==============================] - 0s 666us/step - loss: 0.0181 - accuracy: 0.9862\n",
      "Epoch 119/200\n",
      "29/29 [==============================] - 0s 664us/step - loss: 0.0153 - accuracy: 0.9862\n",
      "Epoch 120/200\n",
      "29/29 [==============================] - 0s 678us/step - loss: 0.0147 - accuracy: 0.9862\n",
      "Epoch 121/200\n",
      "29/29 [==============================] - 0s 707us/step - loss: 0.0195 - accuracy: 0.9862\n",
      "Epoch 122/200\n",
      "29/29 [==============================] - 0s 680us/step - loss: 0.0143 - accuracy: 0.9931\n",
      "Epoch 123/200\n",
      "29/29 [==============================] - 0s 727us/step - loss: 0.0121 - accuracy: 0.9931\n",
      "Epoch 124/200\n",
      "29/29 [==============================] - 0s 650us/step - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "29/29 [==============================] - 0s 646us/step - loss: 0.0201 - accuracy: 0.9931\n",
      "Epoch 126/200\n",
      "29/29 [==============================] - 0s 665us/step - loss: 0.0151 - accuracy: 0.9862\n",
      "Epoch 127/200\n",
      "29/29 [==============================] - 0s 683us/step - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.0044 - accuracy: 1.00 - 0s 710us/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "29/29 [==============================] - 0s 665us/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "29/29 [==============================] - 0s 638us/step - loss: 0.0103 - accuracy: 0.9931\n",
      "Epoch 132/200\n",
      "29/29 [==============================] - 0s 714us/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "29/29 [==============================] - 0s 648us/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "29/29 [==============================] - 0s 661us/step - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "29/29 [==============================] - 0s 658us/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "29/29 [==============================] - 0s 710us/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "29/29 [==============================] - 0s 685us/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "29/29 [==============================] - 0s 690us/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "29/29 [==============================] - 0s 694us/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "29/29 [==============================] - 0s 687us/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "29/29 [==============================] - 0s 671us/step - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "29/29 [==============================] - 0s 648us/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "29/29 [==============================] - 0s 690us/step - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "29/29 [==============================] - 0s 654us/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "29/29 [==============================] - 0s 671us/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "29/29 [==============================] - 0s 646us/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "29/29 [==============================] - 0s 707us/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "29/29 [==============================] - 0s 689us/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "29/29 [==============================] - 0s 696us/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "29/29 [==============================] - 0s 701us/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "29/29 [==============================] - 0s 691us/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "29/29 [==============================] - 0s 642us/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "29/29 [==============================] - 0s 663us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "29/29 [==============================] - 0s 680us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "29/29 [==============================] - 0s 669us/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "29/29 [==============================] - 0s 697us/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "29/29 [==============================] - 0s 667us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "29/29 [==============================] - 0s 658us/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "29/29 [==============================] - 0s 665us/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "29/29 [==============================] - 0s 632us/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "29/29 [==============================] - 0s 601us/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "29/29 [==============================] - 0s 663us/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "29/29 [==============================] - 0s 646us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "29/29 [==============================] - 0s 646us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "29/29 [==============================] - 0s 664us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "29/29 [==============================] - 0s 684us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "29/29 [==============================] - 0s 720us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "29/29 [==============================] - 0s 672us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "29/29 [==============================] - 0s 670us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "29/29 [==============================] - 0s 666us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "29/29 [==============================] - 0s 657us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "29/29 [==============================] - 0s 692us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "29/29 [==============================] - 0s 640us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "29/29 [==============================] - 0s 661us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "29/29 [==============================] - 0s 632us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "29/29 [==============================] - 0s 657us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "29/29 [==============================] - 0s 627us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "29/29 [==============================] - 0s 631us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "29/29 [==============================] - 0s 694us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "29/29 [==============================] - 0s 633us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "29/29 [==============================] - 0s 705us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "29/29 [==============================] - 0s 668us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "29/29 [==============================] - 0s 698us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "29/29 [==============================] - 0s 663us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "29/29 [==============================] - 0s 667us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "29/29 [==============================] - 0s 654us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "29/29 [==============================] - 0s 658us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "29/29 [==============================] - 0s 689us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "29/29 [==============================] - 0s 666us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "29/29 [==============================] - 0s 636us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "29/29 [==============================] - 0s 696us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "29/29 [==============================] - 0s 665us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "29/29 [==============================] - 0s 650us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001BBF7587168> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 464us/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 1/200\n",
      " 1/29 [>.............................] - ETA: 0s - loss: 0.2365 - accuracy: 0.4000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "29/29 [==============================] - 0s 594us/step - loss: 0.2526 - accuracy: 0.5172\n",
      "Epoch 2/200\n",
      "29/29 [==============================] - 0s 555us/step - loss: 0.2394 - accuracy: 0.5862\n",
      "Epoch 3/200\n",
      "29/29 [==============================] - 0s 520us/step - loss: 0.2303 - accuracy: 0.6621\n",
      "Epoch 4/200\n",
      "29/29 [==============================] - 0s 635us/step - loss: 0.2230 - accuracy: 0.6621\n",
      "Epoch 5/200\n",
      "29/29 [==============================] - 0s 610us/step - loss: 0.2189 - accuracy: 0.6621\n",
      "Epoch 6/200\n",
      "29/29 [==============================] - 0s 578us/step - loss: 0.2154 - accuracy: 0.6828\n",
      "Epoch 7/200\n",
      "29/29 [==============================] - 0s 660us/step - loss: 0.2027 - accuracy: 0.7103\n",
      "Epoch 8/200\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.1963 - accuracy: 0.7517\n",
      "Epoch 9/200\n",
      "29/29 [==============================] - 0s 671us/step - loss: 0.1932 - accuracy: 0.6966\n",
      "Epoch 10/200\n",
      "29/29 [==============================] - 0s 626us/step - loss: 0.1844 - accuracy: 0.7586\n",
      "Epoch 11/200\n",
      "29/29 [==============================] - 0s 694us/step - loss: 0.1747 - accuracy: 0.7724\n",
      "Epoch 12/200\n",
      "29/29 [==============================] - 0s 692us/step - loss: 0.1718 - accuracy: 0.8138\n",
      "Epoch 13/200\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.1672 - accuracy: 0.7793\n",
      "Epoch 14/200\n",
      "29/29 [==============================] - 0s 645us/step - loss: 0.1626 - accuracy: 0.8000\n",
      "Epoch 15/200\n",
      "29/29 [==============================] - 0s 675us/step - loss: 0.1556 - accuracy: 0.8000\n",
      "Epoch 16/200\n",
      "29/29 [==============================] - 0s 690us/step - loss: 0.1559 - accuracy: 0.7931\n",
      "Epoch 17/200\n",
      "29/29 [==============================] - 0s 592us/step - loss: 0.1491 - accuracy: 0.7793\n",
      "Epoch 18/200\n",
      "29/29 [==============================] - 0s 705us/step - loss: 0.1483 - accuracy: 0.8000\n",
      "Epoch 19/200\n",
      "29/29 [==============================] - 0s 639us/step - loss: 0.1434 - accuracy: 0.8069\n",
      "Epoch 20/200\n",
      "29/29 [==============================] - 0s 673us/step - loss: 0.1421 - accuracy: 0.8000\n",
      "Epoch 21/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.1393 - accuracy: 0.8276\n",
      "Epoch 22/200\n",
      "29/29 [==============================] - 0s 656us/step - loss: 0.1410 - accuracy: 0.7931\n",
      "Epoch 23/200\n",
      "29/29 [==============================] - 0s 602us/step - loss: 0.1438 - accuracy: 0.8345\n",
      "Epoch 24/200\n",
      "29/29 [==============================] - 0s 618us/step - loss: 0.1318 - accuracy: 0.8483\n",
      "Epoch 25/200\n",
      "29/29 [==============================] - 0s 710us/step - loss: 0.1298 - accuracy: 0.8414\n",
      "Epoch 26/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 666us/step - loss: 0.1272 - accuracy: 0.8483\n",
      "Epoch 27/200\n",
      "29/29 [==============================] - 0s 632us/step - loss: 0.1243 - accuracy: 0.8483\n",
      "Epoch 28/200\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.1294 - accuracy: 0.8345\n",
      "Epoch 29/200\n",
      "29/29 [==============================] - 0s 637us/step - loss: 0.1204 - accuracy: 0.8552\n",
      "Epoch 30/200\n",
      "29/29 [==============================] - 0s 639us/step - loss: 0.1223 - accuracy: 0.8414\n",
      "Epoch 31/200\n",
      "29/29 [==============================] - 0s 643us/step - loss: 0.1185 - accuracy: 0.8483\n",
      "Epoch 32/200\n",
      "29/29 [==============================] - 0s 647us/step - loss: 0.1160 - accuracy: 0.8552\n",
      "Epoch 33/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.1134 - accuracy: 0.8690\n",
      "Epoch 34/200\n",
      "29/29 [==============================] - 0s 629us/step - loss: 0.1153 - accuracy: 0.8483\n",
      "Epoch 35/200\n",
      "29/29 [==============================] - 0s 660us/step - loss: 0.1093 - accuracy: 0.8828\n",
      "Epoch 36/200\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.1122 - accuracy: 0.8552\n",
      "Epoch 37/200\n",
      "29/29 [==============================] - 0s 683us/step - loss: 0.1100 - accuracy: 0.8828\n",
      "Epoch 38/200\n",
      "29/29 [==============================] - 0s 634us/step - loss: 0.1093 - accuracy: 0.8828\n",
      "Epoch 39/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 0.1049 - accuracy: 0.8759\n",
      "Epoch 40/200\n",
      "29/29 [==============================] - 0s 672us/step - loss: 0.1029 - accuracy: 0.8897\n",
      "Epoch 41/200\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.1035 - accuracy: 0.8759\n",
      "Epoch 42/200\n",
      "29/29 [==============================] - 0s 656us/step - loss: 0.1011 - accuracy: 0.8828\n",
      "Epoch 43/200\n",
      "29/29 [==============================] - 0s 702us/step - loss: 0.0995 - accuracy: 0.8966\n",
      "Epoch 44/200\n",
      "29/29 [==============================] - 0s 665us/step - loss: 0.0957 - accuracy: 0.8828\n",
      "Epoch 45/200\n",
      "29/29 [==============================] - 0s 657us/step - loss: 0.0991 - accuracy: 0.8897\n",
      "Epoch 46/200\n",
      "29/29 [==============================] - 0s 631us/step - loss: 0.0941 - accuracy: 0.9034\n",
      "Epoch 47/200\n",
      "29/29 [==============================] - 0s 638us/step - loss: 0.0920 - accuracy: 0.9241\n",
      "Epoch 48/200\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.0950 - accuracy: 0.8621\n",
      "Epoch 49/200\n",
      "29/29 [==============================] - 0s 675us/step - loss: 0.0889 - accuracy: 0.9172\n",
      "Epoch 50/200\n",
      "29/29 [==============================] - 0s 670us/step - loss: 0.0908 - accuracy: 0.9172\n",
      "Epoch 51/200\n",
      "29/29 [==============================] - 0s 631us/step - loss: 0.0938 - accuracy: 0.9034\n",
      "Epoch 52/200\n",
      "29/29 [==============================] - 0s 680us/step - loss: 0.0896 - accuracy: 0.9172\n",
      "Epoch 53/200\n",
      "29/29 [==============================] - 0s 684us/step - loss: 0.0880 - accuracy: 0.9103\n",
      "Epoch 54/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.0892 - accuracy: 0.8828\n",
      "Epoch 55/200\n",
      "29/29 [==============================] - 0s 661us/step - loss: 0.0833 - accuracy: 0.9103\n",
      "Epoch 56/200\n",
      "29/29 [==============================] - 0s 664us/step - loss: 0.0841 - accuracy: 0.9310\n",
      "Epoch 57/200\n",
      "29/29 [==============================] - 0s 661us/step - loss: 0.0832 - accuracy: 0.9103\n",
      "Epoch 58/200\n",
      "29/29 [==============================] - 0s 722us/step - loss: 0.0796 - accuracy: 0.9379\n",
      "Epoch 59/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 0.0804 - accuracy: 0.9172\n",
      "Epoch 60/200\n",
      "29/29 [==============================] - 0s 626us/step - loss: 0.0789 - accuracy: 0.9172\n",
      "Epoch 61/200\n",
      "29/29 [==============================] - 0s 633us/step - loss: 0.0859 - accuracy: 0.8966\n",
      "Epoch 62/200\n",
      "29/29 [==============================] - 0s 661us/step - loss: 0.0844 - accuracy: 0.8966\n",
      "Epoch 63/200\n",
      "29/29 [==============================] - 0s 613us/step - loss: 0.0755 - accuracy: 0.9241\n",
      "Epoch 64/200\n",
      "29/29 [==============================] - 0s 667us/step - loss: 0.0747 - accuracy: 0.9103\n",
      "Epoch 65/200\n",
      "29/29 [==============================] - 0s 614us/step - loss: 0.0776 - accuracy: 0.9241\n",
      "Epoch 66/200\n",
      "29/29 [==============================] - 0s 654us/step - loss: 0.0752 - accuracy: 0.9241\n",
      "Epoch 67/200\n",
      "29/29 [==============================] - 0s 661us/step - loss: 0.0722 - accuracy: 0.9310\n",
      "Epoch 68/200\n",
      "29/29 [==============================] - 0s 654us/step - loss: 0.0714 - accuracy: 0.9448\n",
      "Epoch 69/200\n",
      "29/29 [==============================] - 0s 646us/step - loss: 0.0704 - accuracy: 0.9310\n",
      "Epoch 70/200\n",
      "29/29 [==============================] - 0s 624us/step - loss: 0.0690 - accuracy: 0.9517\n",
      "Epoch 71/200\n",
      "29/29 [==============================] - 0s 689us/step - loss: 0.0711 - accuracy: 0.9379\n",
      "Epoch 72/200\n",
      "29/29 [==============================] - 0s 696us/step - loss: 0.0713 - accuracy: 0.9241\n",
      "Epoch 73/200\n",
      "29/29 [==============================] - 0s 663us/step - loss: 0.0655 - accuracy: 0.9517\n",
      "Epoch 74/200\n",
      "29/29 [==============================] - 0s 665us/step - loss: 0.0715 - accuracy: 0.9172\n",
      "Epoch 75/200\n",
      "29/29 [==============================] - 0s 694us/step - loss: 0.0670 - accuracy: 0.9310\n",
      "Epoch 76/200\n",
      "29/29 [==============================] - 0s 626us/step - loss: 0.0684 - accuracy: 0.9448\n",
      "Epoch 77/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0701 - accuracy: 0.9172\n",
      "Epoch 78/200\n",
      "29/29 [==============================] - 0s 655us/step - loss: 0.0651 - accuracy: 0.9310\n",
      "Epoch 79/200\n",
      "29/29 [==============================] - 0s 667us/step - loss: 0.0640 - accuracy: 0.9241\n",
      "Epoch 80/200\n",
      "29/29 [==============================] - 0s 686us/step - loss: 0.0676 - accuracy: 0.9310\n",
      "Epoch 81/200\n",
      "29/29 [==============================] - 0s 700us/step - loss: 0.0614 - accuracy: 0.9379\n",
      "Epoch 82/200\n",
      "29/29 [==============================] - 0s 633us/step - loss: 0.0607 - accuracy: 0.9379\n",
      "Epoch 83/200\n",
      "29/29 [==============================] - 0s 697us/step - loss: 0.0597 - accuracy: 0.9379\n",
      "Epoch 84/200\n",
      "29/29 [==============================] - 0s 635us/step - loss: 0.0604 - accuracy: 0.9241\n",
      "Epoch 85/200\n",
      "29/29 [==============================] - 0s 755us/step - loss: 0.0584 - accuracy: 0.9517\n",
      "Epoch 86/200\n",
      "29/29 [==============================] - 0s 621us/step - loss: 0.0560 - accuracy: 0.9310\n",
      "Epoch 87/200\n",
      "29/29 [==============================] - 0s 662us/step - loss: 0.0589 - accuracy: 0.9448\n",
      "Epoch 88/200\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.0542 - accuracy: 0.9517\n",
      "Epoch 89/200\n",
      "29/29 [==============================] - 0s 649us/step - loss: 0.0542 - accuracy: 0.9517\n",
      "Epoch 90/200\n",
      "29/29 [==============================] - 0s 654us/step - loss: 0.0546 - accuracy: 0.9448\n",
      "Epoch 91/200\n",
      "29/29 [==============================] - 0s 611us/step - loss: 0.0522 - accuracy: 0.9517\n",
      "Epoch 92/200\n",
      "29/29 [==============================] - 0s 618us/step - loss: 0.0518 - accuracy: 0.9517\n",
      "Epoch 93/200\n",
      "29/29 [==============================] - 0s 622us/step - loss: 0.0503 - accuracy: 0.9448\n",
      "Epoch 94/200\n",
      "29/29 [==============================] - 0s 694us/step - loss: 0.0549 - accuracy: 0.9379\n",
      "Epoch 95/200\n",
      "29/29 [==============================] - 0s 634us/step - loss: 0.0483 - accuracy: 0.9586\n",
      "Epoch 96/200\n",
      "29/29 [==============================] - 0s 675us/step - loss: 0.0634 - accuracy: 0.9448\n",
      "Epoch 97/200\n",
      "29/29 [==============================] - 0s 625us/step - loss: 0.0476 - accuracy: 0.9586\n",
      "Epoch 98/200\n",
      "29/29 [==============================] - 0s 649us/step - loss: 0.0543 - accuracy: 0.9379\n",
      "Epoch 99/200\n",
      "29/29 [==============================] - 0s 647us/step - loss: 0.0467 - accuracy: 0.9517\n",
      "Epoch 100/200\n",
      "29/29 [==============================] - 0s 668us/step - loss: 0.0523 - accuracy: 0.9517\n",
      "Epoch 101/200\n",
      "29/29 [==============================] - 0s 668us/step - loss: 0.0448 - accuracy: 0.9655\n",
      "Epoch 102/200\n",
      "29/29 [==============================] - 0s 676us/step - loss: 0.0436 - accuracy: 0.9655\n",
      "Epoch 103/200\n",
      "29/29 [==============================] - 0s 659us/step - loss: 0.0474 - accuracy: 0.9655\n",
      "Epoch 104/200\n",
      "29/29 [==============================] - 0s 664us/step - loss: 0.0440 - accuracy: 0.9655\n",
      "Epoch 105/200\n",
      "29/29 [==============================] - 0s 634us/step - loss: 0.0423 - accuracy: 0.9724\n",
      "Epoch 106/200\n",
      "29/29 [==============================] - 0s 703us/step - loss: 0.0469 - accuracy: 0.9586\n",
      "Epoch 107/200\n",
      "29/29 [==============================] - 0s 664us/step - loss: 0.0437 - accuracy: 0.9655\n",
      "Epoch 108/200\n",
      "29/29 [==============================] - 0s 635us/step - loss: 0.0419 - accuracy: 0.9655\n",
      "Epoch 109/200\n",
      "29/29 [==============================] - 0s 690us/step - loss: 0.0419 - accuracy: 0.9586\n",
      "Epoch 110/200\n",
      "29/29 [==============================] - 0s 747us/step - loss: 0.0477 - accuracy: 0.9517\n",
      "Epoch 111/200\n",
      "29/29 [==============================] - 0s 621us/step - loss: 0.0401 - accuracy: 0.9724\n",
      "Epoch 112/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0380 - accuracy: 0.9655\n",
      "Epoch 113/200\n",
      "29/29 [==============================] - 0s 653us/step - loss: 0.0500 - accuracy: 0.9379\n",
      "Epoch 114/200\n",
      "29/29 [==============================] - 0s 691us/step - loss: 0.0420 - accuracy: 0.9655\n",
      "Epoch 115/200\n",
      "29/29 [==============================] - 0s 757us/step - loss: 0.0378 - accuracy: 0.9793\n",
      "Epoch 116/200\n",
      "29/29 [==============================] - 0s 757us/step - loss: 0.0381 - accuracy: 0.9724\n",
      "Epoch 117/200\n",
      "29/29 [==============================] - 0s 754us/step - loss: 0.0376 - accuracy: 0.9724\n",
      "Epoch 118/200\n",
      "29/29 [==============================] - 0s 756us/step - loss: 0.0384 - accuracy: 0.9724\n",
      "Epoch 119/200\n",
      "29/29 [==============================] - 0s 826us/step - loss: 0.0369 - accuracy: 0.9793\n",
      "Epoch 120/200\n",
      "29/29 [==============================] - 0s 759us/step - loss: 0.0364 - accuracy: 0.9724\n",
      "Epoch 121/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0393 - accuracy: 0.9655\n",
      "Epoch 122/200\n",
      "29/29 [==============================] - 0s 686us/step - loss: 0.0359 - accuracy: 0.9793\n",
      "Epoch 123/200\n",
      "29/29 [==============================] - 0s 688us/step - loss: 0.0330 - accuracy: 0.9793\n",
      "Epoch 124/200\n",
      "29/29 [==============================] - 0s 723us/step - loss: 0.0361 - accuracy: 0.9724\n",
      "Epoch 125/200\n",
      "29/29 [==============================] - 0s 619us/step - loss: 0.0394 - accuracy: 0.9655\n",
      "Epoch 126/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.0460 - accuracy: 0.9586\n",
      "Epoch 127/200\n",
      "29/29 [==============================] - 0s 584us/step - loss: 0.0351 - accuracy: 0.9793\n",
      "Epoch 128/200\n",
      "29/29 [==============================] - 0s 645us/step - loss: 0.0316 - accuracy: 0.9793\n",
      "Epoch 129/200\n",
      "29/29 [==============================] - 0s 757us/step - loss: 0.0323 - accuracy: 0.9793\n",
      "Epoch 130/200\n",
      "29/29 [==============================] - 0s 553us/step - loss: 0.0325 - accuracy: 0.9793\n",
      "Epoch 131/200\n",
      "29/29 [==============================] - 0s 550us/step - loss: 0.0318 - accuracy: 0.9724\n",
      "Epoch 132/200\n",
      "29/29 [==============================] - 0s 551us/step - loss: 0.0296 - accuracy: 0.9793\n",
      "Epoch 133/200\n",
      "29/29 [==============================] - 0s 518us/step - loss: 0.0301 - accuracy: 0.9793\n",
      "Epoch 134/200\n",
      "29/29 [==============================] - 0s 542us/step - loss: 0.0288 - accuracy: 0.9793\n",
      "Epoch 135/200\n",
      "29/29 [==============================] - 0s 549us/step - loss: 0.0299 - accuracy: 0.9793\n",
      "Epoch 136/200\n",
      "29/29 [==============================] - 0s 575us/step - loss: 0.0280 - accuracy: 0.9793\n",
      "Epoch 137/200\n",
      "29/29 [==============================] - 0s 643us/step - loss: 0.0274 - accuracy: 0.9793\n",
      "Epoch 138/200\n",
      "29/29 [==============================] - 0s 628us/step - loss: 0.0308 - accuracy: 0.9793\n",
      "Epoch 139/200\n",
      "29/29 [==============================] - 0s 649us/step - loss: 0.0262 - accuracy: 0.9862\n",
      "Epoch 140/200\n",
      "29/29 [==============================] - 0s 626us/step - loss: 0.0283 - accuracy: 0.9793\n",
      "Epoch 141/200\n",
      "29/29 [==============================] - 0s 691us/step - loss: 0.0265 - accuracy: 0.9862\n",
      "Epoch 142/200\n",
      "29/29 [==============================] - 0s 683us/step - loss: 0.0257 - accuracy: 0.9862\n",
      "Epoch 143/200\n",
      "29/29 [==============================] - 0s 668us/step - loss: 0.0265 - accuracy: 0.9862\n",
      "Epoch 144/200\n",
      "29/29 [==============================] - 0s 638us/step - loss: 0.0304 - accuracy: 0.9862\n",
      "Epoch 145/200\n",
      "29/29 [==============================] - 0s 626us/step - loss: 0.0265 - accuracy: 0.9862\n",
      "Epoch 146/200\n",
      "29/29 [==============================] - 0s 686us/step - loss: 0.0253 - accuracy: 0.9862\n",
      "Epoch 147/200\n",
      "29/29 [==============================] - 0s 679us/step - loss: 0.0240 - accuracy: 0.9862\n",
      "Epoch 148/200\n",
      "29/29 [==============================] - 0s 650us/step - loss: 0.0284 - accuracy: 0.9862\n",
      "Epoch 149/200\n",
      "29/29 [==============================] - 0s 632us/step - loss: 0.0244 - accuracy: 0.9862\n",
      "Epoch 150/200\n",
      "29/29 [==============================] - 0s 615us/step - loss: 0.0261 - accuracy: 0.9862\n",
      "Epoch 151/200\n",
      "29/29 [==============================] - 0s 647us/step - loss: 0.0250 - accuracy: 0.9862\n",
      "Epoch 152/200\n",
      "29/29 [==============================] - 0s 659us/step - loss: 0.0238 - accuracy: 0.9862\n",
      "Epoch 153/200\n",
      "29/29 [==============================] - 0s 598us/step - loss: 0.0234 - accuracy: 0.9862\n",
      "Epoch 154/200\n",
      "29/29 [==============================] - 0s 659us/step - loss: 0.0222 - accuracy: 0.9862\n",
      "Epoch 155/200\n",
      "29/29 [==============================] - 0s 672us/step - loss: 0.0245 - accuracy: 0.9862\n",
      "Epoch 156/200\n",
      "29/29 [==============================] - 0s 680us/step - loss: 0.0234 - accuracy: 0.9793\n",
      "Epoch 157/200\n",
      "29/29 [==============================] - 0s 620us/step - loss: 0.0223 - accuracy: 0.9862\n",
      "Epoch 158/200\n",
      "29/29 [==============================] - 0s 644us/step - loss: 0.0221 - accuracy: 0.9862\n",
      "Epoch 159/200\n",
      "29/29 [==============================] - 0s 706us/step - loss: 0.0240 - accuracy: 0.9862\n",
      "Epoch 160/200\n",
      "29/29 [==============================] - 0s 611us/step - loss: 0.0211 - accuracy: 0.9862\n",
      "Epoch 161/200\n",
      "29/29 [==============================] - 0s 681us/step - loss: 0.0202 - accuracy: 0.9862\n",
      "Epoch 162/200\n",
      "29/29 [==============================] - 0s 668us/step - loss: 0.0205 - accuracy: 0.9862\n",
      "Epoch 163/200\n",
      "29/29 [==============================] - 0s 636us/step - loss: 0.0204 - accuracy: 0.9862\n",
      "Epoch 164/200\n",
      "29/29 [==============================] - 0s 634us/step - loss: 0.0202 - accuracy: 0.9862\n",
      "Epoch 165/200\n",
      "29/29 [==============================] - 0s 663us/step - loss: 0.0219 - accuracy: 0.9862\n",
      "Epoch 166/200\n",
      "29/29 [==============================] - 0s 665us/step - loss: 0.0215 - accuracy: 0.9862\n",
      "Epoch 167/200\n",
      "29/29 [==============================] - 0s 646us/step - loss: 0.0247 - accuracy: 0.9862\n",
      "Epoch 168/200\n",
      "29/29 [==============================] - 0s 593us/step - loss: 0.0210 - accuracy: 0.9862\n",
      "Epoch 169/200\n",
      "29/29 [==============================] - 0s 650us/step - loss: 0.0206 - accuracy: 0.9862\n",
      "Epoch 170/200\n",
      "29/29 [==============================] - 0s 682us/step - loss: 0.0194 - accuracy: 0.9862\n",
      "Epoch 171/200\n",
      "29/29 [==============================] - 0s 645us/step - loss: 0.0194 - accuracy: 0.9862\n",
      "Epoch 172/200\n",
      "29/29 [==============================] - 0s 657us/step - loss: 0.0221 - accuracy: 0.9862\n",
      "Epoch 173/200\n",
      "29/29 [==============================] - 0s 661us/step - loss: 0.0198 - accuracy: 0.9862\n",
      "Epoch 174/200\n",
      "29/29 [==============================] - 0s 643us/step - loss: 0.0204 - accuracy: 0.9862\n",
      "Epoch 175/200\n",
      "29/29 [==============================] - 0s 684us/step - loss: 0.0193 - accuracy: 0.9862\n",
      "Epoch 176/200\n",
      "29/29 [==============================] - 0s 736us/step - loss: 0.0190 - accuracy: 0.9862\n",
      "Epoch 177/200\n",
      "29/29 [==============================] - 0s 713us/step - loss: 0.0181 - accuracy: 0.9862\n",
      "Epoch 178/200\n",
      "29/29 [==============================] - 0s 646us/step - loss: 0.0184 - accuracy: 0.9862\n",
      "Epoch 179/200\n",
      "29/29 [==============================] - 0s 622us/step - loss: 0.0195 - accuracy: 0.9862\n",
      "Epoch 180/200\n",
      "29/29 [==============================] - 0s 650us/step - loss: 0.0216 - accuracy: 0.9862\n",
      "Epoch 181/200\n",
      "29/29 [==============================] - 0s 684us/step - loss: 0.0197 - accuracy: 0.9862\n",
      "Epoch 182/200\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.0190 - accuracy: 0.9862\n",
      "Epoch 183/200\n",
      "29/29 [==============================] - 0s 602us/step - loss: 0.0175 - accuracy: 0.9862\n",
      "Epoch 184/200\n",
      "29/29 [==============================] - 0s 617us/step - loss: 0.0182 - accuracy: 0.9862\n",
      "Epoch 185/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 702us/step - loss: 0.0185 - accuracy: 0.9862\n",
      "Epoch 186/200\n",
      "29/29 [==============================] - 0s 674us/step - loss: 0.0180 - accuracy: 0.9862\n",
      "Epoch 187/200\n",
      "29/29 [==============================] - 0s 661us/step - loss: 0.0180 - accuracy: 0.9862\n",
      "Epoch 188/200\n",
      "29/29 [==============================] - 0s 665us/step - loss: 0.0172 - accuracy: 0.9862\n",
      "Epoch 189/200\n",
      "29/29 [==============================] - 0s 683us/step - loss: 0.0175 - accuracy: 0.9862\n",
      "Epoch 190/200\n",
      "29/29 [==============================] - 0s 698us/step - loss: 0.0167 - accuracy: 0.9862\n",
      "Epoch 191/200\n",
      "29/29 [==============================] - 0s 578us/step - loss: 0.0164 - accuracy: 0.9862\n",
      "Epoch 192/200\n",
      "29/29 [==============================] - 0s 662us/step - loss: 0.0242 - accuracy: 0.9862\n",
      "Epoch 193/200\n",
      "29/29 [==============================] - 0s 761us/step - loss: 0.0181 - accuracy: 0.9862\n",
      "Epoch 194/200\n",
      "29/29 [==============================] - 0s 615us/step - loss: 0.0197 - accuracy: 0.9862\n",
      "Epoch 195/200\n",
      "29/29 [==============================] - 0s 605us/step - loss: 0.0339 - accuracy: 0.9655\n",
      "Epoch 196/200\n",
      "29/29 [==============================] - 0s 717us/step - loss: 0.0198 - accuracy: 0.9862\n",
      "Epoch 197/200\n",
      "29/29 [==============================] - 0s 598us/step - loss: 0.0187 - accuracy: 0.9862\n",
      "Epoch 198/200\n",
      "29/29 [==============================] - 0s 741us/step - loss: 0.0248 - accuracy: 0.9793\n",
      "Epoch 199/200\n",
      "29/29 [==============================] - 0s 622us/step - loss: 0.0151 - accuracy: 0.9862\n",
      "Epoch 200/200\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.0136 - accuracy: 0.9862\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001BBF7592B88> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0909 - accuracy: 0.9000\n",
      "\n",
      " 10 fold accurccy: ['0.8636', '0.9048', '0.8095', '0.9048', '1.0000', '0.9048', '0.8571', '0.9500', '1.0000', '0.9000']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "n_fold = 10 \n",
    "# 층화 kfold : 한쪽으로 쏠림이 없는 데이터 만들기\n",
    "skf = StratifiedKFold(n_splits = n_fold , shuffle =True, random_state = 3)\n",
    "accuracy = [] \n",
    "for train ,test in skf.split(X,Y):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, input_dim=60, activation='relu')) # 60 * 24 + 24 \n",
    "    model.add(Dense(10, activation='relu')) # 24 x 10\n",
    "    model.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "    model.compile(loss='mean_squared_error',\n",
    "\n",
    "                 optimizer='adam',\n",
    "\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train, Y_train, epochs=200, batch_size=5)\n",
    "    k_accuracy = \"%.4f\" % (model.evaluate(X[test],Y[test])[1])\n",
    "    accuracy.append(k_accuracy)\n",
    "    \n",
    "# 결과 출력 \n",
    "print(\"\\n %.f fold accurccy:\" % n_fold,accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "\n",
    "# callbacks # 시스템이 call 하는 함수 > 이벤트가 벌어지면 실행\n",
    "# accuracy 가 나빠지는 방향으로 변화가 일어 날때 중지\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint , EarlyStopping\n",
    "\n",
    "\n",
    "seed = 0 \n",
    "numpy.random.seed(seed)\n",
    "tf.random.set_seed(3)\n",
    "df_pre = pd.read_csv('./dataset/wine.csv', header=None)\n",
    "df = df_pre.sample(frac=1) # sampling 100 frac =1 전부다가져오기 0.7  / 70프로만 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6497, 13)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "df_pre.head(2)\n",
    "print(df[12].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.600e+00 6.700e-01 1.400e-01 1.500e+00 7.400e-02 2.500e+01 1.680e+02\n",
      " 9.937e-01 3.050e+00 5.100e-01 9.300e+00 5.000e+00] 0.0\n",
      "Epoch 1/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.6469 - accuracy: 0.7850\n",
      "Epoch 00001: val_loss improved from inf to 0.31924, saving model to ./model\\01-0.3192.hdf5\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.4565 - accuracy: 0.7880 - val_loss: 0.3192 - val_accuracy: 0.8400\n",
      "Epoch 2/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.2845 - accuracy: 0.8750\n",
      "Epoch 00002: val_loss improved from 0.31924 to 0.22752, saving model to ./model\\02-0.2275.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2858 - accuracy: 0.8876 - val_loss: 0.2275 - val_accuracy: 0.9231\n",
      "Epoch 3/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.2758 - accuracy: 0.8900\n",
      "Epoch 00003: val_loss improved from 0.22752 to 0.20518, saving model to ./model\\03-0.2052.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2243 - accuracy: 0.9263 - val_loss: 0.2052 - val_accuracy: 0.9208\n",
      "Epoch 4/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.2175 - accuracy: 0.9300\n",
      "Epoch 00004: val_loss improved from 0.20518 to 0.19030, saving model to ./model\\04-0.1903.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2023 - accuracy: 0.9300 - val_loss: 0.1903 - val_accuracy: 0.9315\n",
      "Epoch 5/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1905 - accuracy: 0.9300\n",
      "Epoch 00005: val_loss improved from 0.19030 to 0.18878, saving model to ./model\\05-0.1888.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1918 - accuracy: 0.9327 - val_loss: 0.1888 - val_accuracy: 0.9300\n",
      "Epoch 6/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1689 - accuracy: 0.9250\n",
      "Epoch 00006: val_loss did not improve from 0.18878\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.1872 - accuracy: 0.9338 - val_loss: 0.1902 - val_accuracy: 0.9277\n",
      "Epoch 7/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1556 - accuracy: 0.9550\n",
      "Epoch 00007: val_loss improved from 0.18878 to 0.18063, saving model to ./model\\07-0.1806.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1833 - accuracy: 0.9357 - val_loss: 0.1806 - val_accuracy: 0.9323\n",
      "Epoch 8/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1534 - accuracy: 0.9350\n",
      "Epoch 00008: val_loss did not improve from 0.18063\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.1797 - accuracy: 0.9375 - val_loss: 0.1839 - val_accuracy: 0.9308\n",
      "Epoch 9/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1586 - accuracy: 0.9400\n",
      "Epoch 00009: val_loss improved from 0.18063 to 0.17738, saving model to ./model\\09-0.1774.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1758 - accuracy: 0.9377 - val_loss: 0.1774 - val_accuracy: 0.9338\n",
      "Epoch 10/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1410 - accuracy: 0.9550\n",
      "Epoch 00010: val_loss improved from 0.17738 to 0.17113, saving model to ./model\\10-0.1711.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1752 - accuracy: 0.9365 - val_loss: 0.1711 - val_accuracy: 0.9346\n",
      "Epoch 11/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.2288 - accuracy: 0.9500\n",
      "Epoch 00011: val_loss did not improve from 0.17113\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.1700 - accuracy: 0.9386 - val_loss: 0.1773 - val_accuracy: 0.9315\n",
      "Epoch 12/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1156 - accuracy: 0.9700\n",
      "Epoch 00012: val_loss improved from 0.17113 to 0.16542, saving model to ./model\\12-0.1654.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1650 - accuracy: 0.9402 - val_loss: 0.1654 - val_accuracy: 0.9400\n",
      "Epoch 13/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1446 - accuracy: 0.9400\n",
      "Epoch 00013: val_loss did not improve from 0.16542\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.1625 - accuracy: 0.9427 - val_loss: 0.1656 - val_accuracy: 0.9392\n",
      "Epoch 14/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.2178 - accuracy: 0.9200\n",
      "Epoch 00014: val_loss did not improve from 0.16542\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.1594 - accuracy: 0.9438 - val_loss: 0.1669 - val_accuracy: 0.9354\n",
      "Epoch 15/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1590 - accuracy: 0.9400\n",
      "Epoch 00015: val_loss improved from 0.16542 to 0.15682, saving model to ./model\\15-0.1568.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1584 - accuracy: 0.9436 - val_loss: 0.1568 - val_accuracy: 0.9377\n",
      "Epoch 16/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.2061 - accuracy: 0.9350\n",
      "Epoch 00016: val_loss improved from 0.15682 to 0.15446, saving model to ./model\\16-0.1545.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1531 - accuracy: 0.9457 - val_loss: 0.1545 - val_accuracy: 0.9423\n",
      "Epoch 17/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1256 - accuracy: 0.9550\n",
      "Epoch 00017: val_loss improved from 0.15446 to 0.15047, saving model to ./model\\17-0.1505.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1492 - accuracy: 0.9479 - val_loss: 0.1505 - val_accuracy: 0.9408\n",
      "Epoch 18/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1731 - accuracy: 0.9350\n",
      "Epoch 00018: val_loss improved from 0.15047 to 0.14981, saving model to ./model\\18-0.1498.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1468 - accuracy: 0.9475 - val_loss: 0.1498 - val_accuracy: 0.9408\n",
      "Epoch 19/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1576 - accuracy: 0.9400\n",
      "Epoch 00019: val_loss improved from 0.14981 to 0.14608, saving model to ./model\\19-0.1461.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1470 - accuracy: 0.9475 - val_loss: 0.1461 - val_accuracy: 0.9423\n",
      "Epoch 20/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1273 - accuracy: 0.9550\n",
      "Epoch 00020: val_loss improved from 0.14608 to 0.14159, saving model to ./model\\20-0.1416.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1417 - accuracy: 0.9496 - val_loss: 0.1416 - val_accuracy: 0.9446\n",
      "Epoch 21/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0794 - accuracy: 0.9800\n",
      "Epoch 00021: val_loss did not improve from 0.14159\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.1371 - accuracy: 0.9494 - val_loss: 0.1432 - val_accuracy: 0.9438\n",
      "Epoch 22/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1543 - accuracy: 0.9500\n",
      "Epoch 00022: val_loss improved from 0.14159 to 0.13302, saving model to ./model\\22-0.1330.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1364 - accuracy: 0.9504 - val_loss: 0.1330 - val_accuracy: 0.9446\n",
      "Epoch 23/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0921 - accuracy: 0.9650\n",
      "Epoch 00023: val_loss improved from 0.13302 to 0.13003, saving model to ./model\\23-0.1300.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1321 - accuracy: 0.9509 - val_loss: 0.1300 - val_accuracy: 0.9462\n",
      "Epoch 24/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1325 - accuracy: 0.9400\n",
      "Epoch 00024: val_loss improved from 0.13003 to 0.12539, saving model to ./model\\24-0.1254.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1281 - accuracy: 0.9525 - val_loss: 0.1254 - val_accuracy: 0.9454\n",
      "Epoch 25/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1281 - accuracy: 0.9650\n",
      "Epoch 00025: val_loss did not improve from 0.12539\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.1264 - accuracy: 0.9536 - val_loss: 0.1333 - val_accuracy: 0.9485\n",
      "Epoch 26/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1198 - accuracy: 0.9600\n",
      "Epoch 00026: val_loss improved from 0.12539 to 0.12421, saving model to ./model\\26-0.1242.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1252 - accuracy: 0.9538 - val_loss: 0.1242 - val_accuracy: 0.9508\n",
      "Epoch 27/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1488 - accuracy: 0.9450\n",
      "Epoch 00027: val_loss improved from 0.12421 to 0.12005, saving model to ./model\\27-0.1201.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1234 - accuracy: 0.9538 - val_loss: 0.1201 - val_accuracy: 0.9492\n",
      "Epoch 28/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1649 - accuracy: 0.9450\n",
      "Epoch 00028: val_loss improved from 0.12005 to 0.11436, saving model to ./model\\28-0.1144.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1221 - accuracy: 0.9548 - val_loss: 0.1144 - val_accuracy: 0.9538\n",
      "Epoch 29/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1154 - accuracy: 0.9550\n",
      "Epoch 00029: val_loss improved from 0.11436 to 0.10895, saving model to ./model\\29-0.1089.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1157 - accuracy: 0.9573 - val_loss: 0.1089 - val_accuracy: 0.9554\n",
      "Epoch 30/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0984 - accuracy: 0.9600\n",
      "Epoch 00030: val_loss improved from 0.10895 to 0.10540, saving model to ./model\\30-0.1054.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1153 - accuracy: 0.9588 - val_loss: 0.1054 - val_accuracy: 0.9577\n",
      "Epoch 31/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1097 - accuracy: 0.9550\n",
      "Epoch 00031: val_loss improved from 0.10540 to 0.09670, saving model to ./model\\31-0.0967.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1093 - accuracy: 0.9638 - val_loss: 0.0967 - val_accuracy: 0.9623\n",
      "Epoch 32/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1117 - accuracy: 0.9600\n",
      "Epoch 00032: val_loss improved from 0.09670 to 0.09443, saving model to ./model\\32-0.0944.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1059 - accuracy: 0.9638 - val_loss: 0.0944 - val_accuracy: 0.9669\n",
      "Epoch 33/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1232 - accuracy: 0.9500\n",
      "Epoch 00033: val_loss improved from 0.09443 to 0.09003, saving model to ./model\\33-0.0900.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1091 - accuracy: 0.9644 - val_loss: 0.0900 - val_accuracy: 0.9685\n",
      "Epoch 34/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0877 - accuracy: 0.9900\n",
      "Epoch 00034: val_loss improved from 0.09003 to 0.08908, saving model to ./model\\34-0.0891.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1015 - accuracy: 0.9677 - val_loss: 0.0891 - val_accuracy: 0.9662\n",
      "Epoch 35/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1123 - accuracy: 0.9550\n",
      "Epoch 00035: val_loss did not improve from 0.08908\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.1068 - accuracy: 0.9657 - val_loss: 0.0949 - val_accuracy: 0.9646\n",
      "Epoch 36/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0982 - accuracy: 0.9650\n",
      "Epoch 00036: val_loss improved from 0.08908 to 0.08356, saving model to ./model\\36-0.0836.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0990 - accuracy: 0.9679 - val_loss: 0.0836 - val_accuracy: 0.9677\n",
      "Epoch 37/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0756 - accuracy: 0.9700\n",
      "Epoch 00037: val_loss improved from 0.08356 to 0.08242, saving model to ./model\\37-0.0824.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0955 - accuracy: 0.9673 - val_loss: 0.0824 - val_accuracy: 0.9715\n",
      "Epoch 38/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1271 - accuracy: 0.9550\n",
      "Epoch 00038: val_loss did not improve from 0.08242\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0928 - accuracy: 0.9711 - val_loss: 0.0841 - val_accuracy: 0.9715\n",
      "Epoch 39/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0691 - accuracy: 0.9700\n",
      "Epoch 00039: val_loss improved from 0.08242 to 0.08063, saving model to ./model\\39-0.0806.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0936 - accuracy: 0.9690 - val_loss: 0.0806 - val_accuracy: 0.9723\n",
      "Epoch 40/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1012 - accuracy: 0.9650\n",
      "Epoch 00040: val_loss improved from 0.08063 to 0.07886, saving model to ./model\\40-0.0789.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0941 - accuracy: 0.9702 - val_loss: 0.0789 - val_accuracy: 0.9723\n",
      "Epoch 41/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0671 - accuracy: 0.9800\n",
      "Epoch 00041: val_loss did not improve from 0.07886\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0882 - accuracy: 0.9721 - val_loss: 0.0909 - val_accuracy: 0.9669\n",
      "Epoch 42/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0596 - accuracy: 0.9750\n",
      "Epoch 00042: val_loss improved from 0.07886 to 0.07882, saving model to ./model\\42-0.0788.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1041 - accuracy: 0.9648 - val_loss: 0.0788 - val_accuracy: 0.9746\n",
      "Epoch 43/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0583 - accuracy: 0.9750\n",
      "Epoch 00043: val_loss improved from 0.07882 to 0.07600, saving model to ./model\\43-0.0760.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0860 - accuracy: 0.9723 - val_loss: 0.0760 - val_accuracy: 0.9746\n",
      "Epoch 44/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1122 - accuracy: 0.9600\n",
      "Epoch 00044: val_loss did not improve from 0.07600\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0839 - accuracy: 0.9740 - val_loss: 0.0767 - val_accuracy: 0.9754\n",
      "Epoch 45/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1181 - accuracy: 0.9750\n",
      "Epoch 00045: val_loss did not improve from 0.07600\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0825 - accuracy: 0.9746 - val_loss: 0.0912 - val_accuracy: 0.9677\n",
      "Epoch 46/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0865 - accuracy: 0.9750\n",
      "Epoch 00046: val_loss improved from 0.07600 to 0.07396, saving model to ./model\\46-0.0740.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0916 - accuracy: 0.9696 - val_loss: 0.0740 - val_accuracy: 0.9769\n",
      "Epoch 47/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0491 - accuracy: 0.9850\n",
      "Epoch 00047: val_loss improved from 0.07396 to 0.07101, saving model to ./model\\47-0.0710.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0810 - accuracy: 0.9742 - val_loss: 0.0710 - val_accuracy: 0.9769\n",
      "Epoch 48/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1015 - accuracy: 0.9700\n",
      "Epoch 00048: val_loss improved from 0.07101 to 0.06985, saving model to ./model\\48-0.0698.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0802 - accuracy: 0.9752 - val_loss: 0.0698 - val_accuracy: 0.9785\n",
      "Epoch 49/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0702 - accuracy: 0.9750\n",
      "Epoch 00049: val_loss did not improve from 0.06985\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0802 - accuracy: 0.9748 - val_loss: 0.0726 - val_accuracy: 0.9769\n",
      "Epoch 50/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1181 - accuracy: 0.9650\n",
      "Epoch 00050: val_loss did not improve from 0.06985\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0796 - accuracy: 0.9752 - val_loss: 0.0703 - val_accuracy: 0.9785\n",
      "Epoch 51/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0469 - accuracy: 0.9900\n",
      "Epoch 00051: val_loss improved from 0.06985 to 0.06946, saving model to ./model\\51-0.0695.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0791 - accuracy: 0.9759 - val_loss: 0.0695 - val_accuracy: 0.9785\n",
      "Epoch 52/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0592 - accuracy: 0.9700\n",
      "Epoch 00052: val_loss did not improve from 0.06946\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0864 - accuracy: 0.9721 - val_loss: 0.0704 - val_accuracy: 0.9792\n",
      "Epoch 53/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0777 - accuracy: 0.9650\n",
      "Epoch 00053: val_loss did not improve from 0.06946\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0818 - accuracy: 0.9723 - val_loss: 0.0705 - val_accuracy: 0.9769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0595 - accuracy: 0.9750\n",
      "Epoch 00054: val_loss improved from 0.06946 to 0.06735, saving model to ./model\\54-0.0673.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0755 - accuracy: 0.9763 - val_loss: 0.0673 - val_accuracy: 0.9800\n",
      "Epoch 55/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0844 - accuracy: 0.9800\n",
      "Epoch 00055: val_loss did not improve from 0.06735\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0735 - accuracy: 0.9759 - val_loss: 0.0689 - val_accuracy: 0.9792\n",
      "Epoch 56/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0509 - accuracy: 0.9800\n",
      "Epoch 00056: val_loss improved from 0.06735 to 0.06643, saving model to ./model\\56-0.0664.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0754 - accuracy: 0.9771 - val_loss: 0.0664 - val_accuracy: 0.9808\n",
      "Epoch 57/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0649 - accuracy: 0.9650\n",
      "Epoch 00057: val_loss improved from 0.06643 to 0.06632, saving model to ./model\\57-0.0663.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0739 - accuracy: 0.9771 - val_loss: 0.0663 - val_accuracy: 0.9792\n",
      "Epoch 58/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0841 - accuracy: 0.9800\n",
      "Epoch 00058: val_loss improved from 0.06632 to 0.06507, saving model to ./model\\58-0.0651.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0728 - accuracy: 0.9773 - val_loss: 0.0651 - val_accuracy: 0.9808\n",
      "Epoch 59/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0573 - accuracy: 0.9750\n",
      "Epoch 00059: val_loss did not improve from 0.06507\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0750 - accuracy: 0.9756 - val_loss: 0.0849 - val_accuracy: 0.9692\n",
      "Epoch 60/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0574 - accuracy: 0.9700\n",
      "Epoch 00060: val_loss did not improve from 0.06507\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0784 - accuracy: 0.9746 - val_loss: 0.0666 - val_accuracy: 0.9792\n",
      "Epoch 61/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0995 - accuracy: 0.9650\n",
      "Epoch 00061: val_loss did not improve from 0.06507\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0726 - accuracy: 0.9783 - val_loss: 0.0690 - val_accuracy: 0.9777\n",
      "Epoch 62/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0694 - accuracy: 0.9800\n",
      "Epoch 00062: val_loss did not improve from 0.06507\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0695 - accuracy: 0.9781 - val_loss: 0.0681 - val_accuracy: 0.9792\n",
      "Epoch 63/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0360 - accuracy: 0.9900\n",
      "Epoch 00063: val_loss improved from 0.06507 to 0.06470, saving model to ./model\\63-0.0647.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0699 - accuracy: 0.9781 - val_loss: 0.0647 - val_accuracy: 0.9808\n",
      "Epoch 64/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0515 - accuracy: 0.9800\n",
      "Epoch 00064: val_loss improved from 0.06470 to 0.06154, saving model to ./model\\64-0.0615.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0684 - accuracy: 0.9779 - val_loss: 0.0615 - val_accuracy: 0.9831\n",
      "Epoch 65/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0228 - accuracy: 0.9950\n",
      "Epoch 00065: val_loss did not improve from 0.06154\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0685 - accuracy: 0.9794 - val_loss: 0.0626 - val_accuracy: 0.9808\n",
      "Epoch 66/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0535 - accuracy: 0.9850\n",
      "Epoch 00066: val_loss did not improve from 0.06154\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0723 - accuracy: 0.9773 - val_loss: 0.0688 - val_accuracy: 0.9800\n",
      "Epoch 67/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0552 - accuracy: 0.9850\n",
      "Epoch 00067: val_loss did not improve from 0.06154\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0725 - accuracy: 0.9777 - val_loss: 0.0625 - val_accuracy: 0.9831\n",
      "Epoch 68/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1107 - accuracy: 0.9800\n",
      "Epoch 00068: val_loss did not improve from 0.06154\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0685 - accuracy: 0.9796 - val_loss: 0.0625 - val_accuracy: 0.9792\n",
      "Epoch 69/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1319 - accuracy: 0.9650\n",
      "Epoch 00069: val_loss did not improve from 0.06154\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.9788 - val_loss: 0.0679 - val_accuracy: 0.9777\n",
      "Epoch 70/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0912 - accuracy: 0.9700\n",
      "Epoch 00070: val_loss improved from 0.06154 to 0.06008, saving model to ./model\\70-0.0601.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0704 - accuracy: 0.9771 - val_loss: 0.0601 - val_accuracy: 0.9831\n",
      "Epoch 71/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0513 - accuracy: 0.9800\n",
      "Epoch 00071: val_loss did not improve from 0.06008\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0651 - accuracy: 0.9810 - val_loss: 0.0621 - val_accuracy: 0.9800\n",
      "Epoch 72/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0385 - accuracy: 0.9900\n",
      "Epoch 00072: val_loss did not improve from 0.06008\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0676 - accuracy: 0.9788 - val_loss: 0.0602 - val_accuracy: 0.9823\n",
      "Epoch 73/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0524 - accuracy: 0.9800\n",
      "Epoch 00073: val_loss did not improve from 0.06008\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0674 - accuracy: 0.9796 - val_loss: 0.0602 - val_accuracy: 0.9823\n",
      "Epoch 74/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0562 - accuracy: 0.9800\n",
      "Epoch 00074: val_loss did not improve from 0.06008\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0672 - accuracy: 0.9806 - val_loss: 0.0690 - val_accuracy: 0.9762\n",
      "Epoch 75/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1098 - accuracy: 0.9700\n",
      "Epoch 00075: val_loss did not improve from 0.06008\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0672 - accuracy: 0.9798 - val_loss: 0.0790 - val_accuracy: 0.9754\n",
      "Epoch 76/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1307 - accuracy: 0.9600\n",
      "Epoch 00076: val_loss did not improve from 0.06008\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0783 - accuracy: 0.9765 - val_loss: 0.0762 - val_accuracy: 0.9769\n",
      "Epoch 77/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0633 - accuracy: 0.9800\n",
      "Epoch 00077: val_loss did not improve from 0.06008\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0718 - accuracy: 0.9779 - val_loss: 0.0699 - val_accuracy: 0.9777\n",
      "Epoch 78/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0408 - accuracy: 0.9800\n",
      "Epoch 00078: val_loss did not improve from 0.06008\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0642 - accuracy: 0.9798 - val_loss: 0.0688 - val_accuracy: 0.9777\n",
      "Epoch 79/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1339 - accuracy: 0.9500\n",
      "Epoch 00079: val_loss did not improve from 0.06008\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0673 - accuracy: 0.9806 - val_loss: 0.0616 - val_accuracy: 0.9800\n",
      "Epoch 80/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0874 - accuracy: 0.9750\n",
      "Epoch 00080: val_loss did not improve from 0.06008\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0679 - accuracy: 0.9788 - val_loss: 0.0613 - val_accuracy: 0.9808\n",
      "Epoch 81/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0426 - accuracy: 0.9900\n",
      "Epoch 00081: val_loss did not improve from 0.06008\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0631 - accuracy: 0.9817 - val_loss: 0.0626 - val_accuracy: 0.9808\n",
      "Epoch 82/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0456 - accuracy: 0.9700\n",
      "Epoch 00082: val_loss improved from 0.06008 to 0.05944, saving model to ./model\\82-0.0594.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0633 - accuracy: 0.9813 - val_loss: 0.0594 - val_accuracy: 0.9831\n",
      "Epoch 83/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0360 - accuracy: 0.9850\n",
      "Epoch 00083: val_loss improved from 0.05944 to 0.05844, saving model to ./model\\83-0.0584.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0688 - accuracy: 0.9784 - val_loss: 0.0584 - val_accuracy: 0.9831\n",
      "Epoch 84/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0800 - accuracy: 0.9750\n",
      "Epoch 00084: val_loss did not improve from 0.05844\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0646 - accuracy: 0.9800 - val_loss: 0.0586 - val_accuracy: 0.9838\n",
      "Epoch 85/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0410 - accuracy: 0.9900\n",
      "Epoch 00085: val_loss did not improve from 0.05844\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0631 - accuracy: 0.9810 - val_loss: 0.0590 - val_accuracy: 0.9831\n",
      "Epoch 86/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0607 - accuracy: 0.9800\n",
      "Epoch 00086: val_loss did not improve from 0.05844\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0606 - accuracy: 0.9827 - val_loss: 0.0610 - val_accuracy: 0.9808\n",
      "Epoch 87/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0715 - accuracy: 0.9750\n",
      "Epoch 00087: val_loss did not improve from 0.05844\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0670 - accuracy: 0.9810 - val_loss: 0.0680 - val_accuracy: 0.9831\n",
      "Epoch 88/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0602 - accuracy: 0.9800\n",
      "Epoch 00088: val_loss improved from 0.05844 to 0.05790, saving model to ./model\\88-0.0579.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0607 - accuracy: 0.9808 - val_loss: 0.0579 - val_accuracy: 0.9846\n",
      "Epoch 89/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0669 - accuracy: 0.9750\n",
      "Epoch 00089: val_loss did not improve from 0.05790\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0601 - accuracy: 0.9825 - val_loss: 0.0583 - val_accuracy: 0.9831\n",
      "Epoch 90/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0455 - accuracy: 0.9850\n",
      "Epoch 00090: val_loss did not improve from 0.05790\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0591 - accuracy: 0.9825 - val_loss: 0.0582 - val_accuracy: 0.9823\n",
      "Epoch 91/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0375 - accuracy: 0.9900\n",
      "Epoch 00091: val_loss did not improve from 0.05790\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0611 - accuracy: 0.9815 - val_loss: 0.0600 - val_accuracy: 0.9838\n",
      "Epoch 92/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0682 - accuracy: 0.9700\n",
      "Epoch 00092: val_loss improved from 0.05790 to 0.05694, saving model to ./model\\92-0.0569.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0647 - accuracy: 0.9800 - val_loss: 0.0569 - val_accuracy: 0.9831\n",
      "Epoch 93/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1144 - accuracy: 0.9900\n",
      "Epoch 00093: val_loss did not improve from 0.05694\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0661 - accuracy: 0.9790 - val_loss: 0.0593 - val_accuracy: 0.9823\n",
      "Epoch 94/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0503 - accuracy: 0.9750\n",
      "Epoch 00094: val_loss did not improve from 0.05694\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0621 - accuracy: 0.9821 - val_loss: 0.0578 - val_accuracy: 0.9823\n",
      "Epoch 95/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0325 - accuracy: 0.9900\n",
      "Epoch 00095: val_loss did not improve from 0.05694\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0650 - accuracy: 0.9802 - val_loss: 0.0601 - val_accuracy: 0.9815\n",
      "Epoch 96/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0365 - accuracy: 0.9900\n",
      "Epoch 00096: val_loss improved from 0.05694 to 0.05602, saving model to ./model\\96-0.0560.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0594 - accuracy: 0.9833 - val_loss: 0.0560 - val_accuracy: 0.9831\n",
      "Epoch 97/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0228 - accuracy: 0.9950\n",
      "Epoch 00097: val_loss did not improve from 0.05602\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0596 - accuracy: 0.9817 - val_loss: 0.0566 - val_accuracy: 0.9831\n",
      "Epoch 98/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0234 - accuracy: 0.9950\n",
      "Epoch 00098: val_loss did not improve from 0.05602\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0592 - accuracy: 0.9823 - val_loss: 0.0575 - val_accuracy: 0.9831\n",
      "Epoch 99/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0206 - accuracy: 0.9950\n",
      "Epoch 00099: val_loss improved from 0.05602 to 0.05550, saving model to ./model\\99-0.0555.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0577 - accuracy: 0.9838 - val_loss: 0.0555 - val_accuracy: 0.9854\n",
      "Epoch 100/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0531 - accuracy: 0.9850\n",
      "Epoch 00100: val_loss did not improve from 0.05550\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0582 - accuracy: 0.9827 - val_loss: 0.0579 - val_accuracy: 0.9838\n",
      "Epoch 101/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0240 - accuracy: 1.0000\n",
      "Epoch 00101: val_loss did not improve from 0.05550\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0577 - accuracy: 0.9833 - val_loss: 0.0563 - val_accuracy: 0.9838\n",
      "Epoch 102/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0627 - accuracy: 0.9750\n",
      "Epoch 00102: val_loss did not improve from 0.05550\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0595 - accuracy: 0.9825 - val_loss: 0.0578 - val_accuracy: 0.9838\n",
      "Epoch 103/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0342 - accuracy: 0.9900\n",
      "Epoch 00103: val_loss did not improve from 0.05550\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0611 - accuracy: 0.9829 - val_loss: 0.0564 - val_accuracy: 0.9831\n",
      "Epoch 104/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0557 - accuracy: 0.9850\n",
      "Epoch 00104: val_loss did not improve from 0.05550\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0580 - accuracy: 0.9833 - val_loss: 0.0743 - val_accuracy: 0.9769\n",
      "Epoch 105/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0596 - accuracy: 0.9700\n",
      "Epoch 00105: val_loss did not improve from 0.05550\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0625 - accuracy: 0.9800 - val_loss: 0.0569 - val_accuracy: 0.9846\n",
      "Epoch 106/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0435 - accuracy: 0.9850\n",
      "Epoch 00106: val_loss did not improve from 0.05550\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0569 - accuracy: 0.9846 - val_loss: 0.0556 - val_accuracy: 0.9838\n",
      "Epoch 107/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1330 - accuracy: 0.9750\n",
      "Epoch 00107: val_loss did not improve from 0.05550\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0567 - accuracy: 0.9833 - val_loss: 0.0574 - val_accuracy: 0.9854\n",
      "Epoch 108/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0321 - accuracy: 0.9900\n",
      "Epoch 00108: val_loss improved from 0.05550 to 0.05481, saving model to ./model\\108-0.0548.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0554 - accuracy: 0.9840 - val_loss: 0.0548 - val_accuracy: 0.9862\n",
      "Epoch 109/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0199 - accuracy: 1.0000\n",
      "Epoch 00109: val_loss improved from 0.05481 to 0.05473, saving model to ./model\\109-0.0547.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0564 - accuracy: 0.9835 - val_loss: 0.0547 - val_accuracy: 0.9846\n",
      "Epoch 110/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0270 - accuracy: 0.9900\n",
      "Epoch 00110: val_loss did not improve from 0.05473\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0574 - accuracy: 0.9831 - val_loss: 0.0558 - val_accuracy: 0.9838\n",
      "Epoch 111/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0737 - accuracy: 0.9900\n",
      "Epoch 00111: val_loss did not improve from 0.05473\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0604 - accuracy: 0.9813 - val_loss: 0.0562 - val_accuracy: 0.9854\n",
      "Epoch 112/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0778 - accuracy: 0.9700\n",
      "Epoch 00112: val_loss did not improve from 0.05473\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0611 - accuracy: 0.9821 - val_loss: 0.0637 - val_accuracy: 0.9815\n",
      "Epoch 113/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0564 - accuracy: 0.9700\n",
      "Epoch 00113: val_loss did not improve from 0.05473\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0610 - accuracy: 0.9815 - val_loss: 0.0551 - val_accuracy: 0.9854\n",
      "Epoch 114/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0169 - accuracy: 1.0000\n",
      "Epoch 00114: val_loss improved from 0.05473 to 0.05448, saving model to ./model\\114-0.0545.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0553 - accuracy: 0.9842 - val_loss: 0.0545 - val_accuracy: 0.9846\n",
      "Epoch 115/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0173 - accuracy: 0.9950\n",
      "Epoch 00115: val_loss did not improve from 0.05448\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0591 - accuracy: 0.9829 - val_loss: 0.0563 - val_accuracy: 0.9862\n",
      "Epoch 116/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0701 - accuracy: 0.9750\n",
      "Epoch 00116: val_loss did not improve from 0.05448\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0575 - accuracy: 0.9842 - val_loss: 0.0550 - val_accuracy: 0.9869\n",
      "Epoch 117/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0890 - accuracy: 0.9850\n",
      "Epoch 00117: val_loss did not improve from 0.05448\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0546 - accuracy: 0.9842 - val_loss: 0.0550 - val_accuracy: 0.9838\n",
      "Epoch 118/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0575 - accuracy: 0.9950\n",
      "Epoch 00118: val_loss improved from 0.05448 to 0.05383, saving model to ./model\\118-0.0538.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0558 - accuracy: 0.9833 - val_loss: 0.0538 - val_accuracy: 0.9862\n",
      "Epoch 119/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0578 - accuracy: 0.9900\n",
      "Epoch 00119: val_loss did not improve from 0.05383\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0566 - accuracy: 0.9842 - val_loss: 0.0550 - val_accuracy: 0.9846\n",
      "Epoch 120/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0482 - accuracy: 0.9900\n",
      "Epoch 00120: val_loss improved from 0.05383 to 0.05345, saving model to ./model\\120-0.0535.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0592 - accuracy: 0.9817 - val_loss: 0.0535 - val_accuracy: 0.9846\n",
      "Epoch 121/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0410 - accuracy: 0.9850\n",
      "Epoch 00121: val_loss did not improve from 0.05345\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0539 - accuracy: 0.9846 - val_loss: 0.0553 - val_accuracy: 0.9862\n",
      "Epoch 122/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0458 - accuracy: 0.9850\n",
      "Epoch 00122: val_loss did not improve from 0.05345\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0572 - accuracy: 0.9844 - val_loss: 0.0564 - val_accuracy: 0.9854\n",
      "Epoch 123/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0694 - accuracy: 0.9900\n",
      "Epoch 00123: val_loss did not improve from 0.05345\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0588 - accuracy: 0.9829 - val_loss: 0.0573 - val_accuracy: 0.9854\n",
      "Epoch 124/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0434 - accuracy: 0.9800\n",
      "Epoch 00124: val_loss did not improve from 0.05345\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0569 - accuracy: 0.9827 - val_loss: 0.0553 - val_accuracy: 0.9846\n",
      "Epoch 125/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0928 - accuracy: 0.9700\n",
      "Epoch 00125: val_loss did not improve from 0.05345\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0562 - accuracy: 0.9842 - val_loss: 0.0550 - val_accuracy: 0.9838\n",
      "Epoch 126/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0501 - accuracy: 0.9900\n",
      "Epoch 00126: val_loss did not improve from 0.05345\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0568 - accuracy: 0.9863 - val_loss: 0.0561 - val_accuracy: 0.9869\n",
      "Epoch 127/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0788 - accuracy: 0.9900\n",
      "Epoch 00127: val_loss did not improve from 0.05345\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0557 - accuracy: 0.9838 - val_loss: 0.0548 - val_accuracy: 0.9846\n",
      "Epoch 128/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0389 - accuracy: 0.9850\n",
      "Epoch 00128: val_loss did not improve from 0.05345\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0534 - accuracy: 0.9846 - val_loss: 0.0551 - val_accuracy: 0.9854\n",
      "Epoch 129/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0207 - accuracy: 0.9950\n",
      "Epoch 00129: val_loss did not improve from 0.05345\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0544 - accuracy: 0.9835 - val_loss: 0.0538 - val_accuracy: 0.9854\n",
      "Epoch 130/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0424 - accuracy: 0.9850\n",
      "Epoch 00130: val_loss did not improve from 0.05345\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0571 - accuracy: 0.9838 - val_loss: 0.0550 - val_accuracy: 0.9862\n",
      "Epoch 131/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0743 - accuracy: 0.9850\n",
      "Epoch 00131: val_loss did not improve from 0.05345\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0548 - accuracy: 0.9831 - val_loss: 0.0557 - val_accuracy: 0.9854\n",
      "Epoch 132/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0537 - accuracy: 0.9650\n",
      "Epoch 00132: val_loss did not improve from 0.05345\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0533 - accuracy: 0.9840 - val_loss: 0.0544 - val_accuracy: 0.9862\n",
      "Epoch 133/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0448 - accuracy: 0.9900\n",
      "Epoch 00133: val_loss did not improve from 0.05345\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0531 - accuracy: 0.9858 - val_loss: 0.0544 - val_accuracy: 0.9846\n",
      "Epoch 134/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0765 - accuracy: 0.9650\n",
      "Epoch 00134: val_loss did not improve from 0.05345\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0610 - accuracy: 0.9804 - val_loss: 0.0775 - val_accuracy: 0.9762\n",
      "Epoch 135/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1411 - accuracy: 0.9550\n",
      "Epoch 00135: val_loss did not improve from 0.05345\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0624 - accuracy: 0.9808 - val_loss: 0.0548 - val_accuracy: 0.9846\n",
      "Epoch 136/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0754 - accuracy: 0.9600\n",
      "Epoch 00136: val_loss did not improve from 0.05345\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0530 - accuracy: 0.9848 - val_loss: 0.0535 - val_accuracy: 0.9854\n",
      "Epoch 137/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0234 - accuracy: 0.9950\n",
      "Epoch 00137: val_loss did not improve from 0.05345\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0580 - accuracy: 0.9810 - val_loss: 0.0539 - val_accuracy: 0.9869\n",
      "Epoch 138/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0899 - accuracy: 0.9800\n",
      "Epoch 00138: val_loss did not improve from 0.05345\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0531 - accuracy: 0.9846 - val_loss: 0.0575 - val_accuracy: 0.9831\n",
      "Epoch 139/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0384 - accuracy: 0.9850\n",
      "Epoch 00139: val_loss improved from 0.05345 to 0.05279, saving model to ./model\\139-0.0528.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0546 - accuracy: 0.9836 - val_loss: 0.0528 - val_accuracy: 0.9862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0380 - accuracy: 0.9950\n",
      "Epoch 00140: val_loss did not improve from 0.05279\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0532 - accuracy: 0.9863 - val_loss: 0.0538 - val_accuracy: 0.9854\n",
      "Epoch 141/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0341 - accuracy: 0.9900\n",
      "Epoch 00141: val_loss did not improve from 0.05279\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0574 - accuracy: 0.9840 - val_loss: 0.0551 - val_accuracy: 0.9885\n",
      "Epoch 142/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0871 - accuracy: 0.9750\n",
      "Epoch 00142: val_loss improved from 0.05279 to 0.05259, saving model to ./model\\142-0.0526.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0556 - accuracy: 0.9829 - val_loss: 0.0526 - val_accuracy: 0.9869\n",
      "Epoch 143/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0278 - accuracy: 0.9950\n",
      "Epoch 00143: val_loss did not improve from 0.05259\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0543 - accuracy: 0.9846 - val_loss: 0.0548 - val_accuracy: 0.9862\n",
      "Epoch 144/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0638 - accuracy: 0.9750\n",
      "Epoch 00144: val_loss did not improve from 0.05259\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0568 - accuracy: 0.9831 - val_loss: 0.0535 - val_accuracy: 0.9854\n",
      "Epoch 145/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0799 - accuracy: 0.9850\n",
      "Epoch 00145: val_loss did not improve from 0.05259\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0572 - accuracy: 0.9819 - val_loss: 0.0545 - val_accuracy: 0.9862\n",
      "Epoch 146/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0737 - accuracy: 0.9750\n",
      "Epoch 00146: val_loss did not improve from 0.05259\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0558 - accuracy: 0.9829 - val_loss: 0.0537 - val_accuracy: 0.9854\n",
      "Epoch 147/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0651 - accuracy: 0.9700\n",
      "Epoch 00147: val_loss improved from 0.05259 to 0.05080, saving model to ./model\\147-0.0508.hdf5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0521 - accuracy: 0.9852 - val_loss: 0.0508 - val_accuracy: 0.9869\n",
      "Epoch 148/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0399 - accuracy: 0.9900\n",
      "Epoch 00148: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0546 - accuracy: 0.9850 - val_loss: 0.0548 - val_accuracy: 0.9854\n",
      "Epoch 149/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0360 - accuracy: 0.9900\n",
      "Epoch 00149: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0553 - accuracy: 0.9846 - val_loss: 0.0610 - val_accuracy: 0.9831\n",
      "Epoch 150/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0854 - accuracy: 0.9850\n",
      "Epoch 00150: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0532 - accuracy: 0.9835 - val_loss: 0.0537 - val_accuracy: 0.9862\n",
      "Epoch 151/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0352 - accuracy: 0.9800\n",
      "Epoch 00151: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0511 - accuracy: 0.9860 - val_loss: 0.0526 - val_accuracy: 0.9862\n",
      "Epoch 152/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0737 - accuracy: 0.9800\n",
      "Epoch 00152: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0557 - accuracy: 0.9827 - val_loss: 0.0520 - val_accuracy: 0.9862\n",
      "Epoch 153/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0324 - accuracy: 0.9900\n",
      "Epoch 00153: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0559 - accuracy: 0.9835 - val_loss: 0.0540 - val_accuracy: 0.9854\n",
      "Epoch 154/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0364 - accuracy: 0.9850\n",
      "Epoch 00154: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0519 - accuracy: 0.9869 - val_loss: 0.0569 - val_accuracy: 0.9838\n",
      "Epoch 155/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0419 - accuracy: 0.9850\n",
      "Epoch 00155: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0526 - accuracy: 0.9850 - val_loss: 0.0531 - val_accuracy: 0.9877\n",
      "Epoch 156/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0269 - accuracy: 0.9950\n",
      "Epoch 00156: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0590 - accuracy: 0.9817 - val_loss: 0.0550 - val_accuracy: 0.9869\n",
      "Epoch 157/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0235 - accuracy: 0.9950\n",
      "Epoch 00157: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0532 - accuracy: 0.9850 - val_loss: 0.0524 - val_accuracy: 0.9846\n",
      "Epoch 158/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0407 - accuracy: 0.9800\n",
      "Epoch 00158: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0529 - accuracy: 0.9850 - val_loss: 0.0537 - val_accuracy: 0.9854\n",
      "Epoch 159/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.1219 - accuracy: 0.9800\n",
      "Epoch 00159: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0502 - accuracy: 0.9860 - val_loss: 0.0532 - val_accuracy: 0.9862\n",
      "Epoch 160/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0769 - accuracy: 0.9750\n",
      "Epoch 00160: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0547 - accuracy: 0.9836 - val_loss: 0.0538 - val_accuracy: 0.9869\n",
      "Epoch 161/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0313 - accuracy: 0.9950\n",
      "Epoch 00161: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0527 - accuracy: 0.9852 - val_loss: 0.0521 - val_accuracy: 0.9877\n",
      "Epoch 162/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0777 - accuracy: 0.9800\n",
      "Epoch 00162: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0523 - accuracy: 0.9850 - val_loss: 0.0598 - val_accuracy: 0.9831\n",
      "Epoch 163/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0296 - accuracy: 0.9850\n",
      "Epoch 00163: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0518 - accuracy: 0.9842 - val_loss: 0.0605 - val_accuracy: 0.9838\n",
      "Epoch 164/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0265 - accuracy: 0.9900\n",
      "Epoch 00164: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0526 - accuracy: 0.9846 - val_loss: 0.0556 - val_accuracy: 0.9862\n",
      "Epoch 165/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0557 - accuracy: 0.9800\n",
      "Epoch 00165: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0539 - accuracy: 0.9844 - val_loss: 0.0557 - val_accuracy: 0.9838\n",
      "Epoch 166/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0160 - accuracy: 1.0000\n",
      "Epoch 00166: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0544 - accuracy: 0.9850 - val_loss: 0.0543 - val_accuracy: 0.9862\n",
      "Epoch 167/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0386 - accuracy: 0.9950\n",
      "Epoch 00167: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0543 - accuracy: 0.9833 - val_loss: 0.0556 - val_accuracy: 0.9885\n",
      "Epoch 168/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0333 - accuracy: 0.9950\n",
      "Epoch 00168: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0567 - accuracy: 0.9836 - val_loss: 0.0563 - val_accuracy: 0.9846\n",
      "Epoch 169/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0597 - accuracy: 0.9950\n",
      "Epoch 00169: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0543 - accuracy: 0.9838 - val_loss: 0.0534 - val_accuracy: 0.9862\n",
      "Epoch 170/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0626 - accuracy: 0.9900\n",
      "Epoch 00170: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0507 - accuracy: 0.9860 - val_loss: 0.0589 - val_accuracy: 0.9838\n",
      "Epoch 171/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0617 - accuracy: 0.9750\n",
      "Epoch 00171: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0538 - accuracy: 0.9846 - val_loss: 0.0516 - val_accuracy: 0.9869\n",
      "Epoch 172/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0396 - accuracy: 0.9750\n",
      "Epoch 00172: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0514 - accuracy: 0.9852 - val_loss: 0.0544 - val_accuracy: 0.9846\n",
      "Epoch 173/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0600 - accuracy: 0.9850\n",
      "Epoch 00173: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0502 - accuracy: 0.9858 - val_loss: 0.0517 - val_accuracy: 0.9862\n",
      "Epoch 174/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0498 - accuracy: 0.9750\n",
      "Epoch 00174: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0507 - accuracy: 0.9858 - val_loss: 0.0536 - val_accuracy: 0.9854\n",
      "Epoch 175/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0216 - accuracy: 1.0000\n",
      "Epoch 00175: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0503 - accuracy: 0.9861 - val_loss: 0.0551 - val_accuracy: 0.9862\n",
      "Epoch 176/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0354 - accuracy: 0.9800\n",
      "Epoch 00176: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0526 - accuracy: 0.9850 - val_loss: 0.0543 - val_accuracy: 0.9877\n",
      "Epoch 177/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0348 - accuracy: 0.9850\n",
      "Epoch 00177: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0518 - accuracy: 0.9850 - val_loss: 0.0542 - val_accuracy: 0.9846\n",
      "Epoch 178/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0515 - accuracy: 0.9800\n",
      "Epoch 00178: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0531 - accuracy: 0.9842 - val_loss: 0.0531 - val_accuracy: 0.9862\n",
      "Epoch 179/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0411 - accuracy: 0.9900\n",
      "Epoch 00179: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0491 - accuracy: 0.9860 - val_loss: 0.0583 - val_accuracy: 0.9869\n",
      "Epoch 180/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0462 - accuracy: 0.9800\n",
      "Epoch 00180: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0524 - accuracy: 0.9836 - val_loss: 0.0566 - val_accuracy: 0.9838\n",
      "Epoch 181/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0307 - accuracy: 0.9900\n",
      "Epoch 00181: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0522 - accuracy: 0.9844 - val_loss: 0.0530 - val_accuracy: 0.9854\n",
      "Epoch 182/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0202 - accuracy: 0.9950\n",
      "Epoch 00182: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0489 - accuracy: 0.9869 - val_loss: 0.0575 - val_accuracy: 0.9838\n",
      "Epoch 183/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0279 - accuracy: 0.9850\n",
      "Epoch 00183: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0509 - accuracy: 0.9850 - val_loss: 0.0566 - val_accuracy: 0.9838\n",
      "Epoch 184/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0154 - accuracy: 0.9950\n",
      "Epoch 00184: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0495 - accuracy: 0.9858 - val_loss: 0.0660 - val_accuracy: 0.9808\n",
      "Epoch 185/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0819 - accuracy: 0.9850\n",
      "Epoch 00185: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0527 - accuracy: 0.9854 - val_loss: 0.0528 - val_accuracy: 0.9862\n",
      "Epoch 186/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0504 - accuracy: 0.9800\n",
      "Epoch 00186: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0499 - accuracy: 0.9861 - val_loss: 0.0551 - val_accuracy: 0.9838\n",
      "Epoch 187/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0405 - accuracy: 0.9750\n",
      "Epoch 00187: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0520 - accuracy: 0.9850 - val_loss: 0.0522 - val_accuracy: 0.9854\n",
      "Epoch 188/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0253 - accuracy: 0.9950\n",
      "Epoch 00188: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0522 - accuracy: 0.9846 - val_loss: 0.0560 - val_accuracy: 0.9846\n",
      "Epoch 189/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0376 - accuracy: 0.9850\n",
      "Epoch 00189: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0538 - accuracy: 0.9842 - val_loss: 0.0556 - val_accuracy: 0.9869\n",
      "Epoch 190/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0994 - accuracy: 0.9850\n",
      "Epoch 00190: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0481 - accuracy: 0.9865 - val_loss: 0.0527 - val_accuracy: 0.9854\n",
      "Epoch 191/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0253 - accuracy: 0.9900\n",
      "Epoch 00191: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0493 - accuracy: 0.9852 - val_loss: 0.0509 - val_accuracy: 0.9862\n",
      "Epoch 192/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0294 - accuracy: 0.9900\n",
      "Epoch 00192: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0512 - accuracy: 0.9850 - val_loss: 0.0521 - val_accuracy: 0.9862\n",
      "Epoch 193/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 00193: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0562 - accuracy: 0.9842 - val_loss: 0.0554 - val_accuracy: 0.9862\n",
      "Epoch 194/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0671 - accuracy: 0.9900\n",
      "Epoch 00194: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0509 - accuracy: 0.9850 - val_loss: 0.0544 - val_accuracy: 0.9846\n",
      "Epoch 195/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0217 - accuracy: 0.9900\n",
      "Epoch 00195: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0514 - accuracy: 0.9844 - val_loss: 0.0565 - val_accuracy: 0.9854\n",
      "Epoch 196/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0512 - accuracy: 0.9700\n",
      "Epoch 00196: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0495 - accuracy: 0.9860 - val_loss: 0.0519 - val_accuracy: 0.9862\n",
      "Epoch 197/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0785 - accuracy: 0.9800\n",
      "Epoch 00197: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0482 - accuracy: 0.9863 - val_loss: 0.0523 - val_accuracy: 0.9862\n",
      "Epoch 198/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0302 - accuracy: 0.9900\n",
      "Epoch 00198: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0483 - accuracy: 0.9856 - val_loss: 0.0511 - val_accuracy: 0.9869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0444 - accuracy: 0.9850\n",
      "Epoch 00199: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0481 - accuracy: 0.9858 - val_loss: 0.0520 - val_accuracy: 0.9877\n",
      "Epoch 200/200\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 0.0264 - accuracy: 0.9850\n",
      "Epoch 00200: val_loss did not improve from 0.05080\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0480 - accuracy: 0.9860 - val_loss: 0.0513 - val_accuracy: 0.9854\n",
      "204/204 [==============================] - 0s 583us/step - loss: 0.0468 - accuracy: 0.9866\n",
      "\n",
      " Accuracy: 0.9866\n"
     ]
    }
   ],
   "source": [
    "dataset = df.values\n",
    "X = dataset[:,0:12]\n",
    "Y = dataset[:, 12]\n",
    "\n",
    "print(X[3],Y[3])\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=12 ,activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu')) # 마지막 sigmoid 분류 0~ 1 확률값으로 매핑\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#모델 : 모델 구조 + 가중치\n",
    "#모델 최적화\n",
    "import os \n",
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "\n",
    "modelpath=\"./model/{epoch:02d}-{val_loss:.4f}.hdf5\" # 최적의 모델을 저장할 위치\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience = 100)\n",
    "history =  model.fit(X,Y ,validation_split=0.2,epochs = 200 , batch_size=200 , callbacks=[checkpointer, early_stopping_callback])\n",
    "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X,Y)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_loss = history.history['val_loss']\n",
    "training_accuracy = history.history['val_accuracy']\n",
    "epoch_count = range(1, len(training_loss) + 1 )\n",
    "plt.plot(epoch_count, training_loss,training_accuracy , 'r--')\n",
    "plt.legend(['Training Loss' , 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_158 (Dense)            (None, 30)                390       \n",
      "_________________________________________________________________\n",
      "dense_159 (Dense)            (None, 12)                372       \n",
      "_________________________________________________________________\n",
      "dense_160 (Dense)            (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_161 (Dense)            (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델의 구조\n",
    "model.summary()\n",
    "# 12 x 30 + 30 \n",
    "# 30 x 12 + 12 \n",
    "# 12 x 8 + 8 \n",
    "# 8 x 1 + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평균집값 예측 -> 연속적 수치 예측\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy \n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "seed = 0 \n",
    "\n",
    "numpy.random.seed(seed)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "df = pd.read_csv(\"./dataset/housing.csv\" , delim_whitespace=True, header =None)\n",
    "dataset = df.values\n",
    "\n",
    "X = dataset[:,0:13]\n",
    "Y = dataset[:,13]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y , test_size = 0.3 , random_state = seed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제가격 : 22.600, 예상가격 :0.000\n",
      "실제가격 : 50.000, 예상가격 :0.000\n",
      "실제가격 : 23.000, 예상가격 :0.000\n",
      "실제가격 : 8.300, 예상가격 :0.000\n",
      "실제가격 : 21.200, 예상가격 :0.000\n",
      "실제가격 : 19.900, 예상가격 :0.000\n",
      "실제가격 : 20.600, 예상가격 :0.000\n",
      "실제가격 : 18.700, 예상가격 :0.000\n",
      "실제가격 : 16.100, 예상가격 :0.000\n",
      "실제가격 : 18.600, 예상가격 :0.000\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim = 13, activation='relu'))  # 여백을 확보 - 고차원으로 확대\n",
    "model.add(Dense(6, activation='relu'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "model.compile(loss = 'mean_squared_error' , optimizer='adam' ,metrics=['accuracy']) # 연속형 수치의 예측 : 비선형회귀\n",
    "model.fit(X_train,Y_train, epochs= 200 ,batch_size = 10, verbose=0)\n",
    "Y_prediction = model.predict(X_test).flatten()\n",
    "\n",
    "for i in range(10):\n",
    "    label = Y_test[i]\n",
    "    prediction = Y_prediction[i]\n",
    "    print(\"실제가격 : {:.3f}, 예상가격 :{:.3f}\".format(label,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter tuning \n",
    "- KerasRegressor 를 통해서 scikits 의 GridsearchCV를 사용 \n",
    "- keras의 모델이 함수로 wrapper 되어져야함\n",
    "- GridsearchCV 에서 파라미터 keras 의 모델로 전달해 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "import pandas \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# 케라스 + scikits 연결 : KerasRegressor\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과 : -119.50 (89.83) MSE\n"
     ]
    }
   ],
   "source": [
    "dataframe = pandas.read_csv(\"./dataset/housing.csv\", delim_whitespace=True, header =None)\n",
    "\n",
    "dataset = dataframe.values\n",
    "X =dataset[:,0:13]\n",
    "Y =dataset[:,13]\n",
    "def baseline_model(optimizer='adam'): # keras 모델을 함수화\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim = 13 , kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer ='normal'))\n",
    "    model.compile(loss = 'mean_squared_error' , optimizer= optimizer) # 속도조절 + momentum\n",
    "    return model\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# scikits 에서는 모델을 estimator , tf 에서는 신경망으로 ML을 만들고 estimator\n",
    "# keras를 > scikits 의 함수들을 쓸수 있도록 해주는 작업\n",
    "estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=100 , batch_size=5 ,verbose=0)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "\n",
    "results = cross_val_score(estimator,X,Y,cv=kfold)\n",
    "print(\"결과 : %2f(%.2f) MSE\" % (results.mean(),results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인 + gridsearchcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('standardize', StandardScaler(copy=True, with_mean=True, with_std=True)), ('mlp', <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x000001BB833D0EC8>)]\n",
      "표준화된 결과 : -29.22 (26.77) MSE\n",
      "[[6.3200e-03 1.8000e+01 2.3100e+00 0.0000e+00 5.3800e-01 6.5750e+00\n",
      "  6.5200e+01 4.0900e+00 1.0000e+00 2.9600e+02 1.5300e+01 3.9690e+02\n",
      "  4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 6.4210e+00\n",
      "  7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9690e+02\n",
      "  9.1400e+00]\n",
      " [2.7290e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 7.1850e+00\n",
      "  6.1100e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9283e+02\n",
      "  4.0300e+00]\n",
      " [3.2370e-02 0.0000e+00 2.1800e+00 0.0000e+00 4.5800e-01 6.9980e+00\n",
      "  4.5800e+01 6.0622e+00 3.0000e+00 2.2200e+02 1.8700e+01 3.9463e+02\n",
      "  2.9400e+00]\n",
      " [6.9050e-02 0.0000e+00 2.1800e+00 0.0000e+00 4.5800e-01 7.1470e+00\n",
      "  5.4200e+01 6.0622e+00 3.0000e+00 2.2200e+02 1.8700e+01 3.9690e+02\n",
      "  5.3300e+00]]\n",
      "WARNING:tensorflow:10 out of the last 14 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001BB839F8F78> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([30.027094, 25.186655, 33.130444, 32.832798, 33.322002],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp',KerasRegressor(build_fn=baseline_model, epochs=50 , batch_size = 5, verbose=0)))\n",
    "print(estimators)\n",
    "pipeline = Pipeline(estimators) # 데이터 정규화및 kerasRegressor 작업을 묶어줌\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(pipeline, X,Y, cv=kfold)\n",
    "\n",
    "print(\"표준화된 결과 : %.2f (%.2f) MSE\" % (results.mean(),results.std()))\n",
    "# scikits(머신러닝패키지) > kerasRegressor > keras(tesnorflow 문법)\n",
    "\n",
    "pipeline.fit(X,Y)\n",
    "print(X[0:5])\n",
    "pipeline.predict(X[0:5])\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2-13\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best: -204.500007 using {'mlp__optimizer': 'Nadam'}\n"
     ]
    }
   ],
   "source": [
    "optimizer = ['RMSprop', 'Adagrad', 'Adam', 'Nadam']\n",
    "\n",
    "param_grid = dict(mlp__optimizer=optimizer) # mlp_optimzer가 변수는 아님 달고 파이프 라인으로\n",
    "\n",
    "grid = GridSearchCV(estimator=pipeline, param_grid=param_grid,\n",
    "\n",
    "                   n_jobs=-1)\n",
    "\n",
    "grid_result = grid.fit(X,Y)\n",
    "\n",
    "print('best: %f using %s' % (grid_result.best_score_,\n",
    "\n",
    "                            grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문제 ) activation 의 parameter tuning 을 하시오 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과 : -97.162742(71.01) MSE\n",
      "결과 : -394.209226(71.01) MSE\n",
      "결과 : -90.654224(71.01) MSE\n"
     ]
    }
   ],
   "source": [
    "dataframe = pandas.read_csv(\"./dataset/housing.csv\", delim_whitespace=True, header =None)\n",
    "\n",
    "dataset = dataframe.values\n",
    "X =dataset[:,0:13]\n",
    "Y =dataset[:,13]\n",
    "def baseline_model(activation='elu' ,optimizer= 'Nadam'): # keras 모델을 함수화\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim = 13 , kernel_initializer='normal', activation=activation))\n",
    "    model.add(Dense(1, kernel_initializer ='normal'))\n",
    "    model.compile(loss = 'mean_squared_error' , optimizer= optimizer) # 속도조절 + momentum\n",
    "    return model\n",
    "\n",
    "def larger_model(optimizer='adam', activation='relu'): # keras 모델을 함수화\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim = 13 , kernel_initializer='normal', activation=activation))\n",
    "    model.add(Dense(6, activation= activation,kernel_initializer='normal'))\n",
    "    model.add(Dense(3, activation= activation, kernel_initializer='normal'))\n",
    "    model.add(Dense(1, kernel_initializer ='normal'))\n",
    "    model.compile(loss = 'mean_squared_error' , optimizer= optimizer) # 속도조절 + momentum\n",
    "    return model\n",
    "\n",
    "def wider_model(optimizer='adam', activation='elu'): # keras 모델을 함수화\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim = 13 , kernel_initializer='normal', activation=activation))\n",
    "    model.add(Dense(1, kernel_initializer ='normal'))\n",
    "    model.compile(loss = 'mean_squared_error' , optimizer= optimizer) # 속도조절 + momentum\n",
    "    return model\n",
    "\n",
    "def wider1_model(optimizer='adam', activation='relu'): # keras 모델을 함수화\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim = 13 , kernel_initializer='normal', activation=activation))\n",
    "    model.add(Dense(1, kernel_initializer ='normal'))\n",
    "    model.compile(loss = 'mean_squared_error' , optimizer= optimizer) # 속도조절 + momentum\n",
    "    return model\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# scikits 에서는 모델을 estimator , tf 에서는 신경망으로 ML을 만들고 estimator\n",
    "# keras를 > scikits 의 함수들을 쓸수 있도록 해주는 작업\n",
    "estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=100 , batch_size=5 ,verbose=0)\n",
    "estimator1 = KerasRegressor(build_fn=larger_model, nb_epoch=100 , batch_size=5 ,verbose=0)\n",
    "estimator2 = KerasRegressor(build_fn=wider_model, nb_epoch=100 , batch_size=5 ,verbose=0)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(estimator,X,Y,cv=kfold)\n",
    "results1 = cross_val_score(estimator1,X,Y,cv=kfold)\n",
    "results2 = cross_val_score(estimator2,X,Y,cv=kfold)\n",
    "print(\"결과 : %2f(%.2f) MSE\" % (results.mean(),results.std()))\n",
    "print(\"결과 : %2f(%.2f) MSE\" % (results1.mean(),results.std()))\n",
    "print(\"결과 : %2f(%.2f) MSE\" % (results2.mean(),results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('standardize', StandardScaler(copy=True, with_mean=True, with_std=True)), ('mlp', <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x000001BB84FE2E48>)]\n",
      "표준화된 결과 : -27.33 (38.37) MSE\n",
      "[[6.3200e-03 1.8000e+01 2.3100e+00 0.0000e+00 5.3800e-01 6.5750e+00\n",
      "  6.5200e+01 4.0900e+00 1.0000e+00 2.9600e+02 1.5300e+01 3.9690e+02\n",
      "  4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 6.4210e+00\n",
      "  7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9690e+02\n",
      "  9.1400e+00]\n",
      " [2.7290e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 7.1850e+00\n",
      "  6.1100e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9283e+02\n",
      "  4.0300e+00]\n",
      " [3.2370e-02 0.0000e+00 2.1800e+00 0.0000e+00 4.5800e-01 6.9980e+00\n",
      "  4.5800e+01 6.0622e+00 3.0000e+00 2.2200e+02 1.8700e+01 3.9463e+02\n",
      "  2.9400e+00]\n",
      " [6.9050e-02 0.0000e+00 2.1800e+00 0.0000e+00 4.5800e-01 7.1470e+00\n",
      "  5.4200e+01 6.0622e+00 3.0000e+00 2.2200e+02 1.8700e+01 3.9690e+02\n",
      "  5.3300e+00]]\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001BB941FE048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([27.101479, 23.812962, 32.112972, 29.813847, 30.365288],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline_model \n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp',KerasRegressor(build_fn=baseline_model, epochs=50 , batch_size = 5, verbose=0)))\n",
    "print(estimators)\n",
    "pipeline = Pipeline(estimators) # 데이터 정규화및 kerasRegressor 작업을 묶어줌\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(pipeline, X,Y, cv=kfold)\n",
    "\n",
    "print(\"표준화된 결과 : %.2f (%.2f) MSE\" % (results.mean(),results.std()))\n",
    "# scikits(머신러닝패키지) > kerasRegressor > keras(tesnorflow 문법)\n",
    "\n",
    "pipeline.fit(X,Y)\n",
    "print(X[0:5])\n",
    "pipeline.predict(X[0:5])\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('standardize', StandardScaler(copy=True, with_mean=True, with_std=True)), ('mlp', <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x000001BB95B75C08>)]\n",
      "표준화된 결과 : -21.94 (24.39) MSE\n",
      "[[6.3200e-03 1.8000e+01 2.3100e+00 0.0000e+00 5.3800e-01 6.5750e+00\n",
      "  6.5200e+01 4.0900e+00 1.0000e+00 2.9600e+02 1.5300e+01 3.9690e+02\n",
      "  4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 6.4210e+00\n",
      "  7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9690e+02\n",
      "  9.1400e+00]\n",
      " [2.7290e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 7.1850e+00\n",
      "  6.1100e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9283e+02\n",
      "  4.0300e+00]\n",
      " [3.2370e-02 0.0000e+00 2.1800e+00 0.0000e+00 4.5800e-01 6.9980e+00\n",
      "  4.5800e+01 6.0622e+00 3.0000e+00 2.2200e+02 1.8700e+01 3.9463e+02\n",
      "  2.9400e+00]\n",
      " [6.9050e-02 0.0000e+00 2.1800e+00 0.0000e+00 4.5800e-01 7.1470e+00\n",
      "  5.4200e+01 6.0622e+00 3.0000e+00 2.2200e+02 1.8700e+01 3.9690e+02\n",
      "  5.3300e+00]]\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001BB9ABD49D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([28.439049, 23.823671, 33.22274 , 31.598694, 31.43638 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# larger_model\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp',KerasRegressor(build_fn=larger_model, epochs=50 , batch_size = 5, verbose=0)))\n",
    "print(estimators)\n",
    "pipeline = Pipeline(estimators) # 데이터 정규화및 kerasRegressor 작업을 묶어줌\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(pipeline, X,Y, cv=kfold)\n",
    "\n",
    "print(\"표준화된 결과 : %.2f (%.2f) MSE\" % (results.mean(),results.std()))\n",
    "# scikits(머신러닝패키지) > kerasRegressor > keras(tesnorflow 문법)\n",
    "\n",
    "pipeline.fit(X,Y)\n",
    "print(X[0:5])\n",
    "pipeline.predict(X[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('standardize', StandardScaler(copy=True, with_mean=True, with_std=True)), ('mlp', <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x000001BB8E6960C8>)]\n",
      "표준화된 결과 : -26.86 (40.74) MSE\n",
      "[[6.3200e-03 1.8000e+01 2.3100e+00 0.0000e+00 5.3800e-01 6.5750e+00\n",
      "  6.5200e+01 4.0900e+00 1.0000e+00 2.9600e+02 1.5300e+01 3.9690e+02\n",
      "  4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 6.4210e+00\n",
      "  7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9690e+02\n",
      "  9.1400e+00]\n",
      " [2.7290e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 7.1850e+00\n",
      "  6.1100e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9283e+02\n",
      "  4.0300e+00]\n",
      " [3.2370e-02 0.0000e+00 2.1800e+00 0.0000e+00 4.5800e-01 6.9980e+00\n",
      "  4.5800e+01 6.0622e+00 3.0000e+00 2.2200e+02 1.8700e+01 3.9463e+02\n",
      "  2.9400e+00]\n",
      " [6.9050e-02 0.0000e+00 2.1800e+00 0.0000e+00 4.5800e-01 7.1470e+00\n",
      "  5.4200e+01 6.0622e+00 3.0000e+00 2.2200e+02 1.8700e+01 3.9690e+02\n",
      "  5.3300e+00]]\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001BB998D5798> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([27.286991, 24.070135, 32.40909 , 29.822447, 30.508184],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wider_model\n",
    "\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp',KerasRegressor(build_fn=wider_model, epochs=50 , batch_size = 5, verbose=0)))\n",
    "print(estimators)\n",
    "pipeline = Pipeline(estimators) # 데이터 정규화및 kerasRegressor 작업을 묶어줌\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(pipeline, X,Y, cv=kfold)\n",
    "\n",
    "print(\"표준화된 결과 : %.2f (%.2f) MSE\" % (results.mean(),results.std()))\n",
    "# scikits(머신러닝패키지) > kerasRegressor > keras(tesnorflow 문법)\n",
    "\n",
    "pipeline.fit(X,Y)\n",
    "print(X[0:5])\n",
    "pipeline.predict(X[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('standardize', StandardScaler(copy=True, with_mean=True, with_std=True)), ('mlp', <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x000001BB83891748>)]\n",
      "표준화된 결과 : -26.36 (26.03) MSE\n",
      "[[6.3200e-03 1.8000e+01 2.3100e+00 0.0000e+00 5.3800e-01 6.5750e+00\n",
      "  6.5200e+01 4.0900e+00 1.0000e+00 2.9600e+02 1.5300e+01 3.9690e+02\n",
      "  4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 6.4210e+00\n",
      "  7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9690e+02\n",
      "  9.1400e+00]\n",
      " [2.7290e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 7.1850e+00\n",
      "  6.1100e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9283e+02\n",
      "  4.0300e+00]\n",
      " [3.2370e-02 0.0000e+00 2.1800e+00 0.0000e+00 4.5800e-01 6.9980e+00\n",
      "  4.5800e+01 6.0622e+00 3.0000e+00 2.2200e+02 1.8700e+01 3.9463e+02\n",
      "  2.9400e+00]\n",
      " [6.9050e-02 0.0000e+00 2.1800e+00 0.0000e+00 4.5800e-01 7.1470e+00\n",
      "  5.4200e+01 6.0622e+00 3.0000e+00 2.2200e+02 1.8700e+01 3.9690e+02\n",
      "  5.3300e+00]]\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001BB8FA5A1F8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([29.37829 , 24.070667, 33.707302, 32.377   , 32.614105],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wider_model1\n",
    "\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp',KerasRegressor(build_fn=wider1_model, epochs=50 , batch_size = 5, verbose=0)))\n",
    "print(estimators)\n",
    "pipeline = Pipeline(estimators) # 데이터 정규화및 kerasRegressor 작업을 묶어줌\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(pipeline, X,Y, cv=kfold)\n",
    "\n",
    "print(\"표준화된 결과 : %.2f (%.2f) MSE\" % (results.mean(),results.std()))\n",
    "# scikits(머신러닝패키지) > kerasRegressor > keras(tesnorflow 문법)\n",
    "\n",
    "pipeline.fit(X,Y)\n",
    "print(X[0:5])\n",
    "pipeline.predict(X[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2-13\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\2-13\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best: -43.820604 using {'mlp__activation': 'elu', 'mlp__optimizer': 'Adam'}\n"
     ]
    }
   ],
   "source": [
    "activation = ['tanh', 'sigmoid', 'relu','elu']\n",
    "optimizer = ['RMSprop', 'Adagrad', 'Adam', 'Nadam']\n",
    "\n",
    "param_grid = dict(mlp__activation = activation,mlp__optimizer = optimizer) # mlp_optimzer가 변수는 아님 달고 파이프 라인으로\n",
    "\n",
    "grid = GridSearchCV(estimator=pipeline, param_grid=param_grid,\n",
    "\n",
    "                   n_jobs=-1)\n",
    "\n",
    "grid_result = grid.fit(X,Y)\n",
    "\n",
    "print('best: %f using %s' % (grid_result.best_score_,\n",
    "\n",
    "                            grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24.  21.6 34.7 33.4 36.2]\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001BB8C02A288> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([26.786348, 24.204178, 32.522175, 29.663662, 30.537216],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Y[0:5])\n",
    "grid.predict(X[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2-13\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\2-13\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\2-13\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\2-13\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\2-13\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\2-13\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\2-13\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\2-13\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\2-13\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\2-13\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\2-13\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\2-13\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\2-13\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\2-13\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\2-13\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\2-13\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardized : -35.30 (36.21) MSE\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10, random_state = seed)\n",
    "results = cross_val_score(grid,X,Y,cv=kfold)\n",
    "print(\"standardized : %.2f (%.2f) MSE\" % (results.mean(),results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
